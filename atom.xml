<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RP&#39;s Blog</title>
  
  <subtitle>学习总结  思考感悟</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://paradoxallen.github.io/"/>
  <updated>2018-06-13T04:55:26.430Z</updated>
  <id>https://paradoxallen.github.io/</id>
  
  <author>
    <name>LRP</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>关于机器学习在大气科学的应用</title>
    <link href="https://paradoxallen.github.io/21048/"/>
    <id>https://paradoxallen.github.io/21048/</id>
    <published>2018-05-06T16:00:00.000Z</published>
    <updated>2018-06-13T04:55:26.430Z</updated>
    
    <content type="html"><![CDATA[<p>前阵子阅读了院里一位博士研究生师兄的一篇有关基于神经网络算法对北京近五年的常规探空数据进行自组织分类，并揭示出大气污染物在不同边界层结构下的演变规律和相关机制的文章<a href="https://www.atmos-chem-phys.net/18/6771/2018/" target="_blank" rel="noopener">《Self-organized classification of boundary layer meteorology and associated characteristics of air quality in Beijing》</a>，看完顿时心生膜拜之情；</p><p>然后恰巧也是那个时候吕教授在院群上也转发了一篇关于机器学习预测火势甚至天气的公众号文章<a href="https://mp.weixin.qq.com/s?__biz=MjM5ODE1NDYyMA==&amp;mid=2653384819&amp;idx=2&amp;sn=523f27cb9442ab4af27137edd1280248&amp;chksm=bd1cc8608a6b4176d7cae939f794e082cf86b5bf0ac0deb53790f507f563f588b2148687957c&amp;mpshare=1&amp;scene=1&amp;srcid=0517RYcQWk5dWJCcqoI7jLCe#rd" target="_blank" rel="noopener">《机器学习成功解决“蝴蝶效应”！以后你终于可以相信天气预报了》</a>。</p><p>加之自己报名了一个<a href="https://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;mid=2247487345&amp;idx=1&amp;sn=4acb8978a2d95f0a1926c0e07021bec3&amp;chksm=e89455fcdfe3dcea645499d9321177a99ea713ffe06fe4af9d18c8506dd285755be2a1640539&amp;mpshare=1&amp;scene=1&amp;srcid=04151pmRgGqnfZDpvBL9EKKk#rd" target="_blank" rel="noopener">“预测北京和伦敦两个城市的空气质量”的KDD Cup 2018</a>但是因为自己报名太晚，组队不成（其实更深层的是之前关于机器学习的内容已经忘得差不多了。。。）</p><p>如此的机缘巧合，感觉将机器学习应用于大气科学将前途无量。我自己也想在这一方向进行深入了解，接下来我会进行相关内容的学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前阵子阅读了院里一位博士研究生师兄的一篇有关基于神经网络算法对北京近五年的常规探空数据进行自组织分类，并揭示出大气污染物在不同边界层结构下的演变规律和相关机制的文章&lt;a href=&quot;https://www.atmos-chem-phys.net/18/6771/2018/&quot;
      
    
    </summary>
    
      <category term="个人随笔" scheme="https://paradoxallen.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="大气科学" scheme="https://paradoxallen.github.io/tags/%E5%A4%A7%E6%B0%94%E7%A7%91%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>统计推断(零) 章节简介</title>
    <link href="https://paradoxallen.github.io/5451/"/>
    <id>https://paradoxallen.github.io/5451/</id>
    <published>2018-03-31T16:00:00.000Z</published>
    <updated>2018-06-03T16:17:44.275Z</updated>
    
    <content type="html"><![CDATA[<p>《统计推断(翻译版·原书第2版)》从概率论的基础开始，通过例子与习题的旁征博引，引进了大量近代统计处理的新技术和一些国内同类教材中不常见而又广为使用的分布。</p><p>其内容既包括工科概率入门、经典统计和现代统计的基础，又加进了不少近代统计中数据处理的实用方法和思想，例如：Bootstrap再抽样法、刀切(Jackkrlife)估计、EM算法、Logistic回归、稳健(Robest)回归、Markov链、Monte Carlo方法等。</p><p>它的统计内容与国内流行的教材相比，理论较深，模型较多，案例的涉及面要广，理论的应用面要丰富，统计思想的阐述与算法更为具体。</p><p>《统计推断(翻译版·原书第2版)》可作为工科、管理类学科专业本科生、研究生的教材或参考书，也可供教师、工程技术人员自学之用。</p><a id="more"></a><hr><h3 id="章节简介"><a href="#章节简介" class="headerlink" title="章节简介"></a><strong>章节简介</strong></h3><p><strong>出版说明</strong><br><strong>第2版序</strong><br><strong>第1版序</strong><br><strong>译后序</strong><br><strong>第1章 概率论</strong><br>1.1 集合论<br>1.2 概率论基础<br>1.2.1 公理化基础<br>1.2.2 概率演算<br>1.2.3 计数<br>1.2.4 枚举结果<br>1.3 条件概率与独立性<br>1.4 随机变量<br>1.5 分布函数<br>1.6 概率密度函数和概率质量函数<br>1.7 习题<br>1.8 杂录<br><strong>第2章 变换和期望</strong><br>2.1 随机变量函数的分布<br>2.2 期望<br>2.3 矩和矩母函数<br>2.4 积分号下的求导<br>2.5 习题<br>2.6 杂录<br>2.6.1 矩列的唯一性<br>2.6.2 其他母函数<br>2.6.3 矩母函数能否唯一地确定分布？<br><strong>第3章 常见分布族</strong><br>3.1 引言<br>3.2 离散分布<br>3.3 连续分布<br>3.4 指数族<br>3.5 位置与尺度族<br>3.6 不等式与恒等式<br>3.6.1 概率不等式<br>3.6.2 恒等式<br>3.7 习题<br>3.8 杂录<br>3.8.1 Poisson假设<br>3.8.2 Chebychev不等式及其改进<br>3.8.3 再谈指数族<br><strong>第4章 多维随机变量</strong><br>4.1 联合分布与边缘分布<br>4.2 条件分布与独立性<br>4.3 二维变换<br>4.4 多层模型与混合分布<br>4.5 协方差与相关<br>4.6 多维分布<br>4.7 不等式<br>4.7.1 数值不等式<br>4.7.2 函数不等式<br>4.8 习题<br>4.9 杂录<br>4.9.1 交换悖论<br>4.9.2 算术－几何－调和平均值不等式<br>8.3.1 错误概率与功效函数<br>8.3.2 最大功效检验<br>8.3.3 并－检验与交－并检验的真实水平<br>8.3.4 P-值<br>8.3.5 损失函数最优性<br>8.4 习题<br>8.5 杂录<br>8.5.1 单调功效函数<br>8.5.2 似然比作为证据<br>8.5.3 P-值和后验概率<br>8.5.4 置信集P-值<br><strong>第9章 区间估计</strong><br>9.1 引言<br>9.2 区间估计量的求法<br>9.2.1 反转一个检验统计量<br>9.2.2 枢轴量<br>9.2.3 枢轴化累积分布函数<br>9.2.4 Bayes区间<br>9.3 区间估计量的评价方法<br>9.3.1 尺寸和覆盖概率<br>9.3.2 与检验相关的最优性<br>9.3.3 Bayes最优<br>9.3.4 损失函数最优<br>9.4 习题<br>9.5 杂录<br>9.5.1 置信方法<br>9.5.2 离散分布中的置信区间<br>9.5.3 Fieller定理<br>9.5.4 其他区间如何?<br><strong>第10章 渐近评价</strong><br>10.1 点估计<br>10.1.1 相合性<br>10.1.2 有效性<br>10.1.3 计算与比较<br>10.1.4 自助法标准误差<br>10.2 稳健性<br>10.2.1 均值和中位数<br>10.2.2 M_估计量<br>10.3 假设检验<br>10.3.1 LRT的渐近分布<br>10.3.2 其他大样本检验<br>10.4 区间估计<br>10.4.1 近似极大似然区间<br>10.4.2 其他大样本区间<br>10.5 习题<br>10.6 杂录<br>10.6.1 超有效性<br>10.6.2 适当的正则性条件<br>10.6.3 再谈自助法<br>10.6.4 影响函数<br>10.6.5 自助法区间<br>10.6.6 稳健区间<br><strong>第11章 方差分析和回归分析</strong><br>11.1 引言<br>11.2 一种方式分组的方差分析<br>11.2.1 模型和分布假定<br>11.2.2 经典的ANOVA假设<br>11.2.3 均值的线性组合的推断<br>11.2.4 ANOVAF检验<br>11.2.5 对比的同时估计<br>11.2.6 平方和的分解<br>11.3 简单线性回归<br>11.3.1 最小二乘：数学解<br>11.3.2 最佳线性无偏估计：统计解<br>11.3.3 模型和分布假定<br>11.3.4 正态误差下的估计和检验<br>11.3.5 在给定点x=x0处的估计和预测<br>11.3.6 同时估计和置信带<br>11.4 习题<br>11.5 杂录<br>11.5.1 Cochran定理<br>11.5.2 多重比较<br>11.5.3 随机化完全区组设计<br>11.5.4 其他类型的方差分析<br>11.5.5 置信带的形状<br>11.5.6 Stein悖论<br><strong>第12章 回归模型</strong><br>12.1 引言<br>12.2 变量有误差时的回归<br>12.2.1 函数关系和结构关系<br>12.2.2 最小二乘解<br>12.2.3 极大似然估计<br>12.2.4 置信集<br>12.3 罗吉斯蒂克回归<br>12.3.1 模型<br>12.3.2 估计<br>12.4 稳健回归<br>12.5 习题<br>12.6 杂录<br>12.6.1 函数和结构的意义<br>12.6.2 EIV模型中常规最小乘的相合性<br>12.6.3 EIV模型中的工具变量<br>12.6.4 罗吉斯蒂克似然方程<br>12.6.5 再谈稳健回归<br><strong>附录 计算机代数</strong><br><strong>常用分布表</strong><br><strong>参考文献</strong><br><strong>作者索引</strong><br><strong>名词索引</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《统计推断(翻译版·原书第2版)》从概率论的基础开始，通过例子与习题的旁征博引，引进了大量近代统计处理的新技术和一些国内同类教材中不常见而又广为使用的分布。&lt;/p&gt;
&lt;p&gt;其内容既包括工科概率入门、经典统计和现代统计的基础，又加进了不少近代统计中数据处理的实用方法和思想，例如：Bootstrap再抽样法、刀切(Jackkrlife)估计、EM算法、Logistic回归、稳健(Robest)回归、Markov链、Monte Carlo方法等。&lt;/p&gt;
&lt;p&gt;它的统计内容与国内流行的教材相比，理论较深，模型较多，案例的涉及面要广，理论的应用面要丰富，统计思想的阐述与算法更为具体。&lt;/p&gt;
&lt;p&gt;《统计推断(翻译版·原书第2版)》可作为工科、管理类学科专业本科生、研究生的教材或参考书，也可供教师、工程技术人员自学之用。&lt;/p&gt;
    
    </summary>
    
      <category term="应用统计" scheme="https://paradoxallen.github.io/categories/%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="统计" scheme="https://paradoxallen.github.io/tags/%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>Python基础</title>
    <link href="https://paradoxallen.github.io/14702/"/>
    <id>https://paradoxallen.github.io/14702/</id>
    <published>2017-10-06T16:00:00.000Z</published>
    <updated>2018-06-25T08:19:24.203Z</updated>
    
    <content type="html"><![CDATA[<p>Python是一种计算机编程语言。计算机编程语言和我们日常使用的自然语言有所不同，最大的区别就是，自然语言在不同的语境下有不同的理解，而计算机要根据编程语言执行任务，就必须保证编程语言写出的程序决不能有歧义，所以，任何一种编程语言都有自己的一套语法，编译器或者解释器就是负责把符合语法的程序代码转换成CPU能够执行的机器码，然后执行。Python也不例外。</p><p>Python的语法比较简单，采用缩进方式，写出来的代码就像下面的样子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># print absolute value of an integer:</span><br><span class="line">a = 100</span><br><span class="line">if a &gt;= 0:</span><br><span class="line">    print(a)</span><br><span class="line">else:</span><br><span class="line">    print(-a)</span><br></pre></td></tr></table></figure></p><p>以#开头的语句是注释，注释是给人看的，可以是任意内容，解释器会忽略掉注释。其他每一行都是一个语句，当语句以冒号:结尾时，缩进的语句视为代码块。</p><p>缩进有利有弊。好处是强迫你写出格式化的代码，但没有规定缩进是几个空格还是Tab。按照约定俗成的管理，应该始终坚持使用<strong>4个空格</strong>的缩进。</p><p>缩进的另一个好处是强迫你写出缩进较少的代码，你会倾向于把一段很长的代码拆分成若干函数，从而得到缩进较少的代码。</p><p>缩进的坏处就是“复制－粘贴”功能失效了，这是最坑爹的地方。当你重构代码时，粘贴过去的代码必须重新检查缩进是否正确。此外，IDE很难像格式化Java代码那样格式化Python代码。</p><p>最后，请务必注意，Python程序是<strong>大小写敏感</strong>的，如果写错了大小写，程序会报错。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>Python使用缩进来组织代码块，请务必遵守约定俗成的习惯，坚持使用4个空格的缩进。</p><p>在文本编辑器中，需要设置把Tab自动转换为4个空格，确保不混用Tab和空格。</p><hr><h2 id="数据类型和变量"><a href="#数据类型和变量" class="headerlink" title="数据类型和变量"></a>数据类型和变量</h2><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>计算机顾名思义就是可以做数学计算的机器，因此，计算机程序理所当然地可以处理各种数值。但是，计算机能处理的远不止数值，还可以处理文本、图形、音频、视频、网页等各种各样的数据，不同的数据，需要定义不同的数据类型。在Python中，能够直接处理的数据类型有以下几种：</p><h4 id="整数"><a href="#整数" class="headerlink" title="整数"></a>整数</h4><p>Python可以处理任意大小的整数，当然包括负整数，在程序中的表示方法和数学上的写法一模一样，例如：1，100，-8080，0，等等。</p><p>计算机由于使用二进制，所以，有时候用十六进制表示整数比较方便，十六进制用0x前缀和0-9，a-f表示，例如：0xff00，0xa5b4c3d2，等等。</p><h4 id="浮点数"><a href="#浮点数" class="headerlink" title="浮点数"></a>浮点数</h4><p>浮点数也就是小数，之所以称为浮点数，是因为按照科学记数法表示时，一个浮点数的小数点位置是可变的，比如，1.23x109和12.3x108是完全相等的。浮点数可以用数学写法，如1.23，3.14，-9.01，等等。但是对于很大或很小的浮点数，就必须用科学计数法表示，把10用e替代，1.23x109就是1.23e9，或者12.3e8，0.000012可以写成1.2e-5，等等。</p><p>整数和浮点数在计算机内部存储的方式是不同的，整数运算永远是精确的（除法难道也是精确的？是的！），而浮点数运算则可能会有四舍五入的误差。</p><h4 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h4><p>字符串是以单引号’或双引号”括起来的任意文本，比如’abc’，”xyz”等等。请注意，’’或””本身只是一种表示方式，不是字符串的一部分，因此，字符串’abc’只有a，b，c这3个字符。如果’本身也是一个字符，那就可以用””括起来，比如”I’m OK”包含的字符是I，’，m，空格，O，K这6个字符。</p><p>如果字符串内部既包含’又包含”怎么办？可以用转义字符\来标识，比如：</p><pre><code>&apos;I\&apos;m \&quot;OK\&quot;!&apos;</code></pre><p>表示的字符串内容是：</p><pre><code>I&apos;m &quot;OK&quot;!</code></pre><p>转义字符\可以转义很多字符，比如\n表示换行，\t表示制表符，字符\本身也要转义，所以\表示的字符就是\，可以在Python的交互式命令行用print()打印字符串看看：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; print(&apos;I\&apos;m ok.&apos;)</span><br><span class="line">I&apos;m ok.</span><br><span class="line">&gt;&gt;&gt; print(&apos;I\&apos;m learning\nPython.&apos;)</span><br><span class="line">I&apos;m learning</span><br><span class="line">Python.</span><br><span class="line">&gt;&gt;&gt; print(&apos;\\\n\\&apos;)</span><br><span class="line">\</span><br><span class="line">\</span><br></pre></td></tr></table></figure></p><p>如果字符串里面有很多字符都需要转义，就需要加很多\，为了简化，Python还允许用r’’表示’’内部的字符串默认不转义，可以自己试试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; print(&apos;\\\t\\&apos;)</span><br><span class="line">\       \</span><br><span class="line">&gt;&gt;&gt; print(r&apos;\\\t\\&apos;)</span><br><span class="line">\\\t\\</span><br></pre></td></tr></table></figure></p><p>如果字符串内部有很多换行，用\n写在一行里不好阅读，为了简化，Python允许用’’’…’’’的格式表示多行内容，可以自己试试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; print(&apos;&apos;&apos;line1</span><br><span class="line">... line2</span><br><span class="line">... line3&apos;&apos;&apos;)</span><br><span class="line">line1</span><br><span class="line">line2</span><br><span class="line">line3</span><br></pre></td></tr></table></figure></p><p>上面是在交互式命令行内输入，注意在输入多行内容时，提示符由&gt;&gt;&gt;变为…，提示你可以接着上一行输入，注意…是提示符，不是代码的一部分</p><p>当输入完结束符<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果写成程序并存为.py文件，就是：</span><br></pre></td></tr></table></figure></p><p>print(‘’’line1<br>line2<br>line3’’’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">多行字符串&apos;&apos;&apos;...&apos;&apos;&apos;还可以在前面加上r使用</span><br><span class="line"></span><br><span class="line">#### 布尔值 ####</span><br><span class="line">布尔值和布尔代数的表示完全一致，一个布尔值只有True、False两种值，要么是True，要么是False，在Python中，可以直接用True、False表示布尔值（请注意大小写），也可以通过布尔运算计算出来：</span><br></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><p>True<br>True<br>False<br>False<br>3 &gt; 2<br>True<br>3 &gt; 5<br>False<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">布尔值可以用and、or和not运算。</span><br><span class="line"></span><br><span class="line">and运算是与运算，只有所有都为True，and运算结果才是True：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>True and True<br>True<br>True and False<br>False<br>False and False<br>False<br>5 &gt; 3 and 3 &gt; 1<br>True<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">or运算是或运算，只要其中有一个为True，or运算结果就是True：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>True or True<br>True<br>True or False<br>True<br>False or False<br>False<br>5 &gt; 3 or 1 &gt; 3<br>True<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">not运算是非运算，它是一个单目运算符，把True变成False，False变成True：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>not True<br>False<br>not False<br>True<br>not 1 &gt; 2<br>True<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">布尔值经常用在条件判断中，比如：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><p>if age &gt;= 18:<br>    print(‘adult’)<br>else:<br>    print(‘teenager’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#### 空值 ####</span><br><span class="line">空值是Python里一个特殊的值，用None表示。None不能理解为0，因为0是有意义的，而None是一个特殊的空值。</span><br><span class="line"></span><br><span class="line">此外，Python还提供了列表、字典等多种数据类型，还允许创建自定义数据类型，我们后面会继续讲到。</span><br><span class="line"></span><br><span class="line">### 变量 ###</span><br><span class="line">变量的概念基本上和初中代数的方程变量是一致的，只是在计算机程序中，变量不仅可以是数字，还可以是任意数据类型。</span><br><span class="line"></span><br><span class="line">变量在程序中就是用一个变量名表示了，变量名必须是大小写英文、数字和_的组合，且不能用数字开头，比如：</span><br><span class="line"></span><br><span class="line">    a = 1</span><br><span class="line">变量a是一个整数。</span><br><span class="line"></span><br><span class="line">    t_007 = &apos;T007&apos;</span><br><span class="line">变量t_007是一个字符串。</span><br><span class="line"></span><br><span class="line">    Answer = True</span><br><span class="line">变量Answer是一个布尔值True。</span><br><span class="line"></span><br><span class="line">在Python中，等号=是赋值语句，可以把任意数据类型赋值给变量，同一个变量可以反复赋值，而且可以是不同类型的变量</span><br><span class="line"></span><br><span class="line">这种变量本身类型不固定的语言称之为**动态语言**，与之对应的是**静态语言**。静态语言在定义变量时必须指定变量类型，如果赋值的时候类型不匹配，就会报错。例如Java是静态语言，赋值语句如下（// 表示注释）：</span><br></pre></td></tr></table></figure></p><p>int a = 123; // a是整数类型变量<br>a = “ABC”; // 错误：不能把字符串赋给整型变量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">和静态语言相比，动态语言更灵活，就是这个原因。</span><br><span class="line"></span><br><span class="line">请不要把赋值语句的等号等同于数学的等号。比如下面的代码：</span><br></pre></td></tr></table></figure></p><p>x = 10<br>x = x + 2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果从数学上理解x = x + 2那无论如何是不成立的，在程序中，赋值语句先计算右侧的表达式x + 2，得到结果12，再赋给变量x。由于x之前的值是10，重新赋值后，x的值变成12。</span><br><span class="line"></span><br><span class="line">最后，理解变量在计算机内存中的表示也非常重要。当我们写：</span><br></pre></td></tr></table></figure></p><p>a = ‘ABC’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">时，Python解释器干了两件事情：</span><br><span class="line"></span><br><span class="line">在内存中创建了一个&apos;ABC&apos;的字符串；</span><br><span class="line"></span><br><span class="line">在内存中创建了一个名为a的变量，并把它指向&apos;ABC&apos;。</span><br><span class="line"></span><br><span class="line">也可以把一个变量a赋值给另一个变量b，这个操作实际上是把变量b指向变量a所指向的数据，例如下面的代码：</span><br></pre></td></tr></table></figure></p><h1 id="coding-utf-8"><a href="#coding-utf-8" class="headerlink" title="-- coding: utf-8 --"></a>-<em>- coding: utf-8 -</em>-</h1><p>a = ‘ABC’<br>b = a<br>a = ‘XYZ’<br>print(b)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">最后一行打印出变量b的内容到底是&apos;ABC&apos;呢还是&apos;XYZ&apos;？如果从数学意义上理解，就会错误地得出b和a相同，也应该是&apos;XYZ&apos;，但实际上b的值是&apos;ABC&apos;，让我们一行一行地执行代码，就可以看到到底发生了什么事：</span><br><span class="line"></span><br><span class="line">执行a = &apos;ABC&apos;，解释器创建了字符串&apos;ABC&apos;和变量a，并把a指向&apos;ABC&apos;：</span><br><span class="line"></span><br><span class="line">![](https://i.imgur.com/Nqfoewm.png)</span><br><span class="line"></span><br><span class="line">执行b = a，解释器创建了变量b，并把b指向a指向的字符串&apos;ABC&apos;：</span><br><span class="line"></span><br><span class="line">![](https://i.imgur.com/OZYspx8.png)</span><br><span class="line"></span><br><span class="line">执行a = &apos;XYZ&apos;，解释器创建了字符串&apos;XYZ&apos;，并把a的指向改为&apos;XYZ&apos;，但b并没有更改：</span><br><span class="line"></span><br><span class="line">![](https://i.imgur.com/ZzASTFK.png)</span><br><span class="line"></span><br><span class="line">所以，最后打印变量b的结果自然是&apos;ABC&apos;了。</span><br><span class="line"></span><br><span class="line">### 常量 ###</span><br><span class="line">所谓常量就是不能变的变量，比如常用的数学常数π就是一个常量。在Python中，通常用全部大写的变量名表示常量：</span><br><span class="line"></span><br><span class="line">    PI = 3.14159265359</span><br><span class="line">但事实上PI仍然是一个变量，Python根本没有任何机制保证PI不会被改变，所以，用全部大写的变量名表示常量只是一个习惯上的用法，如果你一定要改变变量PI的值，也没人能拦住你。</span><br><span class="line"></span><br><span class="line">最后解释一下整数的除法为什么也是精确的。在Python中，有两种除法，一种除法是/：</span><br></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><p>10 / 3<br>3.3333333333333335<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/除法计算结果是浮点数，即使是两个整数恰好整除，结果也是浮点数：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>9 / 3<br>3.0<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">还有一种除法是//，称为地板除，两个整数的除法仍然是整数：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>10 // 3<br>3<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">你没有看错，整数的地板除//永远是整数，即使除不尽。要做精确的除法，使用/就可以。</span><br><span class="line"></span><br><span class="line">因为//除法只取结果的整数部分，所以Python还提供一个余数运算，可以得到两个整数相除的余数：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>10 % 3<br>1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">无论整数做//除法还是取余数，结果永远是整数，所以，整数运算结果永远是精确的。</span><br><span class="line"></span><br><span class="line">### 小结 ###</span><br><span class="line">Python支持多种数据类型，在计算机内部，可以把任何数据都看成一个“对象”，而变量就是在程序中用来指向这些数据对象的，对变量赋值就是把数据和变量给关联起来。</span><br><span class="line"></span><br><span class="line">对变量赋值x = y是把变量x指向真正的对象，该对象是变量y所指向的。随后对变量y的赋值不影响变量x的指向。</span><br><span class="line"></span><br><span class="line">注意：Python的整数没有大小限制，而某些语言的整数根据其存储长度是有大小限制的，例如Java对32位整数的范围限制在-2147483648-2147483647。</span><br><span class="line"></span><br><span class="line">Python的浮点数也没有大小限制，但是超出一定范围就直接表示为inf（无限大）。</span><br><span class="line"></span><br><span class="line">----------</span><br><span class="line">## 字符串和编码 ##</span><br><span class="line">### 字符编码 ###</span><br><span class="line">我们已经讲过了，字符串也是一种数据类型，但是，字符串比较特殊的是还有一个编码问题。</span><br><span class="line"></span><br><span class="line">因为计算机只能处理数字，如果要处理文本，就必须先把文本转换为数字才能处理。最早的计算机在设计时采用8个比特（bit）作为一个字节（byte），所以，一个字节能表示的最大的整数就是255（二进制11111111=十进制255），如果要表示更大的整数，就必须用更多的字节。比如两个字节可以表示的最大整数是65535，4个字节可以表示的最大整数是4294967295。</span><br><span class="line"></span><br><span class="line">由于计算机是美国人发明的，因此，最早只有127个字符被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为ASCII编码，比如大写字母A的编码是65，小写字母z的编码是122。</span><br><span class="line"></span><br><span class="line">但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和ASCII编码冲突，所以，中国制定了GB2312编码，用来把中文编进去。</span><br><span class="line"></span><br><span class="line">你可以想得到的是，全世界有上百种语言，日本把日文编到Shift_JIS里，韩国把韩文编到Euc-kr里，各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">因此，Unicode应运而生。Unicode把所有语言都统一到一套编码里，这样就不会再有乱码问题了。</span><br><span class="line"></span><br><span class="line">Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。现代操作系统和大多数编程语言都直接支持Unicode。</span><br><span class="line"></span><br><span class="line">现在，捋一捋ASCII编码和Unicode编码的区别：ASCII编码是1个字节，而Unicode编码通常是2个字节。</span><br><span class="line"></span><br><span class="line">字母A用ASCII编码是十进制的65，二进制的01000001；</span><br><span class="line"></span><br><span class="line">字符0用ASCII编码是十进制的48，二进制的00110000，注意字符&apos;0&apos;和整数0是不同的；</span><br><span class="line"></span><br><span class="line">汉字中已经超出了ASCII编码的范围，用Unicode编码是十进制的20013，二进制的01001110 00101101。</span><br><span class="line"></span><br><span class="line">你可以猜测，如果把ASCII编码的A用Unicode编码，只需要在前面补0就可以，因此，A的Unicode编码是00000000 01000001。</span><br><span class="line"></span><br><span class="line">新的问题又出现了：如果统一成Unicode编码，乱码问题从此消失了。但是，如果你写的文本基本上全部是英文的话，用Unicode编码比ASCII编码需要多一倍的存储空间，在存储和传输上就十分不划算。</span><br><span class="line"></span><br><span class="line">所以，本着节约的精神，又出现了把Unicode编码转化为“可变长编码”的UTF-8编码。UTF-8编码把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。如果你要传输的文本包含大量英文字符，用UTF-8编码就能节省空间</span><br><span class="line"></span><br><span class="line">还可以发现，UTF-8编码有一个额外的好处，就是ASCII编码实际上可以被看成是UTF-8编码的一部分，所以，大量只支持ASCII编码的历史遗留软件可以在UTF-8编码下继续工作。</span><br><span class="line"></span><br><span class="line">搞清楚了ASCII、Unicode和UTF-8的关系，我们就可以总结一下现在计算机系统通用的字符编码工作方式：</span><br><span class="line"></span><br><span class="line">在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。</span><br><span class="line"></span><br><span class="line">用记事本编辑的时候，从文件读取的UTF-8字符被转换为Unicode字符到内存里，编辑完成后，保存的时候再把Unicode转换为UTF-8保存到文件：</span><br><span class="line"></span><br><span class="line">![](https://i.imgur.com/q7kaQ5B.png)</span><br><span class="line"></span><br><span class="line">浏览网页的时候，服务器会把动态生成的Unicode内容转换为UTF-8再传输到浏览器：</span><br><span class="line"></span><br><span class="line">![](https://i.imgur.com/JCGHELm.png)</span><br><span class="line"></span><br><span class="line">所以你看到很多网页的源码上会有类似`&lt;meta charset=&quot;UTF-8&quot; /&gt;`的信息，表示该网页正是用的UTF-8编码。</span><br><span class="line"></span><br><span class="line">### Python的字符串 ###</span><br><span class="line">搞清楚了令人头疼的字符编码问题后，我们再来研究Python的字符串。</span><br><span class="line"></span><br><span class="line">在最新的Python 3版本中，字符串是以Unicode编码的，也就是说，Python的字符串支持多语言，例如：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>print(‘包含中文的str’)<br>包含中文的str<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于单个字符的编码，Python提供了ord()函数获取字符的整数表示，chr()函数把编码转换为对应的字符：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>ord(‘A’)<br>65<br>ord(‘中’)<br>20013<br>chr(66)<br>‘B’<br>chr(25991)<br>‘文’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果知道字符的整数编码，还可以用十六进制这么写str：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>‘\u4e2d\u6587’<br>‘中文’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">两种写法完全是等价的。</span><br><span class="line"></span><br><span class="line">由于Python的字符串类型是str，在内存中以Unicode表示，一个字符对应若干个字节。如果要在网络上传输，或者保存到磁盘上，就需要把str变为以字节为单位的bytes。</span><br><span class="line"></span><br><span class="line">Python对bytes类型的数据用带b前缀的单引号或双引号表示：</span><br><span class="line"></span><br><span class="line">    x = b&apos;ABC&apos;</span><br><span class="line">要注意区分&apos;ABC&apos;和b&apos;ABC&apos;，前者是str，后者虽然内容显示得和前者一样，但bytes的每个字符都只占用一个字节。</span><br><span class="line"></span><br><span class="line">以Unicode表示的str通过encode()方法可以编码为指定的bytes，例如：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>‘ABC’.encode(‘ascii’)<br>b’ABC’<br>‘中文’.encode(‘utf-8’)<br>b’\xe4\xb8\xad\xe6\x96\x87’<br>‘中文’.encode(‘ascii’)<br>Traceback (most recent call last):<br>  File “<stdin>“, line 1, in <module><br>UnicodeEncodeError: ‘ascii’ codec can’t encode characters in position 0-1: ordinal not in range(128)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">纯英文的str可以用ASCII编码为bytes，内容是一样的，含有中文的str可以用UTF-8编码为bytes。含有中文的str无法用ASCII编码，因为中文编码的范围超过了ASCII编码的范围，Python会报错。</span><br><span class="line"></span><br><span class="line">在bytes中，无法显示为ASCII字符的字节，用\x##显示。</span><br><span class="line"></span><br><span class="line">反过来，如果我们从网络或磁盘上读取了字节流，那么读到的数据就是bytes。要把bytes变为str，就需要用decode()方法：</span><br></pre></td></tr></table></figure></module></stdin></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>b’ABC’.decode(‘ascii’)<br>‘ABC’<br>b’\xe4\xb8\xad\xe6\x96\x87’.decode(‘utf-8’)<br>‘中文’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果bytes中包含无法解码的字节，decode()方法会报错：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>b’\xe4\xb8\xad\xff’.decode(‘utf-8’)<br>Traceback (most recent call last):<br>  …<br>UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xff in position 3: invalid start byte<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果bytes中只有一小部分无效的字节，可以传入errors=&apos;ignore&apos;忽略错误的字节：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>b’\xe4\xb8\xad\xff’.decode(‘utf-8’, errors=’ignore’)<br>‘中’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要计算str包含多少个字符，可以用len()函数：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>len(‘ABC’)<br>3<br>len(‘中文’)<br>2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len()函数计算的是str的字符数，如果换成bytes，len()函数就计算字节数：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>len(b’ABC’)<br>3<br>len(b’\xe4\xb8\xad\xe6\x96\x87’)<br>6<br>len(‘中文’.encode(‘utf-8’))<br>6<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">可见，1个中文字符经过UTF-8编码后通常会占用3个字节，而1个英文字符只占用1个字节。</span><br><span class="line"></span><br><span class="line">在操作字符串时，我们经常遇到str和bytes的互相转换。为了避免乱码问题，应当始终坚持使用UTF-8编码对str和bytes进行转换。</span><br><span class="line"></span><br><span class="line">由于Python源代码也是一个文本文件，所以，当你的源代码中包含中文的时候，在保存源代码时，就需要务必指定保存为UTF-8编码。当Python解释器读取源代码时，为了让它按UTF-8编码读取，我们通常在文件开头写上这两行：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><p>#!/usr/bin/env python3</p><h1 id="coding-utf-8-1"><a href="#coding-utf-8-1" class="headerlink" title="-- coding: utf-8 --"></a>-<em>- coding: utf-8 -</em>-</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">第一行注释是为了告诉Linux/OS X系统，这是一个Python可执行程序，Windows系统会忽略这个注释；</span><br><span class="line"></span><br><span class="line">第二行注释是为了告诉Python解释器，按照UTF-8编码读取源代码，否则，你在源代码中写的中文输出可能会有乱码。</span><br><span class="line"></span><br><span class="line">申明了UTF-8编码并不意味着你的.py文件就是UTF-8编码的，必须并且要确保文本编辑器正在使用UTF-8 without BOM编码：</span><br><span class="line"></span><br><span class="line">![](https://i.imgur.com/XbYlfHM.png)</span><br><span class="line"></span><br><span class="line">如果.py文件本身使用UTF-8编码，并且也申明了# -*- coding: utf-8 -*-，打开命令提示符测试就可以正常显示中文：</span><br><span class="line"></span><br><span class="line">![](https://i.imgur.com/OSmkxcu.png)</span><br><span class="line"></span><br><span class="line">### 格式化 ###</span><br><span class="line">最后一个常见的问题是如何输出格式化的字符串。我们经常会输出类似`&apos;亲爱的xxx你好！你xx月的话费是xx，余额是xx&apos;`之类的字符串，而xxx的内容都是根据变量变化的，所以，需要一种简便的格式化字符串的方式。</span><br><span class="line"></span><br><span class="line">在Python中，采用的格式化方式和C语言是一致的，用%实现，举例如下：</span><br></pre></td></tr></table></figure><blockquote><blockquote><blockquote><p>‘Hello, %s’ % ‘world’<br>‘Hello, world’<br>‘Hi, %s, you have $%d.’ % (‘Michael’, 1000000)<br>‘Hi, Michael, you have $1000000.’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">你可能猜到了，%运算符就是用来格式化字符串的。在字符串内部，%s表示用字符串替换，%d表示用整数替换，有几个%?占位符，后面就跟几个变量或者值，顺序要对应好。如果只有一个%?，括号可以省略。</span><br><span class="line"></span><br><span class="line">常见的占位符有：</span><br><span class="line"></span><br><span class="line">占位符替换内容</span><br><span class="line">%d整数</span><br><span class="line">%f浮点数</span><br><span class="line">%s字符串</span><br><span class="line">%x十六进制整数</span><br><span class="line">其中，格式化整数和浮点数还可以指定是否补0和整数与小数的位数：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><h1 id="coding-utf-8-2"><a href="#coding-utf-8-2" class="headerlink" title="-- coding: utf-8 --"></a>-<em>- coding: utf-8 -</em>-</h1><p>print(‘%2d-%02d’ % (3, 1))<br>print(‘%.2f’ % 3.1415926)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果你不太确定应该用什么，%s永远起作用，它会把任何数据类型转换为字符串：</span><br></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><p>‘Age: %s. Gender: %s’ % (25, True)<br>‘Age: 25. Gender: True’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">有些时候，字符串里面的%是一个普通字符怎么办？这个时候就需要转义，用%%来表示一个%：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>‘growth rate: %d %%’ % 7<br>‘growth rate: 7 %’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### format() ###</span><br><span class="line"></span><br><span class="line">另一种格式化字符串的方法是使用字符串的format()方法，它会用传入的参数依次替换字符串内的占位符&#123;0&#125;、&#123;1&#125;……，不过这种方式写起来比%要麻烦得多：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>‘Hello, {0}, 成绩提升了 {1:.1f}%’.format(‘小明’, 17.125)<br>‘Hello, 小明, 成绩提升了 17.1%’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">小结</span><br><span class="line">Python 3的字符串使用Unicode，直接支持多语言。</span><br><span class="line"></span><br><span class="line">当str和bytes互相转换时，需要指定编码。最常用的编码是UTF-8。Python当然也支持其他编码方式，比如把Unicode编码成GB2312：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>‘中文’.encode(‘gb2312’)<br>b’\xd6\xd0\xce\xc4’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">但这种方式纯属自找麻烦，如果没有特殊业务要求，请牢记仅使用UTF-8编码。</span><br><span class="line"></span><br><span class="line">格式化字符串的时候，可以用Python的交互式环境测试，方便快捷。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">----------</span><br><span class="line">## 使用list和tuple ##</span><br><span class="line">### list ###</span><br><span class="line">Python内置的一种数据类型是列表：list。list是一种有序的集合，可以随时添加和删除其中的元素。</span><br><span class="line"></span><br><span class="line">比如，列出班里所有同学的名字，就可以用一个list表示：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>classmates = [‘Michael’, ‘Bob’, ‘Tracy’]<br>classmates<br>[‘Michael’, ‘Bob’, ‘Tracy’]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">变量classmates就是一个list。用len()函数可以获得list元素的个数：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>len(classmates)<br>3<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用索引来访问list中每一个位置的元素，记得索引是从0开始的：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>classmates[0]<br>‘Michael’<br>classmates[1]<br>‘Bob’<br>classmates[2]<br>‘Tracy’<br>classmates[3]<br>Traceback (most recent call last):<br>  File “<stdin>“, line 1, in <module><br>IndexError: list index out of range<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">当索引超出了范围时，Python会报一个IndexError错误，所以，要确保索引不要越界，记得最后一个元素的索引是len(classmates) - 1。</span><br><span class="line"></span><br><span class="line">如果要取最后一个元素，除了计算索引位置外，还可以用-1做索引，直接获取最后一个元素：</span><br></pre></td></tr></table></figure></module></stdin></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>classmates[-1]<br>‘Tracy’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以此类推，可以获取倒数第2个、倒数第3个：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>classmates[-2]<br>‘Bob’<br>classmates[-3]<br>‘Michael’<br>classmates[-4]<br>Traceback (most recent call last):<br>  File “<stdin>“, line 1, in <module><br>IndexError: list index out of range<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">当然，倒数第4个就越界了。</span><br><span class="line"></span><br><span class="line">list是一个可变的有序表，所以，可以往list中追加元素到末尾：</span><br></pre></td></tr></table></figure></module></stdin></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>classmates.append(‘Adam’)<br>classmates<br>[‘Michael’, ‘Bob’, ‘Tracy’, ‘Adam’]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">也可以把元素插入到指定的位置，比如索引号为1的位置：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>classmates.insert(1, ‘Jack’)<br>classmates<br>[‘Michael’, ‘Jack’, ‘Bob’, ‘Tracy’, ‘Adam’]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要删除list末尾的元素，用pop()方法：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>classmates.pop()<br>‘Adam’<br>classmates<br>[‘Michael’, ‘Jack’, ‘Bob’, ‘Tracy’]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要删除指定位置的元素，用pop(i)方法，其中i是索引位置：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>classmates.pop(1)<br>‘Jack’<br>classmates<br>[‘Michael’, ‘Bob’, ‘Tracy’]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要把某个元素替换成别的元素，可以直接赋值给对应的索引位置：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>classmates[1] = ‘Sarah’<br>classmates<br>[‘Michael’, ‘Sarah’, ‘Tracy’]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list里面的元素的数据类型也可以不同，比如：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>L = [‘Apple’, 123, True]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list元素也可以是另一个list，比如：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>s = [‘python’, ‘java’, [‘asp’, ‘php’], ‘scheme’]<br>len(s)<br>4<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要注意s只有4个元素，其中s[2]又是一个list，如果拆开写就更容易理解了：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>p = [‘asp’, ‘php’]<br>s = [‘python’, ‘java’, p, ‘scheme’]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">要拿到&apos;php&apos;可以写p[1]或者s[2][1]，因此s可以看成是一个二维数组，类似的还有三维、四维……数组，不过很少用到。</span><br><span class="line"></span><br><span class="line">如果一个list中一个元素也没有，就是一个空的list，它的长度为0：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>L = []<br>len(L)<br>0<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### tuple ###</span><br><span class="line">另一种有序列表叫元组：tuple。tuple和list非常类似，但是tuple一旦初始化就不能修改，比如同样是列出同学的名字：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>classmates = (‘Michael’, ‘Bob’, ‘Tracy’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">现在，classmates这个tuple不能变了，它也没有append()，insert()这样的方法。其他获取元素的方法和list是一样的，你可以正常地使用classmates[0]，classmates[-1]，但不能赋值成另外的元素。</span><br><span class="line"></span><br><span class="line">不可变的tuple有什么意义？因为tuple不可变，所以代码更安全。如果可能，能用tuple代替list就尽量用tuple。</span><br><span class="line"></span><br><span class="line">tuple的陷阱：当你定义一个tuple时，在定义的时候，tuple的元素就必须被确定下来，比如：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>t = (1, 2)<br>t<br>(1, 2)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果要定义一个空的tuple，可以写成()：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>t = ()<br>t<br>()<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">但是，要定义一个只有1个元素的tuple，如果你这么定义：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>t = (1)<br>t<br>1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">定义的不是tuple，是1这个数！这是因为括号()既可以表示tuple，又可以表示数学公式中的小括号，这就产生了歧义，因此，Python规定，这种情况下，按小括号进行计算，计算结果自然是1。</span><br><span class="line"></span><br><span class="line">所以，只有1个元素的tuple定义时必须加一个逗号,，来消除歧义：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>t = (1,)<br>t<br>(1,)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Python在显示只有1个元素的tuple时，也会加一个逗号,，以免你误解成数学计算意义上的括号。</span><br><span class="line"></span><br><span class="line">最后来看一个“可变的”tuple：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>t = (‘a’, ‘b’, [‘A’, ‘B’])<br>t[2][0] = ‘X’<br>t[2][1] = ‘Y’<br>t<br>(‘a’, ‘b’, [‘X’, ‘Y’])<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">这个tuple定义的时候有3个元素，分别是&apos;a&apos;，&apos;b&apos;和一个list。不是说tuple一旦定义后就不可变了吗？怎么后来又变了？</span><br><span class="line"></span><br><span class="line">别急，我们先看看定义的时候tuple包含的3个元素：</span><br><span class="line"></span><br><span class="line">![](https://i.imgur.com/qUlZc9A.png)</span><br><span class="line"></span><br><span class="line">当我们把list的元素&apos;A&apos;和&apos;B&apos;修改为&apos;X&apos;和&apos;Y&apos;后，tuple变为：</span><br><span class="line"></span><br><span class="line">![](https://i.imgur.com/s8K4auU.png)</span><br><span class="line"></span><br><span class="line">表面上看，tuple的元素确实变了，但其实变的不是tuple的元素，而是list的元素。tuple一开始指向的list并没有改成别的list，所以，tuple所谓的“不变”是说，tuple的每个元素，指向永远不变。即指向&apos;a&apos;，就不能改成指向&apos;b&apos;，指向一个list，就不能改成指向其他对象，但指向的这个list本身是可变的！</span><br><span class="line"></span><br><span class="line">理解了“指向不变”后，要创建一个内容也不变的tuple怎么做？那就必须保证tuple的每一个元素本身也不能变。</span><br><span class="line"></span><br><span class="line">### 小结 ###</span><br><span class="line">list和tuple是Python内置的有序集合，一个可变，一个不可变。根据需要来选择使用它们。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">----------</span><br><span class="line">## 条件判断 ##</span><br><span class="line">计算机之所以能做很多自动化的任务，因为它可以自己做条件判断。</span><br><span class="line"></span><br><span class="line">比如，输入用户年龄，根据年龄打印不同的内容，在Python程序中，用if语句实现：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><p>age = 20<br>if age &gt;= 18:<br>    print(‘your age is’, age)<br>    print(‘adult’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">根据Python的缩进规则，如果if语句判断是True，就把缩进的两行print语句执行了，否则，什么也不做。</span><br><span class="line"></span><br><span class="line">也可以给if添加一个else语句，意思是，如果if判断是False，不要执行if的内容，去把else执行了：</span><br></pre></td></tr></table></figure></p><p>age = 3<br>if age &gt;= 18:<br>    print(‘your age is’, age)<br>    print(‘adult’)<br>else:<br>    print(‘your age is’, age)<br>    print(‘teenager’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意不要少写了冒号:。</span><br><span class="line"></span><br><span class="line">当然上面的判断是很粗略的，完全可以用elif做更细致的判断：</span><br></pre></td></tr></table></figure></p><p>age = 3<br>if age &gt;= 18:<br>    print(‘adult’)<br>elif age &gt;= 6:<br>    print(‘teenager’)<br>else:<br>    print(‘kid’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">elif是else if的缩写，完全可以有多个elif，所以if语句的完整形式就是：</span><br></pre></td></tr></table></figure></p><p>if &lt;条件判断1&gt;:<br>    &lt;执行1&gt;<br>elif &lt;条件判断2&gt;:<br>    &lt;执行2&gt;<br>elif &lt;条件判断3&gt;:<br>    &lt;执行3&gt;<br>else:<br>    &lt;执行4&gt;<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">if语句执行有个特点，它是从上往下判断，如果在某个判断上是True，把该判断对应的语句执行后，就忽略掉剩下的elif和else，所以，请测试并解释为什么下面的程序打印的是teenager：</span><br></pre></td></tr></table></figure></p><p>age = 20<br>if age &gt;= 6:<br>    print(‘teenager’)<br>elif age &gt;= 18:<br>    print(‘adult’)<br>else:<br>    print(‘kid’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">if判断条件还可以简写，比如写：</span><br></pre></td></tr></table></figure></p><p>if x:<br>    print(‘True’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">只要x是非零数值、非空字符串、非空list等，就判断为True，否则为False。</span><br><span class="line"></span><br><span class="line">### 再议 input ###</span><br><span class="line">最后看一个有问题的条件判断。很多同学会用input()读取用户的输入，这样可以自己输入，程序运行得更有意思：</span><br></pre></td></tr></table></figure></p><p>birth = input(‘birth: ‘)<br>if birth &lt; 2000:<br>    print(‘00前’)<br>else:<br>    print(‘00后’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">输入1982，结果报错：</span><br></pre></td></tr></table></figure></p><p>Traceback (most recent call last):<br>  File “<stdin>“, line 1, in <module><br>TypeError: unorderable types: str() &gt; int()<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这是因为input()返回的数据类型是str，str不能直接和整数比较，必须先把str转换成整数。Python提供了int()函数来完成这件事情：</span><br></pre></td></tr></table></figure></module></stdin></p><p>s = input(‘birth: ‘)<br>birth = int(s)<br>if birth &lt; 2000:<br>    print(‘00前’)<br>else:<br>    print(‘00后’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再次运行，就可以得到正确地结果。但是，如果输入abc呢？又会得到一个错误信息：</span><br></pre></td></tr></table></figure></p><p>Traceback (most recent call last):<br>  File “<stdin>“, line 1, in <module><br>ValueError: invalid literal for int() with base 10: ‘abc’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">原来int()函数发现一个字符串并不是合法的数字时就会报错，程序就退出了。</span><br><span class="line"></span><br><span class="line">如何检查并捕获程序运行期的错误呢？后面的错误和调试会讲到。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 小结 ###</span><br><span class="line">条件判断可以让计算机自己做选择，Python的if...elif...else很灵活。</span><br><span class="line"></span><br><span class="line">条件判断从上向下匹配，当满足条件时执行对应的块内语句，后续的elif和else都不再执行。</span><br><span class="line"></span><br><span class="line">----------</span><br><span class="line">## 循环 ##</span><br><span class="line">要计算1+2+3，我们可以直接写表达式：</span><br></pre></td></tr></table></figure></module></stdin></p><blockquote><blockquote><blockquote><p>1 + 2 + 3<br>6<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">要计算1+2+3+...+10，勉强也能写出来。</span><br><span class="line"></span><br><span class="line">但是，要计算1+2+3+...+10000，直接写表达式就不可能了。</span><br><span class="line"></span><br><span class="line">为了让计算机能计算成千上万次的重复运算，我们就需要循环语句。</span><br><span class="line"></span><br><span class="line">Python的循环有两种，一种是**for...in**循环，依次把list或tuple中的每个元素迭代出来，看例子：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><p>names = [‘Michael’, ‘Bob’, ‘Tracy’]<br>for name in names:<br>    print(name)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">执行这段代码，会依次打印names的每一个元素：</span><br></pre></td></tr></table></figure></p><p>Michael<br>Bob<br>Tracy<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">所以for x in ...循环就是把每个元素代入变量x，然后执行缩进块的语句。</span><br><span class="line"></span><br><span class="line">再比如我们想计算1-10的整数之和，可以用一个sum变量做累加：</span><br></pre></td></tr></table></figure></p><p>sum = 0<br>for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:<br>    sum = sum + x<br>print(sum)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">如果要计算1-100的整数之和，从1写到100有点困难，幸好Python提供一个range()函数，可以生成一个整数序列，再通过list()函数可以转换为list。比如range(5)生成的序列是从0开始小于5的整数：</span><br></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><p>list(range(5))<br>[0, 1, 2, 3, 4]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">range(101)就可以生成0-100的整数序列，计算如下：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><h1 id="coding-utf-8-3"><a href="#coding-utf-8-3" class="headerlink" title="-- coding: utf-8 --"></a>-<em>- coding: utf-8 -</em>-</h1><p>sum = 0<br>for x in range(101):<br>    sum = sum + x<br>print(sum)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">第二种循环是**while**循环，只要条件满足，就不断循环，条件不满足时退出循环。比如我们要计算100以内所有奇数之和，可以用while循环实现：</span><br></pre></td></tr></table></figure></p><p>sum = 0<br>n = 99<br>while n &gt; 0:<br>    sum = sum + n<br>    n = n - 2<br>print(sum)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">在循环内部变量n不断自减，直到变为-1时，不再满足while条件，循环退出。</span><br><span class="line"></span><br><span class="line">### break ###</span><br><span class="line">在循环中，break语句可以提前退出循环。例如，本来要循环打印1～100的数字：</span><br></pre></td></tr></table></figure></p><p>n = 1<br>while n &lt;= 100:<br>    print(n)<br>    n = n + 1<br>print(‘END’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">上面的代码可以打印出1~100。</span><br><span class="line"></span><br><span class="line">如果要提前结束循环，可以用break语句：</span><br></pre></td></tr></table></figure></p><p>n = 1<br>while n &lt;= 100:<br>    if n &gt; 10: # 当n = 11时，条件满足，执行break语句<br>        break # break语句会结束当前循环<br>    print(n)<br>    n = n + 1<br>print(‘END’)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">执行上面的代码可以看到，打印出1~10后，紧接着打印END，程序结束。</span><br><span class="line"></span><br><span class="line">可见break的作用是提前结束循环。</span><br><span class="line"></span><br><span class="line">### continue ###</span><br><span class="line">在循环过程中，也可以通过continue语句，跳过当前的这次循环，直接开始下一次循环。</span><br></pre></td></tr></table></figure></p><p>n = 0<br>while n &lt; 10:<br>    n = n + 1<br>    print(n)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">上面的程序可以打印出1～10。但是，如果我们想只打印奇数，可以用continue语句跳过某些循环：</span><br></pre></td></tr></table></figure></p><p>n = 0<br>while n &lt; 10:<br>    n = n + 1<br>    if n % 2 == 0: # 如果n是偶数，执行continue语句<br>        continue # continue语句会直接继续下一轮循环，后续的print()语句不会执行<br>    print(n)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">执行上面的代码可以看到，打印的不再是1～10，而是1，3，5，7，9。</span><br><span class="line"></span><br><span class="line">可见continue的作用是提前结束本轮循环，并直接开始下一轮循环。</span><br><span class="line"></span><br><span class="line">### 小结 ###</span><br><span class="line">循环是让计算机做重复任务的有效的方法。</span><br><span class="line"></span><br><span class="line">break语句可以在循环过程中直接退出循环，而continue语句可以提前结束本轮循环，并直接开始下一轮循环。这两个语句通常都必须配合if语句使用。</span><br><span class="line"></span><br><span class="line">要特别注意，不要滥用break和continue语句。break和continue会造成代码执行逻辑分叉过多，容易出错。大多数循环并不需要用到break和continue语句，上面的两个例子，都可以通过改写循环条件或者修改循环逻辑，去掉break和continue语句。</span><br><span class="line"></span><br><span class="line">有些时候，如果代码写得有问题，会让程序陷入“死循环”，也就是永远循环下去。这时可以用Ctrl+C退出程序，或者强制结束Python进程。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">----------</span><br><span class="line">## 使用dict和set ##</span><br><span class="line">### dict ###</span><br><span class="line">Python内置了字典：dict的支持，dict全称dictionary，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度。</span><br><span class="line"></span><br><span class="line">举个例子，假设要根据同学的名字查找对应的成绩，如果用list实现，需要两个list：</span><br></pre></td></tr></table></figure></p><p>names = [‘Michael’, ‘Bob’, ‘Tracy’]<br>scores = [95, 75, 85]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">给定一个名字，要查找对应的成绩，就先要在names中找到对应的位置，再从scores取出对应的成绩，list越长，耗时越长。</span><br><span class="line"></span><br><span class="line">如果用dict实现，只需要一个“名字”-“成绩”的对照表，直接根据名字查找成绩，无论这个表有多大，查找速度都不会变慢。用Python写一个dict如下：</span><br></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><p>d = {‘Michael’: 95, ‘Bob’: 75, ‘Tracy’: 85}<br>d[‘Michael’]<br>95<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">为什么dict查找速度这么快？因为dict的实现原理和查字典是一样的。假设字典包含了1万个汉字，我们要查某一个字，一个办法是把字典从第一页往后翻，直到找到我们想要的字为止，这种方法就是在list中查找元素的方法，list越大，查找越慢。</span><br><span class="line"></span><br><span class="line">第二种方法是先在字典的索引表里（比如部首表）查这个字对应的页码，然后直接翻到该页，找到这个字。无论找哪个字，这种查找速度都非常快，不会随着字典大小的增加而变慢。</span><br><span class="line"></span><br><span class="line">dict就是第二种实现方式，给定一个名字，比如&apos;Michael&apos;，dict在内部就可以直接计算出Michael对应的存放成绩的“页码”，也就是95这个数字存放的内存地址，直接取出来，所以速度非常快。</span><br><span class="line"></span><br><span class="line">你可以猜到，这种key-value存储方式，在放进去的时候，必须根据key算出value的存放位置，这样，取的时候才能根据key直接拿到value。</span><br><span class="line"></span><br><span class="line">把数据放入dict的方法，除了初始化时指定外，还可以通过key放入：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>d[‘Adam’] = 67<br>d[‘Adam’]<br>67<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">由于一个key只能对应一个value，所以，多次对一个key放入value，后面的值会把前面的值冲掉：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>d[‘Jack’] = 90<br>d[‘Jack’]<br>90<br>d[‘Jack’] = 88<br>d[‘Jack’]<br>88<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">如果key不存在，dict就会报错：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>d[‘Thomas’]<br>Traceback (most recent call last):<br>  File “<stdin>“, line 1, in <module><br>KeyError: ‘Thomas’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">要避免key不存在的错误，有两种办法，一是**通过in判断key是否存在**：</span><br></pre></td></tr></table></figure></module></stdin></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>‘Thomas’ in d<br>False<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">二是**通过dict提供的get()方法**，如果key不存在，可以返回None，或者自己指定的value：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>d.get(‘Thomas’)<br>d.get(‘Thomas’, -1)<br>-1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意：返回None的时候Python的交互环境不显示结果。</span><br><span class="line"></span><br><span class="line">要删除一个key，用pop(key)方法，对应的value也会从dict中删除：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>d.pop(‘Bob’)<br>75<br>d<br>{‘Michael’: 95, ‘Tracy’: 85}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">请务必注意，dict内部存放的顺序和key放入的顺序是没有关系的。</span><br><span class="line"></span><br><span class="line">和list比较，dict有以下几个特点：</span><br><span class="line"></span><br><span class="line">查找和插入的速度极快，不会随着key的增加而变慢；</span><br><span class="line">需要占用大量的内存，内存浪费多。</span><br><span class="line">而list相反：</span><br><span class="line"></span><br><span class="line">查找和插入的时间随着元素的增加而增加；</span><br><span class="line">占用空间小，浪费内存很少。</span><br><span class="line">所以，dict是用空间来换取时间的一种方法。</span><br><span class="line"></span><br><span class="line">dict可以用在需要高速查找的很多地方，在Python代码中几乎无处不在，正确使用dict非常重要，需要牢记的第一条就是dict的key必须是**不可变对象**。</span><br><span class="line"></span><br><span class="line">这是因为dict根据key来计算value的存储位置，如果每次计算相同的key得出的结果不同，那dict内部就完全混乱了。这个通过key计算位置的算法称为哈希算法（Hash）。</span><br><span class="line"></span><br><span class="line">要保证hash的正确性，作为key的对象就不能变。在Python中，字符串、整数等都是不可变的，因此，可以放心地作为key。而list是可变的，就不能作为key：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>key = [1, 2, 3]<br>d[key] = ‘a list’<br>Traceback (most recent call last):<br>  File “<stdin>“, line 1, in <module><br>TypeError: unhashable type: ‘list’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">### set ###</span><br><span class="line">set和dict类似，也是一组key的集合，但不存储value。由于key不能重复，所以，在set中，没有重复的key。</span><br><span class="line"></span><br><span class="line">要创建一个set，需要提供一个list作为输入集合：</span><br></pre></td></tr></table></figure></module></stdin></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>s = set([1, 2, 3])<br>s<br>{1, 2, 3}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">注意，传入的参数[1, 2, 3]是一个list，而显示的&#123;1, 2, 3&#125;只是告诉你这个set内部有1，2，3这3个元素，显示的顺序也不表示set是有序的。。</span><br><span class="line"></span><br><span class="line">重复元素在set中自动被过滤：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>s = set([1, 1, 2, 2, 3, 3])<br>s<br>{1, 2, 3}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">通过add(key)方法可以添加元素到set中，可以重复添加，但不会有效果：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>s.add(4)<br>s<br>{1, 2, 3, 4}<br>s.add(4)<br>s<br>{1, 2, 3, 4}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">通过remove(key)方法可以删除元素：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>s.remove(4)<br>s<br>{1, 2, 3}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set可以看成数学意义上的无序和无重复元素的集合，因此，两个set可以做数学意义上的交集、并集等操作：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>s1 = set([1, 2, 3])<br>s2 = set([2, 3, 4])<br>s1 &amp; s2<br>{2, 3}<br>s1 | s2<br>{1, 2, 3, 4}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">set和dict的唯一区别仅在于没有存储对应的value，但是，set的原理和dict一样，所以，同样不可以放入可变对象，因为无法判断两个可变对象是否相等，也就无法保证set内部“不会有重复元素”。试试把list放入set，看看是否会报错。</span><br><span class="line"></span><br><span class="line">### 再议不可变对象 ###</span><br><span class="line">上面我们讲了，str是不变对象，而list是可变对象。</span><br><span class="line"></span><br><span class="line">对于可变对象，比如list，对list进行操作，list内部的内容是会变化的，比如：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>a = [‘c’, ‘b’, ‘a’]<br>a.sort()<br>a<br>[‘a’, ‘b’, ‘c’]<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">而对于不可变对象，比如str，对str进行操作呢：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>a = ‘abc’<br>a.replace(‘a’, ‘A’)<br>‘Abc’<br>a<br>‘abc’<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">虽然字符串有个replace()方法，也确实变出了&apos;Abc&apos;，但变量a最后仍是&apos;abc&apos;，应该怎么理解呢？</span><br><span class="line"></span><br><span class="line">我们先把代码改成下面这样：</span><br></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>a = ‘abc’<br>b = a.replace(‘a’, ‘A’)<br>b<br>‘Abc’<br>a<br>‘abc’<br><code>`</code><br>要始终牢记的是，a是变量，而’abc’才是字符串对象！有些时候，我们经常说，对象a的内容是’abc’，但其实是指，a本身是一个变量，它指向的对象的内容才是’abc’</p></blockquote></blockquote></blockquote><p>当我们调用a.replace(‘a’, ‘A’)时，实际上调用方法replace是作用在字符串对象’abc’上的，而这个方法虽然名字叫replace，但却没有改变字符串’abc’的内容。相反，replace方法创建了一个新字符串’Abc’并返回，如果我们用变量b指向该新字符串，就容易理解了，变量a仍指向原有的字符串’abc’，但变量b却指向新字符串’Abc’了</p><p>所以，对于不变对象来说，调用对象自身的任意方法，也不会改变该对象自身的内容。相反，这些方法会创建新的对象并返回，这样，就保证了不可变对象本身永远是不可变的。</p><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>使用key-value存储结构的dict在Python中非常有用，选择不可变对象作为key很重要，最常用的key是字符串。</p><p>tuple虽然是不变对象，但试试把(1, 2, 3)和(1, [2, 3])放入dict或set中，并解释结果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Python是一种计算机编程语言。计算机编程语言和我们日常使用的自然语言有所不同，最大的区别就是，自然语言在不同的语境下有不同的理解，而计算机要根据编程语言执行任务，就必须保证编程语言写出的程序决不能有歧义，所以，任何一种编程语言都有自己的一套语法，编译器或者解释器就是负责
      
    
    </summary>
    
      <category term="编程相关" scheme="https://paradoxallen.github.io/categories/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="python" scheme="https://paradoxallen.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第一个Python程序</title>
    <link href="https://paradoxallen.github.io/10471/"/>
    <id>https://paradoxallen.github.io/10471/</id>
    <published>2017-10-03T16:00:00.000Z</published>
    <updated>2018-06-25T06:35:09.371Z</updated>
    
    <content type="html"><![CDATA[<h3 id="命令行模式"><a href="#命令行模式" class="headerlink" title="命令行模式"></a>命令行模式</h3><p>在Windows开始菜单选择“命令提示符”，就进入到命令行模式，它的提示符类似C:>：</p><h3 id="Python交互模式"><a href="#Python交互模式" class="headerlink" title="Python交互模式"></a>Python交互模式</h3><p>在命令行模式下敲命令python，就看到类似如下的一堆文本输出，然后就进入到Python交互模式，它的提示符是&gt;&gt;&gt;。</p><p>在Python交互模式下输入exit()并回车，就退出了Python交互模式，并回到命令行模式：</p><p>也可以直接通过开始菜单选择Python (command line)菜单项，直接进入Python交互模式，但是输入exit()后窗口会直接关闭，不会回到命令行模式。</p><p>了解了如何启动和退出Python的交互模式，我们就可以正式开始编写Python代码了。</p><p>在写代码之前，请千万不要用“复制”-“粘贴”把代码从页面粘贴到你自己的电脑上。写程序也讲究一个感觉，你需要一个字母一个字母地把代码自己敲进去，在敲代码的过程中，初学者经常会敲错代码：拼写不对，大小写不对，混用中英文标点，混用空格和Tab键，所以，你需要仔细地检查、对照，才能以最快的速度掌握如何写程序。</p><p>在交互模式的提示符&gt;&gt;&gt;下，直接输入代码，按回车，就可以立刻得到代码执行结果。现在，试试输入100+200，看看计算结果是不是300：</p><p>很简单吧，任何有效的数学计算都可以算出来。</p><p>如果要让Python打印出指定的文字，可以用print()函数，然后把希望打印的文字用单引号或者双引号括起来，但不能混用单引号和双引号：</p><p>这种用单引号或者双引号括起来的文本在程序中叫字符串，今后我们还会经常遇到。</p><p>最后，用exit()退出Python，我们的第一个Python程序完成！唯一的缺憾是没有保存下来，下次运行时还要再输入一遍代码。</p><h3 id="命令行模式和Python交互模式"><a href="#命令行模式和Python交互模式" class="headerlink" title="命令行模式和Python交互模式"></a>命令行模式和Python交互模式</h3><p>请注意区分命令行模式和Python交互模式。</p><p>在命令行模式下，可以执行python进入Python交互式环境，也可以执行python hello.py运行一个.py文件。</p><p>执行一个.py文件只能在命令行模式执行。如果敲一个命令python hello.py，看到如下错误：</p><p>错误提示No such file or directory说明这个hello.py在当前目录找不到，必须先把当前目录切换到hello.py所在的目录下，才能正常执行：</p><p>此外，在命令行模式运行.py文件和在Python交互式环境下直接运行Python代码有所不同。Python交互式环境会把每一行Python代码的结果自动打印出来，但是，直接运行Python代码却不会。</p><p>例如，在Python交互式环境下，输入：</p><blockquote><blockquote><blockquote><p>100 + 200 + 300<br>600<br>直接可以看到结果600。</p></blockquote></blockquote></blockquote><p>但是，写一个calc.py的文件，内容如下：</p><p>100 + 200 + 300<br>然后在命令行模式下执行：</p><p>C:\work&gt;python calc.py<br>发现什么输出都没有。</p><p>这是正常的。想要输出结果，必须自己用print()打印出来。把calc.py改造一下：</p><p>print(100 + 200 + 300)<br>再执行，就可以看到结果：</p><p>C:\work&gt;python calc.py<br>600<br>最后，Python交互模式的代码是输入一行，执行一行，而命令行模式下直接运行.py文件是一次性执行该文件内的所有代码。可见，Python交互模式主要是为了调试Python代码用的，也便于初学者学习，它不是正式运行Python代码的环境！</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>在Python交互式模式下，可以直接输入代码，然后执行，并立刻得到结果。</p><p>在命令行模式下，可以直接运行.py文件。</p><hr><h2 id="文本编辑器"><a href="#文本编辑器" class="headerlink" title="文本编辑器"></a>文本编辑器</h2><p>在Python的交互式命令行写程序，好处是一下就能得到结果，坏处是没法保存，下次还想运行的时候，还得再敲一遍。</p><p>所以，实际开发的时候，我们总是使用一个文本编辑器来写代码，写完了，保存为一个文件，这样，程序就可以反复运行了。</p><p>现在，我们就把上次的’hello, world’程序用文本编辑器写出来，保存下来。</p><p>那么问题来了：文本编辑器到底哪家强？</p><p>推荐两款文本编辑器：</p><p>一个是Sublime Text，免费使用，但是不付费会弹出提示框：</p><p><img src="https://i.imgur.com/Hyc7im0.jpg" alt=""></p><p>一个是Notepad++，免费使用，有中文界面：</p><p><img src="https://i.imgur.com/Xhx1QSE.jpg" alt=""></p><p>请注意，用哪个都行，但<strong>是绝对不能用Word和Windows自带的记事本</strong>。Word保存的不是纯文本文件，而记事本会自作聪明地在文件开始的地方加上几个特殊字符（UTF-8 BOM），结果会导致程序运行出现莫名其妙的错误。</p><p>安装好文本编辑器后，输入以下代码：</p><pre><code>print(&apos;hello, world&apos;)</code></pre><p>注意print前面不要有任何空格。然后，选择一个目录，例如C:\work，把文件保存为hello.py，就可以打开命令行窗口，把当前目录切换到hello.py所在目录，就可以运行这个程序了：</p><p><code>C:\work&gt;python hello.pyhello, world</code><br>也可以保存为别的名字，比如first.py，但是必须要以.py结尾，其他的都不行。此外，文件名只能是英文字母、数字和下划线的组合。</p><p>如果当前目录下没有hello.py这个文件，运行python hello.py就会报错：</p><p><code>C:\Users\IEUser&gt;python hello.pypython: can&#39;t open file &#39;hello.py&#39;: [Errno 2] No such file or directory</code><br>报错的意思就是，无法打开hello.py这个文件，因为文件不存在。这个时候，就要检查一下当前目录下是否有这个文件了。如果hello.py存放在另外一个目录下，要首先用cd命令切换当前目录。</p><h3 id="直接运行py文件"><a href="#直接运行py文件" class="headerlink" title="直接运行py文件"></a>直接运行py文件</h3><p>有同学问，能不能像.exe文件那样直接运行.py文件呢？在Windows上是不行的，但是，在Mac和Linux上是可以的，方法是在.py文件的第一行加上一个特殊的注释：</p><pre><code>#!/usr/bin/env python3</code></pre><p>print(‘hello, world’)<br>然后，通过命令给hello.py以执行权限：</p><pre><code>$ chmod a+x hello.py</code></pre><p>就可以直接运行hello.py了，比如在Mac下运行：</p><p><img src="https://i.imgur.com/99hTSjo.png" alt=""></p><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>用文本编辑器写Python程序，然后保存为后缀为.py的文件，就可以用Python直接运行这个程序了。</p><p>Python的交互模式和直接运行.py文件有什么区别呢？</p><p>直接输入python进入交互模式，相当于启动了Python解释器，但是等待你一行一行地输入源代码，每输入一行就执行一行。</p><p>直接运行.py文件相当于启动了Python解释器，然后一次性把.py文件的源代码给执行了，你是没有机会以交互的方式输入源代码的。</p><p>用Python开发程序，完全可以一边在文本编辑器里写代码，一边开一个交互式命令窗口，在写代码的过程中，把部分代码粘到命令行去验证，事半功倍！前提是得有个27’的超大显示器！</p><hr><h2 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h2><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><p>用print()在括号中加上字符串，就可以向屏幕上输出指定的文字。比如输出’hello, world’，用代码实现如下：</p><pre><code>&gt;&gt;&gt; print(&apos;hello, world&apos;)</code></pre><p>print()函数也可以接受多个字符串，用逗号“,”隔开，就可以连成一串输出：</p><pre><code>&gt;&gt;&gt; print(&apos;The quick brown fox&apos;, &apos;jumps over&apos;, &apos;the lazy dog&apos;)The quick brown fox jumps over the lazy dog</code></pre><p>print()会依次打印每个字符串，遇到逗号“,”会输出一个空格</p><p>print()也可以打印整数，或者计算结果：</p><pre><code>&gt;&gt;&gt; print(300)300&gt;&gt;&gt; print(100 + 200)300</code></pre><p>因此，我们可以把计算100 + 200的结果打印得更漂亮一点：</p><pre><code>&gt;&gt;&gt; print(&apos;100 + 200 =&apos;, 100 + 200)100 + 200 = 300</code></pre><p>注意，对于100 + 200，Python解释器自动计算出结果300，但是，’100 + 200 =’是字符串而非数学公式，Python把它视为字符串，请自行解释上述打印结果。</p><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><p>现在，你已经可以用print()输出你想要的结果了。但是，如果要让用户从电脑输入一些字符怎么办？Python提供了一个input()，可以让用户输入字符串，并存放到一个变量里。比如输入用户的名字：</p><pre><code>&gt;&gt;&gt; name = input()Michael</code></pre><p>当你输入name = input()并按下回车后，Python交互式命令行就在等待你的输入了。这时，你可以输入任意字符，然后按回车后完成输入。</p><p>输入完成后，不会有任何提示，Python交互式命令行又回到&gt;&gt;&gt;状态了。那我们刚才输入的内容到哪去了？答案是存放到name变量里了。可以直接输入name查看变量内容：</p><pre><code>&gt;&gt;&gt; name&apos;Michael&apos;</code></pre><p><strong>什么是变量？</strong>请回忆初中数学所学的代数基础知识：</p><p>设正方形的边长为a，则正方形的面积为a x a。把边长a看做一个变量，我们就可以根据a的值计算正方形的面积，比如：</p><p>若a=2，则面积为a x a = 2 x 2 = 4；</p><p>若a=3.5，则面积为a x a = 3.5 x 3.5 = 12.25。</p><p>在计算机程序中，变量不仅可以为整数或浮点数，还可以是字符串，因此，name作为一个变量就是一个字符串。</p><p>要打印出name变量的内容，除了直接写name然后按回车外，还可以用print()函数：</p><pre><code>&gt;&gt;&gt; print(name)Michael</code></pre><p>有了输入和输出，我们就可以把上次打印’hello, world’的程序改成有点意义的程序了：</p><pre><code>name = input()print(&apos;hello,&apos;, name)</code></pre><p>运行上面的程序，第一行代码会让用户输入任意字符作为自己的名字，然后存入name变量中；第二行代码会根据用户的名字向用户说hello，比如输入Michael：</p><pre><code>C:\Workspace&gt; python hello.pyMichaelhello, Michael</code></pre><p>但是程序运行的时候，没有任何提示信息告诉用户：“嘿，赶紧输入你的名字”，这样显得很不友好。幸好，input()可以让你显示一个字符串来提示用户，于是我们把代码改成：</p><pre><code>name = input(&apos;please enter your name: &apos;)print(&apos;hello,&apos;, name)</code></pre><p>再次运行这个程序，你会发现，程序一运行，会首先打印出please enter your name:，这样，用户就可以根据提示，输入名字后，得到hello, xxx的输出：</p><pre><code>C:\Workspace&gt; python hello.pyplease enter your name: Michaelhello, Michael</code></pre><p>每次运行该程序，根据用户输入的不同，输出结果也会不同。</p><p>在命令行下，输入和输出就是这么简单。</p><h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><p>任何计算机程序都是为了执行一个特定的任务，有了输入，用户才能告诉计算机程序所需的信息，有了输出，程序运行后才能告诉用户任务的结果。</p><p>输入是Input，输出是Output，因此，我们把输入输出统称为Input/Output，或者简写为IO。</p><p>input()和print()是在命令行下面最基本的输入和输出，但是，用户也可以通过其他更高级的图形界面完成输入和输出，比如，在网页上的一个文本框输入自己的名字，点击“确定”后在网页上看到输出信息。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;命令行模式&quot;&gt;&lt;a href=&quot;#命令行模式&quot; class=&quot;headerlink&quot; title=&quot;命令行模式&quot;&gt;&lt;/a&gt;命令行模式&lt;/h3&gt;&lt;p&gt;在Windows开始菜单选择“命令提示符”，就进入到命令行模式，它的提示符类似C:&gt;：&lt;/p&gt;
&lt;h3 id=&quot;Py
      
    
    </summary>
    
      <category term="编程相关" scheme="https://paradoxallen.github.io/categories/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="python" scheme="https://paradoxallen.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Python安装编译和运行</title>
    <link href="https://paradoxallen.github.io/25029/"/>
    <id>https://paradoxallen.github.io/25029/</id>
    <published>2017-10-02T16:00:00.000Z</published>
    <updated>2018-06-25T06:32:47.469Z</updated>
    
    <content type="html"><![CDATA[<p>因为Python是跨平台的，它可以运行在Windows、Mac和各种Linux/Unix系统上。在Windows上写Python程序，放到Linux上也是能够运行的。</p><p>要开始学习Python编程，首先就得把Python安装到你的电脑里。安装后，你会得到Python解释器（就是负责运行Python程序的），一个命令行交互环境，还有一个简单的集成开发环境。</p><h3 id="安装Python-3-6"><a href="#安装Python-3-6" class="headerlink" title="安装Python 3.6"></a>安装Python 3.6</h3><p>目前，Python有两个版本，一个是2.x版，一个是3.x版，这两个版本是不兼容的。由于3.x版越来越普及，以最新的Python 3.6版本为基础。请确保你的电脑上安装的Python版本是最新的3.6.x。</p><h3 id="在Mac上安装Python"><a href="#在Mac上安装Python" class="headerlink" title="在Mac上安装Python"></a>在Mac上安装Python</h3><p>如果你正在使用Mac，系统是OS X 10.8~10.10，那么系统自带的Python版本是2.7。要安装最新的Python 3.6，有两个方法：</p><p>方法一：从Python官网下载Python 3.6的安装程序，双击运行并安装；</p><p>方法二：如果安装了Homebrew，直接通过命令<code>brew install python3</code>安装即可。</p><h3 id="在Linux上安装Python"><a href="#在Linux上安装Python" class="headerlink" title="在Linux上安装Python"></a>在Linux上安装Python</h3><p>如果你正在使用Linux，那我可以假定你有Linux系统管理经验，自行安装Python 3应该没有问题，否则，请换回Windows系统。</p><p>对于大量的目前仍在使用Windows的同学，如果短期内没有打算换Mac，就可以继续阅读以下内容。</p><h3 id="在Windows上安装Python"><a href="#在Windows上安装Python" class="headerlink" title="在Windows上安装Python"></a>在Windows上安装Python</h3><p>首先，根据你的Windows版本（64位还是32位）从Python的官方网站下载Python 3.6对应的64位安装程序或32位安装程序，然后，运行下载的EXE安装包：</p><p><img src="https://i.imgur.com/uedLZTE.png" alt=""></p><p>特别要注意勾上<code>Add Python 3.6 to PATH</code>，然后点“Install Now”即可完成安装。</p><h3 id="运行Python"><a href="#运行Python" class="headerlink" title="运行Python"></a>运行Python</h3><p>安装成功后，打开命令提示符窗口，敲入python后，会出现两种情况：</p><p><strong>情况一：</strong></p><p><img src="https://i.imgur.com/vBSk5dP.png" alt=""></p><p>看到上面的画面，就说明Python安装成功！</p><p>你看到提示符<code>&gt;&gt;&gt;</code>就表示我们已经在Python交互式环境中了，可以输入任何Python代码，回车后会立刻得到执行结果。现在，输入<code>exit()</code>并回车，就可以退出Python交互式环境（直接关掉命令行窗口也可以）。</p><p>情况二：得到一个错误：</p><pre><code>‘python’ 不是内部或外部命令，也不是可运行的程序或批处理文件。</code></pre><p><img src="https://i.imgur.com/nwJZ2pJ.png" alt=""></p><p>这是因为Windows会根据一个<code>Path</code>的环境变量设定的路径去查找<code>python.exe</code>，如果没找到，就会报错。如果在安装时漏掉了勾选Add <code>Python 3.6 to PATH</code>，那就要手动把<code>python.exe</code>所在的路径添加到Path中。</p><p>如果你不知道怎么修改环境变量，建议把Python安装程序重新运行一遍，务必记得勾上<code>Add Python 3.6 to PATH</code>。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>学会如何把Python安装到计算机中，并且熟练打开和退出Python交互式环境。</p><p>在Windows上运行Python时，请先启动命令行，然后运行<code>python</code>。</p><p>在Mac和Linux上运行Python时，请打开终端，然后运行<code>python3</code>。</p><p>当我们编写Python代码时，我们得到的是一个包含Python代码的以<code>.py</code>为扩展名的文本文件。要运行代码，就需要Python解释器去执行<code>.py</code>文件。</p><p>由于整个Python语言从规范到解释器都是开源的，所以理论上，只要水平够高，任何人都可以编写Python解释器来执行Python代码（当然难度很大）。事实上，确实存在多种Python解释器。</p><h4 id="CPython"><a href="#CPython" class="headerlink" title="CPython"></a>CPython</h4><p>当我们从Python官方网站下载并安装好Python 3.x后，我们就直接获得了一个官方版本的解释器：CPython。这个解释器是用C语言开发的，所以叫CPython。在命令行下运行python就是启动CPython解释器。</p><p>CPython是使用最广的Python解释器。</p><h4 id="IPython"><a href="#IPython" class="headerlink" title="IPython"></a>IPython</h4><p>IPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强，但是执行Python代码的功能和CPython是完全一样的。好比很多国产浏览器虽然外观不同，但内核其实都是调用了IE。</p><p>CPython用<code>&gt;&gt;&gt;</code>作为提示符，而IPython用<code>In [序号]:</code>作为提示符。</p><h4 id="PyPy"><a href="#PyPy" class="headerlink" title="PyPy"></a>PyPy</h4><p>PyPy是另一个Python解释器，它的目标是执行速度。PyPy采用JIT技术，对Python代码进行动态编译（注意不是解释），所以可以显著提高Python代码的执行速度。</p><p>绝大部分Python代码都可以在PyPy下运行，但是PyPy和CPython有一些是不同的，这就导致相同的Python代码在两种解释器下执行可能会有不同的结果。如果你的代码要放到PyPy下执行，就需要了解PyPy和CPython的不同点。</p><h4 id="Jython"><a href="#Jython" class="headerlink" title="Jython"></a>Jython</h4><p>Jython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行。</p><h4 id="IronPython"><a href="#IronPython" class="headerlink" title="IronPython"></a>IronPython</h4><p>IronPython和Jython类似，只不过IronPython是运行在微软.Net平台上的Python解释器，可以直接把Python代码编译成.Net的字节码。</p><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>Python的解释器很多，但使用最广泛的还是CPython。如果要和Java或.Net平台交互，最好的办法不是用Jython或IronPython，而是通过网络调用来交互，确保各程序之间的独立性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;因为Python是跨平台的，它可以运行在Windows、Mac和各种Linux/Unix系统上。在Windows上写Python程序，放到Linux上也是能够运行的。&lt;/p&gt;
&lt;p&gt;要开始学习Python编程，首先就得把Python安装到你的电脑里。安装后，你会得到Pyth
      
    
    </summary>
    
      <category term="编程相关" scheme="https://paradoxallen.github.io/categories/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="python" scheme="https://paradoxallen.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Python前言与目录</title>
    <link href="https://paradoxallen.github.io/45/"/>
    <id>https://paradoxallen.github.io/45/</id>
    <published>2017-09-30T16:00:00.000Z</published>
    <updated>2018-06-25T06:32:47.465Z</updated>
    
    <content type="html"><![CDATA[<p>Python是一种计算机程序设计语言。你可能已经听说过很多种流行的编程语言，比如非常难学的C语言，非常流行的Java语言，适合初学者的Basic语言，适合网页编程的JavaScript语言等等。</p><p>那Python是一种什么语言？</p><p>首先，我们普及一下编程语言的基础知识。用任何编程语言来开发程序，都是为了让计算机干活，比如下载一个MP3，编写一个文档等等，而计算机干活的CPU只认识机器指令，所以，尽管不同的编程语言差异极大，最后都得“翻译”成CPU可以执行的机器指令。而不同的编程语言，干同一个活，编写的代码量，差距也很大。</p><p>比如，完成同一个任务，C语言要写1000行代码，Java只需要写100行，而Python可能只要20行。</p><p>所以Python是一种相当高级的语言。</p><p>你也许会问，代码少还不好？代码少的代价是运行速度慢，C程序运行1秒钟，Java程序可能需要2秒，而Python程序可能就需要10秒。</p><p>那是不是越低级的程序越难学，越高级的程序越简单？表面上来说，是的，但是，在非常高的抽象计算中，高级的Python程序设计也是非常难学的，所以，高级程序语言不等于简单。</p><p>但是，对于初学者和完成普通任务，Python语言是非常简单易用的。连Google都在大规模使用Python，你就不用担心学了会没用。</p><p>用Python可以做什么？可以做日常任务，比如自动备份你的MP3；可以做网站，很多著名的网站包括YouTube就是Python写的；可以做网络游戏的后台，很多在线游戏的后台都是Python开发的。总之就是能干很多很多事啦。</p><p>Python当然也有不能干的事情，比如写操作系统，这个只能用C语言写；写手机应用，只能用Swift/Objective-C（针对iPhone）和Java（针对Android）；写3D游戏，最好用C或C++。</p><p>Python是著名的“龟叔”Guido van Rossum在1989年圣诞节期间，为了打发无聊的圣诞节而编写的一个编程语言。</p><p>现在，全世界差不多有600多种编程语言，但流行的编程语言也就那么20来种。如果你听说过TIOBE排行榜，你就能知道编程语言的大致流行程度。这是最近10年最常用的10种编程语言的变化图：</p><p><img src="https://i.imgur.com/Y2lJyTv.png" alt=""></p><p>总的来说，这几种编程语言各有千秋。C语言是可以用来编写操作系统的贴近硬件的语言，所以，C语言适合开发那些追求运行速度、充分发挥硬件性能的程序。而Python是用来编写应用程序的高级编程语言。</p><p>当你用一种语言开始作真正的软件开发时，你除了编写代码外，还需要很多基本的已经写好的现成的东西，来帮助你加快开发进度。比如说，要编写一个电子邮件客户端，如果先从最底层开始编写网络协议相关的代码，那估计一年半载也开发不出来。高级编程语言通常都会提供一个比较完善的基础代码库，让你能直接调用，比如，针对电子邮件协议的SMTP库，针对桌面环境的GUI库，在这些已有的代码库的基础上开发，一个电子邮件客户端几天就能开发出来。</p><p>Python就为我们提供了非常完善的基础代码库，覆盖了网络、文件、GUI、数据库、文本等大量内容，被形象地称作“内置电池（batteries included）”。用Python开发，许多功能不必从零编写，直接使用现成的即可。</p><p>除了内置的库外，Python还有大量的第三方库，也就是别人开发的，供你直接使用的东西。当然，如果你开发的代码通过很好的封装，也可以作为第三方库给别人使用。</p><p>许多大型网站就是用Python开发的，例如YouTube、Instagram，还有国内的豆瓣。很多大公司，包括Google、Yahoo等，甚至NASA（美国航空航天局）都大量地使用Python。</p><p>龟叔给Python的定位是“优雅”、“明确”、“简单”，所以Python程序看上去总是简单易懂，初学者学Python，不但入门容易，而且将来深入下去，可以编写那些非常非常复杂的程序。</p><p>总的来说，Python的哲学就是简单优雅，尽量写容易看明白的代码，尽量写少的代码。如果一个资深程序员向你炫耀他写的晦涩难懂、动不动就几万行的代码，你可以尽情地嘲笑他。</p><p><strong>那Python适合开发哪些类型的应用呢？</strong></p><p>首选是网络应用，包括网站、后台服务等等；</p><p>其次是许多日常需要的小工具，包括系统管理员需要的脚本任务等等；</p><p>另外就是把其他语言开发的程序再包装起来，方便使用。</p><p><strong>最后说说Python的缺点。</strong></p><p>任何编程语言都有缺点，Python也不例外。优点说过了，那Python有哪些缺点呢？</p><p>第一个缺点就是运行速度慢，和C程序相比非常慢，因为Python是解释型语言，你的代码在执行时会一行一行地翻译成CPU能理解的机器码，这个翻译过程非常耗时，所以很慢。而C程序是运行前直接编译成CPU能执行的机器码，所以非常快。</p><p>但是大量的应用程序不需要这么快的运行速度，因为用户根本感觉不出来。例如开发一个下载MP3的网络应用程序，C程序的运行时间需要0.001秒，而Python程序的运行时间需要0.1秒，慢了100倍，但由于网络更慢，需要等待1秒，你想，用户能感觉到1.001秒和1.1秒的区别吗？这就好比F1赛车和普通的出租车在北京三环路上行驶的道理一样，虽然F1赛车理论时速高达400公里，但由于三环路堵车的时速只有20公里，因此，作为乘客，你感觉的时速永远是20公里。</p><p><img src="https://i.imgur.com/sSnnJo1.jpg" alt=""></p><p>第二个缺点就是代码不能加密。如果要发布你的Python程序，实际上就是发布源代码，这一点跟C语言不同，C语言不用发布源代码，只需要把编译后的机器码（也就是你在Windows上常见的xxx.exe文件）发布出去。要从机器码反推出C代码是不可能的，所以，凡是编译型的语言，都没有这个问题，而解释型的语言，则必须把源码发布出去。</p><p>这个缺点仅限于你要编写的软件需要卖给别人挣钱的时候。好消息是目前的互联网时代，靠卖软件授权的商业模式越来越少了，靠网站和移动应用卖服务的模式越来越多了，后一种模式不需要把源码给别人。</p><p>再说了，现在如火如荼的开源运动和互联网自由开放的精神是一致的，互联网上有无数非常优秀的像Linux一样的开源代码，我们千万不要高估自己写的代码真的有非常大的“商业价值”。那些大公司的代码不愿意开放的更重要的原因是代码写得太烂了，一旦开源，就没人敢用他们的产品了。</p><p><img src="https://i.imgur.com/NcfLHCr.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Python是一种计算机程序设计语言。你可能已经听说过很多种流行的编程语言，比如非常难学的C语言，非常流行的Java语言，适合初学者的Basic语言，适合网页编程的JavaScript语言等等。&lt;/p&gt;
&lt;p&gt;那Python是一种什么语言？&lt;/p&gt;
&lt;p&gt;首先，我们普及一下
      
    
    </summary>
    
      <category term="编程相关" scheme="https://paradoxallen.github.io/categories/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="python" scheme="https://paradoxallen.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Python中的按位运算</title>
    <link href="https://paradoxallen.github.io/1340/"/>
    <id>https://paradoxallen.github.io/1340/</id>
    <published>2017-09-29T16:00:00.000Z</published>
    <updated>2018-06-24T06:32:35.344Z</updated>
    
    <content type="html"><![CDATA[<p>位操作是程序设计中对位模式或二进制数的一元和二元操作. 在许多古老的微处理器上, 位运算比加减运算略快, 通常位运算比乘除法运算要快很多. 在现代架构中, 情况并非如此:位运算的运算速度通常与加法运算相同(仍然快于乘法运算).</p><p>简单来说，<strong>按位运算就把数字转换为机器语言——二进制的数字来运算的一种运算形式</strong>。在计算机系统中，数值一律用补码来表示(存储)。</p><p>Python中的按位运算符有：<strong>左移运算符（&lt;&lt;），右移运算符（&gt;&gt;）,按位与（&amp;），按位或（|），按位翻转（～）</strong>。这些运算符中只有按位翻转运算符是单目运算符，其他的都是双目运算符。</p><h4 id="按位与-amp"><a href="#按位与-amp" class="headerlink" title="按位与    &amp;"></a>按位与    &amp;</h4><p><strong>举例：</strong><br><strong>3&amp;5</strong><br>解法：3的二进制补码是 11,  5的是101, 3&amp;5也就是011&amp;101,先看百位(其实不是百位,这样做只是便于理解) 一个0一个1,根据(1&amp;1=1，1&amp;0=0，0&amp;0=0，0&amp;1=0)可知百位应该是1,同样十位上的数字1&amp;0=0,个位上的数字1&amp;1=1,因此最后的结果是1.(这之后本来应该还有一步,因为我们现在得到的数值只是所求答案的补码,但是因为正数的补码即是它本身,所以就省略了。不过,下面的例子就不能省略最后这一步了).    </p><p><strong>-1&amp;-2</strong><br>解法:-1的补码是11111111,  -2的补码是11111110, 11111111&amp;11111110得到的结果是:11111110,这个是补码,再转化位原码为100000010 (负数转换位原码的方法是减一取反),最后转换为十进制是 -2.</p><p><strong>-2&amp;6</strong><br>解法:-2的补码是11111110,  6的补码是110,   11111110&amp;110,也就是11111110&amp;00000110(这样写的目的是让初学者能够更好理解按位运算),按照上面的方法得到的结果是:110,转化位十进制就是6.</p><p><strong>小技巧</strong>：利用按位与可以将任意二进制数的最后一位变为0,即就是X&amp;0.</p><h4 id="按位并"><a href="#按位并" class="headerlink" title="按位并    |"></a>按位并    |</h4><p><strong>举例： </strong></p><p><strong>4|7</strong><br>解法：按位并的计算规律和按位与的很相似，只不过换了逻辑运算符，并的规律是： 1|1=1 ,1 |0=1, 0|0=0.   4|7转换位二进制就是:100|111=111.  二进制111即为十进制的7.<br><strong>小技巧</strong>：利用按位并可以将任意二进制数的最后一位变为1,即就是X|1.</p><h4 id="按位异或"><a href="#按位异或" class="headerlink" title="按位异或    ^"></a>按位异或    ^</h4><p><strong>方法</strong>:  对位相加,<strong>特别要注意的是不进位. </strong></p><p><strong>举例：</strong> </p><p><strong>2^5</strong><br>解法:10^101=111,二进制111得到十进制的结果是7.</p><p><strong>1^1</strong><br>解法:1+1=0.(本来二进制1+1=10,但不能进位,所以结果是0) </p><p><strong>-3^4  </strong><br>解法: -3的补码是11111101,4的补码是100 (也即00000100),11111101^00000100=11111101,补码                               11111101转为原码是1000111,即十进制的-7.</p><h4 id="按位翻转"><a href="#按位翻转" class="headerlink" title="按位翻转  ~"></a>按位翻转  ~</h4><p><strong>方法:</strong>   将二进制数+1之后乘以-1,x的按位翻转是-(x+1) . 注意,按位运算符是单目运算符.  -9 ,  1+~4是正确的,5~3就不对了.</p><p><strong>举例:</strong><br><strong>~3</strong><br>解法:3的二进制是11, -(11+1)=-100B=-4D. (注:B和D分别表示二进制和十进制).<br><strong>~-2</strong><br>解法:   -  (-10+1)  =1</p><h4 id="左移运算符-lt-lt"><a href="#左移运算符-lt-lt" class="headerlink" title="左移运算符  &lt;&lt;"></a>左移运算符  &lt;&lt;</h4><p><strong>方法: </strong>   X&lt;&lt;N 将一个数字X所对应的二进制数向左移动N位.</p><p><strong>举例:</strong></p><p><strong>3&lt;&lt;2</strong><br>解法:11向左移动两位变为1100,即12 .</p><h4 id="右移动运算符-gt-gt"><a href="#右移动运算符-gt-gt" class="headerlink" title="右移动运算符  &gt;&gt;"></a>右移动运算符  &gt;&gt;</h4><p><strong>方法:</strong>    X&gt;&gt;N 将一个数字X所对应的二进制数向右移动N位.</p><p><strong>举例: </strong></p><p><strong>3&gt;&gt;2</strong><br>解法:11向右移动两位变为0.</p><p><strong>10&gt;&gt;1</strong><br>解法:10的二进制是1010,向右边移动一位是101,即5.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;位操作是程序设计中对位模式或二进制数的一元和二元操作. 在许多古老的微处理器上, 位运算比加减运算略快, 通常位运算比乘除法运算要快很多. 在现代架构中, 情况并非如此:位运算的运算速度通常与加法运算相同(仍然快于乘法运算).&lt;/p&gt;
&lt;p&gt;简单来说，&lt;strong&gt;按位运
      
    
    </summary>
    
      <category term="编程相关" scheme="https://paradoxallen.github.io/categories/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="python" scheme="https://paradoxallen.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (11) Large Scale Machine Learning</title>
    <link href="https://paradoxallen.github.io/6245/"/>
    <id>https://paradoxallen.github.io/6245/</id>
    <published>2017-09-28T16:00:00.000Z</published>
    <updated>2018-06-21T16:14:54.319Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Big-Data"><a href="#1-Big-Data" class="headerlink" title="1. Big Data"></a>1. Big Data</h3><p>海量的数据作为训练样本，<strong>“low bias algo + big data”</strong>基本都能让算法精确度更高。但是随之而来的就是极大的计算量。</p><p>正如<a href="https://paradoxallen.github.io/9059/">《Machine Learning Advice》</a>中所提到的，如果算法是 high bias，那么1000个样本与100,000,000个样本效果都不会有变化</p><p>那如何避免一开始就陷入了 high bias 或者因为自己的选择的方法、编程的细节错误所浪费大量时间呢？</p><p>我们应该在海量数据中<strong>选取一部分</strong>尝试。例如，随机选择 100,000,000个样本中的1000个，使用各种学习方法选择在这1000个采样学习中表现最好的一个或几个方法，进行海量数据的学习</p><p>尝试之后，我们即将进行真正的海量数据学习。学习过程中，我们经常会用到梯度下降法。我们以线性回归为例（<strong>任何使用到梯度下降法的算法：逻辑回归、神经网络等，都可以举一反三适用</strong>），参数θ的update函数为：<br><img src="https://i.imgur.com/2PwCkzy.png" alt=""></p><p>公式推导内容见<a href="https://paradoxallen.github.io/58080/">《Linear Regression》</a></p><p>上式中有一处求和<img src="https://i.imgur.com/4HlhA0F.png" alt="">，面对海量数据时会消耗大量时间，而且对每一个样本都会计算一次。这种计算模式称为<strong>“批处理”(batch)</strong>，面对海量数据，显然批处理模式并不适合。以下介绍两种海量数据模型中经常使用的方法</p><h3 id="2-随机梯度下降（Stochastic-Gradient-Descent）"><a href="#2-随机梯度下降（Stochastic-Gradient-Descent）" class="headerlink" title="2. 随机梯度下降（Stochastic Gradient Descent）"></a>2. 随机梯度下降（Stochastic Gradient Descent）</h3><p>介绍随机梯度下降方法之前，我们引入一个新的表达式：<img src="https://i.imgur.com/xdwKU3A.png" alt=""> 这代替了原本的 J(θ) 成为新的代价函数</p><p>根据新的代价函数，我们会得到新的参数θ的update函数：<br><img src="https://i.imgur.com/0qISHJa.png" alt=""></p><p>仔细观察这个update函数，是不是发现求和过程没有了？<br>批处理中，参数的改变需要当前所有样本共同作用，达到整体最优。但是随机方法中，<strong>每次仅关注当前单个样本的最优作用。这种做法会导致最优值的反复，但是却极大提高了效率</strong></p><p>随机梯度下降的具体流程为：<br><img src="https://i.imgur.com/ASzT2Zw.png" alt=""></p><p>因为随机梯度下降会导致最优值的反复，所以有时上图中的 Repeat 会进行1~10次。而且随机梯度下降方法的收敛是随机的，只能保证逐渐向最优值徘徊，而不会真正到达最优值。例如下图：<br><img src="https://i.imgur.com/QNkfNRs.png" alt=""></p><p>如果我们希望达到最优值，应该怎么办呢？可以通过逐渐缩小学习步长α，以求最终的收敛范围逐渐减小到最优值。可以使用：<br><img src="https://i.imgur.com/pGRoyMs.png" alt="">来逐渐缩小学习步长</p><h3 id="3-小规模批处理（Mini-batch）"><a href="#3-小规模批处理（Mini-batch）" class="headerlink" title="3. 小规模批处理（Mini-batch）"></a>3. 小规模批处理（Mini-batch）</h3><p>有没有方法兼顾批处理方法，与随机梯度下降方法的优势？就是Mini-batch Gradient Descent。基本思想是：不是批处理的每次处理所有样本，也不是随机方法每次单个样本，而是每次处理b个样本（1&lt;b≪m）</p><p>假设有1000个样本，b=10。算法的具体执行过程为：<br><img src="https://i.imgur.com/sNBITho.png" alt=""></p><p>实际计算中，小规模批处理方法，可能会比随机梯度下降方法更快！因为<strong>不同组的b个样本可以进行并行计算</strong>（一次并行m个样本计算量太大）。但这种方法多引入了一个参数b，需要更多的调试，算是个缺点</p><h3 id="4-Big-Data-判断收敛"><a href="#4-Big-Data-判断收敛" class="headerlink" title="4. Big Data 判断收敛"></a>4. Big Data 判断收敛</h3><p>批处理方法中，如何判断算法是否收敛？绘制 “Jtrain(θ)−#iterations” 的走势图（见<a href="https://paradoxallen.github.io/58080/">《Linear Regression》</a>，Jtrain(θ)是基于所有样本的和。走势逐渐降低即为收敛</p><p>随机梯度下降方法中，再去计算所有样本的Jtrain(θ)显然不合适，我们转而记录<img src="https://i.imgur.com/BgFkQHn.png" alt="">。例如，每1000个样本各自的<img src="https://i.imgur.com/BgFkQHn.png" alt="">求和后平均，记录这个<strong>平均误差</strong>后再更新θ。</p><p>最后，绘制“平均误差”与迭代次数的关系图。因为收敛是随机的，可能出现噪声，只要<strong>保证整体趋势是逐渐下降的即为收敛</strong></p><p>接下来我们看一下以下几种“平均误差”与迭代次数的关系图，看看有什么问题：<br><img src="https://i.imgur.com/CmhZn1V.png" alt=""></p><p>左上角的图是一种收敛的情况，但是红色曲线的α更小，虽然学习速度较慢，但是却收敛到更接近最优值的区域（更小误差）</p><p>右上角的图是一种收敛的情况，其中红色曲线的取“平均误差”的范围更大，是5000个样本取一次，所以噪声更少更平滑</p><p>左下角的图，因为蓝色曲线噪声过大，看不出是否收敛，建议增大取“平均误差”的范围</p><p>右下角的图，抱歉，完全不收敛。建议使用更小的α再尝试，如果还是不收敛就可能需要选择其他的算法或者增加特征？</p><h3 id="5-Online-Learning"><a href="#5-Online-Learning" class="headerlink" title="5. Online Learning"></a>5. Online Learning</h3><p>试想一种场景：一家新闻网站不断根据不断涌入的新用户的喜好以及老用户的兴趣改变，来定义头条新闻的内容，是不是还要保存这种海量数据呢？</p><p>这需要极大的空间，并且每次都基于所有样本进行学习，那么学习的时间还赶不上产生新样本的时间</p><p>如果使用随机梯度下降方法，每次仅仅关注当前样本的梯度下降，并且<strong>同一个样本仅学习一次</strong>（上文“随机梯度下降”流程图中 Repeat 次数为1），是不是就能解决这个问题。这就是online learning</p><p>online learning 思想可以<strong>适用于无限的数据流</strong>，尤其适用于不断更新中的互联网大数据上的机器学习算法。</p><p>并且，这种方法对于新事物有着极强的适应性，因为每次更新都是基于当前样本，历史样本的作用随着迭代的进行会逐渐失去。因此非常适用于<strong>互联网大数据的实时更新</strong></p><h3 id="6-Data-Parallelism"><a href="#6-Data-Parallelism" class="headerlink" title="6. Data Parallelism"></a>6. Data Parallelism</h3><p>最后一部分是对于并行计算的简介。试想一下，如果我要求4亿个样本某个特征的和，仅在一台机器上求是很慢的。但是，如果我将它分散在四台机器上求解，最后使用一台机器汇总这四台机器的求解结果，是不是快了很多？除去数据传输耗费的时间，是不是快了接近4倍？其中，<strong>分散</strong>的过程我们称为<strong>Map</strong>，而<strong>汇总</strong>的过程我们称为<strong>Reduce</strong></p><p>不仅对于多台机器，同一台机器如果有着多个计算核（CPU），同样可以适用。形象化的示意图如下：<br><img src="https://i.imgur.com/vsnpNZi.png" alt=""><br><img src="https://i.imgur.com/TsFW2LW.png" alt=""></p><p>本节仅仅举了加法求和的例子。但实际情况中，只要是无数据相关性的计算，也就是前一部分的计算结果不作为后一部分的计算输入，两个部分的计算是彼此独立的，都可以并行计算</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (10) Recommender Systems</title>
    <link href="https://paradoxallen.github.io/62349/"/>
    <id>https://paradoxallen.github.io/62349/</id>
    <published>2017-09-24T16:00:00.000Z</published>
    <updated>2018-06-20T16:46:31.977Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-推荐系统"><a href="#1-推荐系统" class="headerlink" title="1. 推荐系统"></a>1. 推荐系统</h3><p><strong>推荐系统</strong>是机器学习的重要应用之一，但是学术界关注较少。但是它的确出现在生活中的方方面面。例如我们上豆瓣、网易云音乐（没错，叫我文青）等，总会有根据你的个人喜好来推荐你可能喜欢的电影、音乐的功能，就是典型的推荐系统。</p><p>我们以外国豆瓣为例，见下图（问号代表尚未评分）：<br><img src="https://i.imgur.com/to4tI0H.png" alt=""></p><p>另外我们需要设计几个变量：</p><p>nu=#users数目；</p><p>nu=#movies数目；</p><p>r(i,j)代表用户j是否已经对电影i评分，取值为0或1；</p><p>y(i,j)代表用户j是否对电影i的具体评分，取值范围0~5</p><p>一般电影评分是<strong>基于内容（content-based）</strong>，取值范围为0~1。例如某部电影的浪漫成分为0.3，动作成分为0.8，这部电影就主要是动作片，掺杂一些浪漫成分。“浪漫”特征可以设定为x1，“动作”特征可以设定为x2，共有n个特征。</p><p>如何寻找相似的电影呢？就是选取“距离”最短的另一部电影，即<img src="https://i.imgur.com/OirBtIk.png" alt=""></p><p>很容易地，我们会想到线性回归的方法。这个方法的前提是<strong>已知每部电影属于每种特征的得分</strong>即x(1),x(2),…,x(m)。</p><p>若每个用户基于特征可以有一个θ(j)，则用户j对电影i的评分为<img src="https://i.imgur.com/6FNr4VE.png" alt="">。</p><p>根据线性回归，我们可以得到优化目标为：<br><img src="https://i.imgur.com/sQY0vHd.png" alt=""></p><p>这里需要注意的是，为了简化系统，我们去除了求和符号之前的1/2m，而是1/2。这样并不影响结果。 </p><p>对上图中的优化目标，利用梯度下降的办法求出 <strong>θ 的最优值</strong>（如果对这一部分不熟悉，建议查看<a href="https://paradoxallen.github.io/58080/">《Linear Regression》</a><br><img src="https://i.imgur.com/NzShBbD.png" alt=""></p><h3 id="2-换个角度思考？"><a href="#2-换个角度思考？" class="headerlink" title="2. 换个角度思考？"></a>2. 换个角度思考？</h3><p>上一部分，我们是假设已知每部电影在每种特征上的得分。如果我们已知有两种特征：<strong>romance和action</strong>，但是不知道每种电影在哪种特征上得分多少。在这里，我们的求解对象变为了x(i).</p><p>但是此时我们已知得知用户对于每种特征的喜欢程度，即已知θ(j)。同时已知y(i,j)，求解x(i)。因此我们的优化目标为：<br><img src="https://i.imgur.com/bczIp7x.png" alt=""></p><p>同理，也可以使用梯度下降法进行最优值求解。</p><p>上一部分的求解过程是<strong>x⇒θ</strong>，这一部分的求解过程是<strong>θ⇒x</strong>。</p><p>如果这两个过程交替进行，是不是就可以达到整体的最优值？答案是肯定的。</p><p>这就是下一部分<strong>“协同过滤”</strong>的基本思想，但是具体实施过程有待改进。</p><h3 id="3-协同过滤（Collaborative-Filtering）"><a href="#3-协同过滤（Collaborative-Filtering）" class="headerlink" title="3. 协同过滤（Collaborative Filtering）"></a>3. 协同过滤（Collaborative Filtering）</h3><p>对于求解过程x⇒θ，与求解过程θ⇒x 交替进行，以求达到整体的最优值。协同过滤基本思想是这样的，但是能不能将一切置于同一个公式下？</p><p>查看下图，红色方框中是两个优化目标的求解部分中相同的部分，即使求和顺序不同。而红框之外的部分分别加到新的优化目标中<br><img src="https://i.imgur.com/MQKWsqs.png" alt=""></p><p>这里，我们去除掉x0,θ0这些恒为1的变量（其实我忍很久了），让学习更加灵活。</p><p>因此，协同过滤的具体算法流程（第一步类似于神经网络，对系统中的参数首先都进行随机化；后两步完全类似于线性回归）。因为我们不再需要k=0的情况，所以不需要单独区分：<br><img src="https://i.imgur.com/c1gFkK7.png" alt=""></p><h3 id="4-均值归一化（Mean-Normalization）"><a href="#4-均值归一化（Mean-Normalization）" class="headerlink" title="4. 均值归一化（Mean Normalization）"></a>4. 均值归一化（Mean Normalization）</h3><p>如果有另一个新用户 Eve，她对于任何电影都尚未评分，我们如何给她推荐呢？很简单，推荐那些大部分都觉得高分的电影呗。</p><p>但是，如果直接利用我们之前的方法求解以下情形：<br><img src="https://i.imgur.com/te6xHCZ.png" alt=""></p><p>为了归一化，肯定最后 Eve 的 θ(5) 一定是全0向量。因此，我们需要对原先的方法采取一些<strong>预操作</strong>，来避免出现全 0 的预测向量，而是让它变成每部电影的平均得分。</p><p>方法也十分简单，就是对每部电影求出一个平均值（只统计那些对该部电影打过分数的样本），然后将每个人对每部电影的得分减去这部电影的平均值，然后进行<strong>协同过滤计算</strong>。但是，最后的预测得分公式也需要小小的改变：<img src="https://i.imgur.com/QfYFAWe.png" alt="">（其中μi表示电影i的平均得分）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (9) Anomaly Detection</title>
    <link href="https://paradoxallen.github.io/39500/"/>
    <id>https://paradoxallen.github.io/39500/</id>
    <published>2017-09-18T16:00:00.000Z</published>
    <updated>2018-06-19T17:00:26.256Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-异常检测-amp-高斯分布"><a href="#1-异常检测-amp-高斯分布" class="headerlink" title="1. 异常检测 &amp; 高斯分布"></a>1. 异常检测 &amp; 高斯分布</h3><p><strong>异常检测是一种介于监督学习与非监督学习之间的机器学习方式</strong>。一般用于检查大规模正品中的<strong>小规模</strong>次品。</p><p>根据单个特征量的概率分布，从而求出某个样本正常的概率，若正常的概率小于阈值，即 p(x)&lt;ϵ 视其为异常（次品）。正品与次品的 label 值 y 定义为：<br><img src="https://i.imgur.com/o9Rt4yM.png" alt=""></p><p>如果某个样本由x1,x2两个变量决定，如下图红色叉所示：<br><img src="https://i.imgur.com/ylEk8oe.png" alt=""></p><p>同一个圆圈内部，表示的是成为正品的概率相同。越中心的圆圈内部正品率越高。越外层的圆圈内正品率越低。</p><p>异常检测一般<strong>将每个特征量的分布假设为正态分布</strong>（如果特征量与正态分布差距很大，之后我们会提到方法对其进行修正）。</p><p>为什么是正态分布？因为在生产与科学实验中发现，很多随机变量的概率分布都可以近似地用正态分布来描述（猜测正确的概率更大）。因</p><p>此，以下稍微介绍一下正态分布的基础知识，如果很熟悉的同学可以略过这部分。</p><p><strong>正态分布（高斯分布）</strong>，包含两个参数：均值μ（分布函数取峰值时所对应横坐标轴的值），与方差σ^2（标准差为σ，控制分布函数的“胖瘦”）。</p><p>如果变量 x 满足于正态分布，将其记为 x∼N(μ,σ^2)。而取某个 x 的对应正品概率为：<img src="https://i.imgur.com/RhxicSW.png" alt=""></p><p>均值 <img src="https://i.imgur.com/bzn9fbc.png" alt="">，方差<img src="https://i.imgur.com/88vXqR7.png" alt=""></p><p>正态分布曲线与坐标轴之间的面积（即函数积分）恒定为 1，因此<strong>“高”曲线必然“瘦”，“矮”曲线必然“胖”</strong>：<br><img src="https://i.imgur.com/QUEyjr8.png" alt=""></p><p>由图可知，标准差σ控制着分布函数的“胖瘦”。原因是因为<strong>标准差有关的取值范围，有着固定的分布概率（积分）</strong>：<br><img src="https://i.imgur.com/GPZPvbN.png" alt=""></p><h3 id="2-异常检测算法流程："><a href="#2-异常检测算法流程：" class="headerlink" title="2. 异常检测算法流程："></a>2. 异常检测算法流程：</h3><p>我们拥有一组训练数据：x(1),x(2),…,x(m)，每个样本有着 m 个特征量 x1,x2,…,xn </p><p>将每个样本投影到不同的特征的坐标轴上，基于样本得到各个特征的概率正态分布曲线</p><p>假设各个特征的概率是独立的，因此单个样本的异常概率为<br><img src="https://i.imgur.com/B4JPDaw.png" alt=""></p><p>各个特征的均值为<img src="https://i.imgur.com/v3TxnSr.png" alt="">，方差为 <img src="https://i.imgur.com/oO4VpDF.png" alt=""></p><p>如果我们有着 10000 个正品样本，以及 20 个次品样本，我们应该这样区分<strong>训练集、交叉验证集，与测试集</strong>： </p><p><strong>训练集</strong>：6000个正品作为训练集（不包括次品样本）</p><p><strong>交叉验证集</strong>：2000 个正品样本 + 10 个次品样本。用以确定次品概率的阈值 ϵ</p><p><strong>测试集</strong>：2000 个正品样本 + 10 个次品样本。用以判断算法的检测效果</p><p>特别注意，因为使用异常检测的样本集合一般都是偏斜严重的（正品样本远远多于次品样本）。</p><p>因此，需要在<a href="https://paradoxallen.github.io/9059/">Machine Learning Advice</a>中提到的 <strong>precision/recall/F-score</strong> 来进行判断算法的检测效果。</p><h3 id="3-异常检测-VS-监督学习"><a href="#3-异常检测-VS-监督学习" class="headerlink" title="3. 异常检测 VS. 监督学习"></a>3. 异常检测 VS. 监督学习</h3><p>监督学习方法与异常检测类似，处理对象都是一堆有 label 的样本，并且目标都是预测新样本的类别。那么什么时候使用监督学习的方法？什么时候使用异常检测的方法？</p><p>大体上，区别如下：<br>样本比例：异常检测适用于<strong>正样本（y=1，即次品）个数远远小于负样本</strong>的个数的情况；监督学习适用于<strong>正负样本个数都非常多</strong>的情况</p><p>异常规律：如果<strong>正样本（y=1，即次品）有着难以预测的模式，引起正样本的原因有很多很多</strong>，适用于异常检测；但是如果<strong>正样本有着固定的规律</strong>，比如感冒（病因已被研究透彻），可以尝试基于大量的样本使用监督学习的方法建立模式进行判断</p><h3 id="4-特征选择"><a href="#4-特征选择" class="headerlink" title="4. 特征选择"></a>4. 特征选择</h3><p>绝大多数情况下，特征量符合正态分布的分布情况。但如果特征的分布极端不符合，我们只能对其进行一些处理，以产生全新的特征来适用于异常检测算法。例如：<br><img src="https://i.imgur.com/LuwgqTb.png" alt=""></p><p>此时，我们有着变换后的特征变量：<strong>xnew=log(x1)</strong></p><p>或者，一般情况下我们希望<strong>正品的 p(x) 很大，次品的 p(x) 很小</strong>。也就是说，<strong>在异常情况下某些特征应该变得极大或极小</strong>（正态分布中对于极大值或者极小值的对应概率都是极小的，所以整个样本的正品概率相乘会很容易满足 p(x)&lt;ϵ）：例如创建新变量<img src="https://i.imgur.com/qP5XSx6.png" alt="">，进一步放大了值增大或减小的程度。</p><h3 id="5-Multivariate-Gaussion"><a href="#5-Multivariate-Gaussion" class="headerlink" title="5. Multivariate Gaussion"></a>5. Multivariate Gaussion</h3><p>如果一个样本有着多种特征，那么整体的正品概率可以按照以上提到的，视每个变量为互相独立然后各自概率相乘进行求解（我们称之为 <strong>original model</strong>）。</p><p>但是，如果出现了下图这种正相关（负相关）极强的特征量，同心圆内部的正品概率必然不同，显然不合适了：<br><img src="https://i.imgur.com/PQ9GGVM.png" alt=""></p><p>我们希望原本的同心圆可以更扁，可以变换方向，例如上图的蓝色椭圆。</p><p>此时，我们可以利用协方差矩阵，构造全新的多变量正态分布公式。此时我们用到的不再是方差 σ2，而是<strong>协方差矩阵</strong><img src="https://i.imgur.com/TOlEvpS.png" alt="">。<strong>多变量正态分布的概率公式</strong>为：<br><img src="https://i.imgur.com/qyUGsLg.png" alt="">，其中|Σ|表示协方差矩阵的行列式。</p><p>协方差矩阵与均值，对概率分布图的影响如下：<br><img src="https://i.imgur.com/QQAdk48.png" alt=""><br><img src="https://i.imgur.com/edL5bda.png" alt=""></p><h3 id="6-original-model-VS-multivariate-Gaussian"><a href="#6-original-model-VS-multivariate-Gaussian" class="headerlink" title="6. original model VS. multivariate Gaussian"></a>6. original model VS. multivariate Gaussian</h3><p>如果一个样本有着多种特征，那我们究竟是应该使用 original model，还是 multivariate Gaussian？</p><p>大体上，区别如下：<br><strong>特征选择：</strong>original model 中的各个单个特征（或创造出的新特征），应该尽量满足在异常情况下产生概率极小的特性；而如果特征之间，发现了正相关或负相关的关系，应该用 multivariate Gaussian</p><p><strong>计算效率：</strong>original model 仅仅乘法，效率较高；multivariate Gaussian 需要计算协方差的逆矩阵，效率较低</p><p><strong>样本数目：</strong>original model 在训练集极小的情况下也可以计算；</p><p>multivariate Gaussian 至少需要训练集样本数目大于特征数目，否则协方差矩阵无法求逆</p><p>协方差矩阵无法求逆（奇异矩阵）的情况极少发生，但是一旦发生，可能有以下几种原因：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">特征的数量大于训练集的样本个数</span><br><span class="line">冗余的特征变量（x1≈x2 或者 x3=x4+x5 这样的高度冗余情况）</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (8) PCA</title>
    <link href="https://paradoxallen.github.io/22819/"/>
    <id>https://paradoxallen.github.io/22819/</id>
    <published>2017-09-14T16:00:00.000Z</published>
    <updated>2018-06-19T14:30:00.421Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Motivation-for-Dimensionality-Reduction"><a href="#1-Motivation-for-Dimensionality-Reduction" class="headerlink" title="1. Motivation for Dimensionality Reduction"></a>1. Motivation for Dimensionality Reduction</h3><p>为什么要数据降维？目的性一般来说有三个：</p><p><strong>（1）加速计算，要计算的维度更少；</strong></p><p><strong>（2）节省空间；</strong></p><p><strong>（3）可视化</strong></p><p>因为现在的可视化只能对于2D或者3D有较好的处理。</p><p>那么，数据为什么可以降维呢？举个例子，我用一幅图表示厘米与英尺的关系，这时特征有（厘米，英尺）。但其实我存储的时候，<strong>只需要其中任意一个，另外一个是极强相关度</strong>。此时只有一维特征。</p><h3 id="2-Principal-Component-Analysis-PCA-简介"><a href="#2-Principal-Component-Analysis-PCA-简介" class="headerlink" title="2. Principal Component Analysis (PCA) 简介"></a>2. Principal Component Analysis (PCA) 简介</h3><p><strong>主成分分析（Principal Component Analysis）</strong>是数据降维的一种极为有效的方法。</p><p>通过将高维数据投影到低维空间，以求通过<strong>牺牲极少的数据精度来换取更低维度的数据</strong>。</p><p>先来举个例子，例如下图中的5个点：<br><img src="https://i.imgur.com/bAi9DAY.png" alt=""></p><p>如果精确描述这5个点，那么需要2D的特征(x1,x2)。但是，如果我们以图中的直线作为唯一的1D特征，将每个点投影到直线上，是不是基本可以区分出这5个点？这肯定丧失了一部分精度。</p><p>注意：此时的特征正方向（1D空间的基向量），方向沿着直线的任意两方都正确，我们<strong>关心的只是直线而不是方向</strong>。</p><p>上图的例子中，PCA的任务就是找到这条直线，然后投影到直线上去。而这条直线，必须满足<strong>每个点到直线欧氏距离的平方和最小，即丧失的精度最少</strong>：<img src="https://i.imgur.com/KPVsso6.png" alt=""></p><p>既然都是找直线，那肯定有人自然而然想到了线性回归。但是两者之间有着极大不同。首先就是优化目标，见下图：<br><img src="https://i.imgur.com/lySevWa.png" alt=""></p><p>左图是线性回归所需要最小化的距离（蓝色线段和），右图是2D空间PCA所需要最小化的距离（蓝色线段和）。除此之外，线性回归是为了拟合特殊的变量y，而PCA是一种无监督学习，目的性并不明确，每个变量都会降维。</p><h3 id="3-Principal-Component-Analysis-PCA-流程"><a href="#3-Principal-Component-Analysis-PCA-流程" class="headerlink" title="3. Principal Component Analysis (PCA) 流程"></a>3. Principal Component Analysis (PCA) 流程</h3><p>述说流程之前，希望大家可以看看这篇分析（本课程缺少理论分析）：<a href="http://sebastianraschka.com/Articles/2014_kernel_pca.html" target="_blank" rel="noopener">Kernel tricks and nonlinear dimensionality reduction via RBF kernel PCA</a></p><p>老规矩，首先要 <strong>mean normalization / feature scaling</strong>。方法就跟之前课程提到的一样，对每个特征 j 求均值<img src="https://i.imgur.com/tfWfqOH.png" alt=""></p><p>然后更新特征的值<img src="https://i.imgur.com/nXB3It5.png" alt="">其中Sj是特征值的范围，可以是最大值减去最小值。</p><p>计算协方差矩阵：<img src="https://i.imgur.com/BTVkiWi.png" alt="">，协方差矩阵一般是<strong>对称正定矩阵</strong>，一定会求出特征值与特征向量。</p><p>计算特征值与特征向量，matlab中写作：<code>[U, S, V] = svd(simga)</code>. </p><p>U 是特征向量集合，<img src="https://i.imgur.com/BpkU4CG.png" alt=""></p><p>S是特征值集合<img src="https://i.imgur.com/USdq9tK.png" alt=""></p><p>U∈Rn×n，降维之后取U的前k个特征向量u(1),u(2)…u(k)作为子空间的基向量，构造<img src="https://i.imgur.com/pfb6Kwe.png" alt="">。</p><p>此时x∈Rn→z∈Rk <img src="https://i.imgur.com/auNcI4d.png" alt=""><br>其中z是k×1的列向量，x是n×1的列向量，UTreduce 是k×n的矩阵</p><p>降维之后，进行计算获得结果之后，总是需要升维到原空间中。这个时候的升维精确度已经有所下降，但已经是尽量减少损耗。具体的降维转换公式为<img src="https://i.imgur.com/Wp14DBC.png" alt=""></p><p>也就是下图中的绿色交叉点返回到2D坐标系中表示。<br><img src="https://i.imgur.com/RBzzKVQ.png" alt=""></p><h3 id="4-Principal-Component-Analysis-PCA-其他问题"><a href="#4-Principal-Component-Analysis-PCA-其他问题" class="headerlink" title="4. Principal Component Analysis (PCA) 其他问题"></a>4. Principal Component Analysis (PCA) 其他问题</h3><p>OK，知道怎么使用PCA，那么我们应该选择降维k=？我们应该有评判标准。我们大体目标是<img src="https://i.imgur.com/HuLOWKM.png" alt=""></p><p>因此我们选择标准为：<br><img src="https://i.imgur.com/YjJRuyL.png" alt="">，表示意义为“<strong>保留99%的差异性</strong>”。 </p><p>如果使用特征值来表示就是，<img src="https://i.imgur.com/DTImTvI.png" alt=""></p><p>PCA 中，无监督学习的唯一学习参数是 Ureduce。<strong>Only Training Data！</strong>仅仅是由训练集训练出来的参数。</p><p>可以用<strong>Cross-Validation data 和 test Data进行检验</strong>，但是选择主分量的时候<strong>只应用training data</strong>。</p><p>有时候 PCA 被用于解决 overfitting ，但这样做其实非常不好。PCA 降低了数据维度，但同时损失了数据原有的标记信息。而之前的课程中提到的<strong>regularization 技术</strong>才是解决过拟合的正途。</p><p>使用 PCA 之前，一定要利用原维度的数据进行计算。仅仅当计算过程出现了<strong>（1）计算过慢；（2）占用内存过多</strong>；这两个问题之后，才应该考虑 PCA。盲目的使用，是不可取的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (7) K-Means</title>
    <link href="https://paradoxallen.github.io/48762/"/>
    <id>https://paradoxallen.github.io/48762/</id>
    <published>2017-09-10T16:00:00.000Z</published>
    <updated>2018-06-18T15:56:13.732Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Unsupervised-Learning"><a href="#1-Unsupervised-Learning" class="headerlink" title="1. Unsupervised Learning"></a>1. Unsupervised Learning</h3><p>之前课程中说到的学习，都是监督学习，即有一个label，明确告诉你这个样本，属于哪个类型，或者导致的值是多少。但是，如果我碰到没有label，或者我也不知道label是怎样的情况，但是我还是想要分成若干类。这样的问题，就是一种<strong>无监督</strong>问题。</p><p><strong>聚类（clustering） </strong></p><p>聚类是一种典型的无监督学习例子，但是<strong>聚类不等同于无监督学习</strong>，密度估计同样是一个典型的无监督学习例子。回到聚类，例如有下图：<br><img src="https://i.imgur.com/RWM0hdQ.png" alt=""></p><p>每种样本（蓝色圆圈）都没有label指定类别，但是人眼一看就知道分成两类比较合适。如何让机器也知道如何分类呢？这就是聚类问题。</p><h3 id="2-K-Means-Algorithm"><a href="#2-K-Means-Algorithm" class="headerlink" title="2. K-Means Algorithm"></a>2. K-Means Algorithm</h3><p><strong>K-Means 算法是解决无监督学习的有效算法之一</strong>。K（大写）表示将样本分为K个类型。算法具体的过程通俗易懂，如下图所示：<br><img src="https://i.imgur.com/XXqO0uo.png" alt=""><br><img src="https://i.imgur.com/mTXlNHO.png" alt=""></p><p>配合上图，再作一些简单的解释：<br><img src="https://i.imgur.com/CJB9nyT.png" alt=""></p><p>K-Means 算法进行一段时间后。<strong>可能会出现某个中心点没有分配到任何一个样本点的情况，这个时候可以直接去掉这个分类变为 (K-1) 簇</strong>。</p><p>但是在某些情况我们很执着的要分为 K 簇，这种情况下需要随机确定一个新的聚类中心替代当前中心点，之后再次迭代。</p><h4 id="Optimization-Objective"><a href="#Optimization-Objective" class="headerlink" title="Optimization Objective"></a>Optimization Objective</h4><p>K-Means 算法同样有着 cost function，目标同样是为了最小化 cost。cost 记为：<img src="https://i.imgur.com/Uvbvz66.png" alt=""></p><p>分析K-Means的两个主要步骤，簇分配是以μ为常量，c为变量；移动聚类中心是以c为常量，μ为变量。通过不断修改，减小 cost。</p><p>不同于回归问题，因为学习率过大导致随着迭代次数增加竟然还会增大 cost。聚类问题中不会出现这种问题（就没有学习率），聚类中随着迭代次数增加 <strong>cost 一定是逐渐下降的</strong>。</p><h4 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h4><p>之前只是说，随机化取得K个聚类中心。具体究竟如何执行呢？ </p><p><strong>（1）在m个样本中，随机取得K个样本 (K&lt;m) </strong></p><p><strong>（2）令μ1,…,μK等同于所取到的K个样本</strong> </p><p>但是，有的时候因为初始化的随机性，会陷入局部最大值的情况。例如下图所示：<br><img src="https://i.imgur.com/xKhfaFg.png" alt=""></p><p>这样即使 K-Means 算法停止了，所取得的依然不是较好的分类方法，cost 依然十分大。怎么解决？<strong>多随机初始化几次！一般是运行50~1000次 K-Means 算法，取其中 cost 最小一次的结果</strong>。</p><h4 id="Choosing-the-numbers-of-clusters"><a href="#Choosing-the-numbers-of-clusters" class="headerlink" title="Choosing the numbers of clusters"></a>Choosing the numbers of clusters</h4><p>好吧，最后一个问题来了，究竟应该分成几类？K=？ </p><p>这个问题没有标准解，依照可视化图来观察是个不错的主意。有一种特殊情况，如果K−J(cost)图如下图左边所示，圆圈所指位置为<strong>“肘区”</strong>，一般我们取“肘区”所对应的 K 值。这种方法也称为 Elbow method.</p><p>但是绝大多数时候，我们看到的是如下图右边的情况，这种时候……好吧看你的心情以及实际需求（最多只能承受多少类的负担）取值吧。<br><img src="https://i.imgur.com/mUOx6Rc.png" alt=""></p><p>如果出现K−J(cost)图随着 K 值的增加上下起伏，那说明出现了“局部最大值”的问题。这个时候就应该使用上一小节提到的，多进行几次 K-Means 算法，求出一个J(cost)最小时的 cost 作为当前 K 值的 cost.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (6) Machine Learning Advice</title>
    <link href="https://paradoxallen.github.io/9059/"/>
    <id>https://paradoxallen.github.io/9059/</id>
    <published>2017-09-04T16:00:00.000Z</published>
    <updated>2018-06-15T16:45:25.622Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Evaluating-a-Learning-Algorithm"><a href="#1-Evaluating-a-Learning-Algorithm" class="headerlink" title="1. Evaluating a Learning Algorithm"></a>1. Evaluating a Learning Algorithm</h3><p>如果一个机器学习方法的结果不令人满意，可能有各种方法来解决。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">获取更多的训练样本（最为耗时，不作为优先考虑）</span><br><span class="line">尝试更少 / 更多的特征</span><br><span class="line">尝试引入多项式特征</span><br><span class="line">增加 / 减少正则化系数 λ</span><br></pre></td></tr></table></figure></p><p>究竟应该使用哪种方法来解决问题，需要一个诊断过程，称为 <strong>Maching Learning Diagnostic</strong>。为了更好的阐述，我们先引入两个名词：<strong>欠拟合、过拟合 </strong></p><p><strong>欠拟合：</strong>对于训练集，hypothesis 得到的结果，与真实的结果差距较大，并不能对样本集有效拟合；</p><p><strong>过拟合：</strong>对于训练集，hypothesis 得到的结果，与真实的结果差距较小；但是对于测试集，hypothesis 得到的结果，与真实的结果差距较大。这说明 hypothesis 的泛化能力较差，只是在训练集上得到的效果较好</p><p>通常情况下，我们会得到一组数据而不是区分好的训练集与测试集。这时就需要我们做一些<strong>处理</strong>：</p><p>首先打乱数据的次序，然后将其之前大约70%的部分来作为训练集，训练样本总数记为m，训练样本记为<img src="https://i.imgur.com/S04RXiX.png" alt=""></p><p>剩下的30%部分作为测试集，测试样本总数记为mtest，测试样本记为<img src="https://i.imgur.com/rluZ3Dt.png" alt=""></p><p>在测试集上，我们会计算 hypothesis 与真实数据的偏差。对于线性回归与逻辑回归，有一些不同。这些在之前的系列中都有提及： </p><p><strong>线性回归</strong>：<img src="https://i.imgur.com/BmXcDfD.png" alt=""><br><strong>逻辑回归</strong>：<img src="https://i.imgur.com/g2yXaRe.png" alt=""></p><p>我们在训练集、测试集的基础上，再次添加一种数据集：<strong>交叉验证集（cross validation set）</strong>。</p><p>比例大约是训练集60%，测试集20%，交叉验证集20%，交叉验证样本总数记为mcv，交叉验证样本记为<img src="https://i.imgur.com/B6gENOi.png" alt="">。而Jcv(θ)的计算方式与Jtest(θ)相同，只是适用的数据集范围不同。 </p><p>那么问题来了，为什么要区分这三个集合？以及它们的区别是什么呢？</p><p>这三个名词在机器学习领域的文章中极其常见，但很多人对他们的概念并不是特别清楚，尤其是后两个经常被人混用。Ripley, B.D（1996）在他的经典专著《Pattern Recognition and Neural Networks》中给出了这三个词的定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Training set: A set of examples used for learning, which is to fit the parameters [i.e., weights] of the classifier.</span><br><span class="line">Validation set: A set of examples used to tune the parameters [i.e., architecture, not weights] of a classifier, for example to choose the number of hidden units in a neural network.</span><br><span class="line">Test set: A set of examples used only to assess the performance [generalization] of a fully specified classifier.</span><br></pre></td></tr></table></figure></p><p>显然，<strong>training set是用来训练模型或确定模型参数的</strong>，如ANN中权值等； <strong>validation set是用来做模型选择（model selection），即做模型的最终优化及确定的</strong>，如ANN的结构；而<strong> test set则纯粹是为了测试已经训练好的模型的推广能力</strong>。</p><p>当然，test set这并不能保证模型的正确性，他只是说相似的数据用此模型会得出相似的结果。但实际应用中，一般只将数据集分成两类，即training set 和test set，大多数文章并不涉及validation set。<br>Ripley还谈到了<strong>Why separate test and validation sets?</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The error rate estimate of the final model on validation data will be biased (smaller than the true error rate) since the validation set is used to select the final model.</span><br><span class="line">After assessing the final model with the test set, YOU MUST NOT tune the model any further.</span><br></pre></td></tr></table></figure></p><p>对于过拟合，如何选择 model？使用多高次方的多项式拟合？答案是<strong>遍历尝试</strong>，如下图所示。分别在训练集上计算得到各个θ的值之后，再在交叉验证集（而不是测试集）上计算Jcv(θ)，选择Jcv(θ)最小的一个model作为hypothesis.<br><img src="https://i.imgur.com/mOk3wc8.png" alt=""></p><p>如此一来，测试集上的Jtest(θ)就可以用于其他评估方向，而依靠交叉验证集来决定多项式拟合的次数。</p><h3 id="2-Bias-vs-Variance"><a href="#2-Bias-vs-Variance" class="headerlink" title="2. Bias vs. Variance"></a>2. Bias vs. Variance</h3><p><strong>Bias 与 Variance </strong>并非是通常意义上的名词偏差、方差。而是用于描述两种机器学习中出现的问题。</p><p><strong>high bias意味着欠拟合，high variance意味着过拟合。</strong>下图可以直观看出来：<br><img src="https://i.imgur.com/CzSF40I.png" alt=""></p><p>接下来，我们从另一些角度来看看 Bias 与 Variance 的表现<br>以 hypothesis 的多项式次数 d 为横轴，error 为纵轴，我们可以得到在训练集与交叉验证集上的误差曲线为：<br><img src="https://i.imgur.com/o40hQlt.png" alt=""></p><p>对于 Bias 与 Variance 也有了各自的概念： </p><p><strong>Bias：</strong>Jtrain(θ)较大，Jcv(θ)较大，Jtrain(θ)≈Jcv(θ)。一般是 d（维度）较小的时候，才会产生 bias，欠拟合阶段；</p><p><strong>Variance：</strong>Jtrain(θ)较小，Jcv(θ)较大，Jtrain(θ)≪Jcv(θ)。一般是 d（维度）较高的时候，才会产生 variance，过拟合阶段</p><p>以 hypothesis 的正则化系数 λ 为横轴，error 为纵轴，我们可以得到在训练集与交叉验证集上的误差曲线与上图左右反转：<br><img src="https://i.imgur.com/tT6dRPD.png" alt=""></p><p><strong>Bias：</strong>Jtrain(θ)较大，Jcv(θ)较大，Jtrain(θ)≈Jcv(θ)。一般是 λ 较大的时候，才会产生 bias，欠拟合阶段；</p><p><strong>Variance：</strong>Jtrain(θ)较小，Jcv(θ)较大，Jtrain(θ)≪Jcv(θ)。一般是 λ 较小的时候，才会产生 variance，过拟合阶段</p><p>那么如何选择 λ 呢？其实类似于选择多项式的维度 d，依次尝试。在确定了多项式维度之后，依次增大 λ 的值后再次计算 Jcv(θ) 的值，找到使得 Jcv(θ) 最小的 λ 值。增大的幅度可以较大，例如 λ=0,0.01,0.02,0.04,0.08,…</p><h3 id="3-Learning-Curve"><a href="#3-Learning-Curve" class="headerlink" title="3. Learning Curve"></a>3. Learning Curve</h3><p><strong>learning curve </strong>同样是以 error 为纵轴，只不过这次的横轴换成了训练集的样本数目，但同样是 Jcv(θ) 与 Jtest(θ) 的曲线。一般形状是这样：<br><img src="https://i.imgur.com/jBnE12f.png" alt=""></p><p>在样本较少的时候，训练集很容易被拟合，因此误差较小，但是因为训练样本少所以很难保证对于交叉验证集也是有效的，因此误差较大。</p><p>在样本较多的时候，训练集较难被拟合，因此误差升高，但是因为训练样本更多所以有更大可能保证对于交叉验证集也是有效的，因此 Jcv(θ) 会下降。</p><p>那么对于 high bias 与 high variance 而言，如果我们使用增加样本个数（最为耗时的方法）m，可否起到什么作用？</p><p><img src="https://i.imgur.com/1GHLkWf.png" alt=""></p><p>由图可以看出，在 high bias 情况下，一般都是由于本身选择的 hypothesis 的模型错误，而与训练集样本个数无关。错的模型训练再多次还是错的。</p><p><img src="https://i.imgur.com/iWW3jJS.png" alt=""></p><p>由图可以看出，在 high variance 情况下，很可能是过拟合问题，而增加训练集样本个数会使得模型的泛化能力进一步增强，从而消除过拟合问题，使得误差降低。</p><h3 id="4-Deciding-What-to-Do-Next-Revisited"><a href="#4-Deciding-What-to-Do-Next-Revisited" class="headerlink" title="4. Deciding What to Do Next Revisited"></a>4. Deciding What to Do Next Revisited</h3><p>回想一下最开始提出的若干种优化方案，各自适用情况有：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">获取更多的训练样本 ⇒ high variance</span><br><span class="line">尝试更少的特征 ⇒ high bias</span><br><span class="line">尝试更多的特征 ⇒ high variance</span><br><span class="line">尝试引入多项式特征 ⇒ high bias</span><br><span class="line">增加正则化系数 λ ⇒ high variance</span><br><span class="line">减少正则化系数 λ ⇒ high bias</span><br></pre></td></tr></table></figure></p><p>将以上所有内容，如果引用到神经网络中，同样适用。 </p><p>小规模（层数少、各层神经元数目少）的神经网络容易出现欠拟合问题；大规模的神经网络容易出现过拟合问题。 </p><p>那么使用多少个隐藏层？可以使用依次递增层数，交叉验证集上误差来确定。 </p><p>λ 该如何确定？根据上文，利用交叉验证集，依次递增选择误差最小的确定即可。</p><h3 id="5-Building-a-Spam-Classifier"><a href="#5-Building-a-Spam-Classifier" class="headerlink" title="5. Building a Spam Classifier"></a>5. Building a Spam Classifier</h3><p>让我们以<strong>“垃圾邮件分类（Spam-Classifier）”</strong>问题作为例子：如何判断一封邮件是垃圾邮件，还是正常邮件？</p><p>很容易我们会想到，依据某些单词作为特征，这些单词的出现与否决定这封邮件的性质。</p><p>所以，我们会想到一个0，1组成的特征向量，1代表邮件具备这个特征（出现这个单词），0代表邮件不具备这个特征（没出现这个单词）。</p><p>有了基本大方向之后，就要开始一场头脑风暴了，以求提高机器学习的效果。这种行为，不存在标准答案，任何结果都是有可能的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">增加数据</span><br><span class="line">采用更为复杂的特征（邮件路径）</span><br><span class="line">基于正文寻找更为精确复杂的特征（discount, discounts 视为同一单词；deal, dealer 视为同一单词）</span><br><span class="line">算法改进（故意拼错隐瞒检查的单词？deeler = dealer）</span><br></pre></td></tr></table></figure></p><p>头脑风暴之后，谁说的才更有道理呢？自然需要各种方法对于误差的分析（error analysis）。这里需要注意：<strong>所有的 error analysis 都是在交叉验证集上完成的。</strong></p><p>一般来说，机器学习算法的设计会经历以下几个过程：<br>以最快的速度（一天之内？）用最简单的方法，尝试去解决眼下的问题，并在交叉验证集上验证；</p><p>画出 learning curve，去观察，发现现在的问题是 high bias，还是 high variance，需不需要更多的特征？更多的数据集？</p><p>error analysis：上面已经在cross-validation数据集上测试了系统性能，现在我们人工去看哪些数据造成了大error的产生？是否可以通过改变systematic trend减少error？</p><p>Spam-Classifier 举例，我们看一下进行Error Analysis的步骤：<br>所有 spam 分为四类：pharma，replica/fake，Steal password 和 其他</p><p>如下图，寻找一些可能有助于改善分类效果的features<br><img src="https://i.imgur.com/lZ2lLvZ.png" alt=""></p><p>然后，在是否引入特征的问题上，一定要做实验：例如，可以比较引入此特征前后，预测的准确率<img src="https://i.imgur.com/CJLBwew.png" alt="">是否提高？</p><p>error analysis 的核心在于：一定要找到一种<strong>数值化的</strong>评定方法，以求判断 error 的大小。</p><h3 id="6-Handling-Skewed-Data"><a href="#6-Handling-Skewed-Data" class="headerlink" title="6. Handling Skewed Data"></a>6. Handling Skewed Data</h3><p>首先，我们需要介绍，什么样的数据称为<strong>偏斜数据（skewed data）</strong>。这次，我们举的例子是预测癌症。</p><p>预测癌症例子：实际生活中，癌症的发病率极低，可能在人群中只有0.5%的发病率。假设我们手头有一种预测方法，预测出来有1%的人会发病。这样，按照之前的评价标准，有0.5%的误诊率。但是，如果我不做任何的检查，直接判断病人是没病的，是不是同样有着0.5%的误诊率？这是不应该的，我们化验检查一番力气之后的答案竟然和瞎猜的误诊率相同。</p><p>当分类问题中，某一类所占比例极小<strong>（一般将较小比例的类别置为1）</strong>，就会有偏斜数据的问题。</p><p>这时候，直接用 error 来描述在这种数据集上的问题不合适，我们需要寻求更新的数值化评价标准。</p><p>引入新的标准 precision 和 recall 之前，我们需要介绍几个新名字：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">True positive：预测结果为真，并且实际分类同样为真</span><br><span class="line">True negative：预测结果为假，并且实际分类同样为假</span><br><span class="line">False positive：预测结果为真，但是实际分类为假</span><br><span class="line">False negative：预测结果为假，但是实际分类为真</span><br></pre></td></tr></table></figure></p><p>说起来有点复杂，来看下面这张图一目了然：<br><img src="https://i.imgur.com/bVYkrZU.png" alt=""></p><p>接下来，我们在此基础上，介绍两个评价标准：</p><p><strong>查准率（precision）</strong>：在查出有癌症的患者中，实际患病的概率，公式见上图。<strong>precision 越高算法越好</strong>；</p><p><strong>召回率（recall）：</strong>所有身患癌症的病人中，有多少概率被查出，公式见上图。 <strong>recall越高算法越好</strong>；</p><p>再回到一开始的例子，如果我全部将病人视为健康，准确率是99.5%，但是我的查准率和召回率极低。可以判定不是一个好算法。</p><p>回到预测癌症例子</p><p>如果为了保证确诊率，可以将逻辑回归的分类值由0.5改为0.7，这样一来 precision 必然会升高，但是也会导致 recall 的下降</p><p>如果为了引起大家的更多重视，可以将逻辑回归的分类值由0.5改为0.3，这样一来 precision 必然会下降，但是也会导致 recall 的升高</p><p>貌似 precision（记为P） 和 recall（记为R） 总是背道而驰，偏偏两者还都是重要的偏斜类上的算法评价标准，如何将这两个标准合为一个？平均值显然是不行的，P=1且R=0时平均值为0.5，看不出端倪。</p><p><strong>F值（F-score）</strong>：F=2PR/(P+R)，完美地解决了这个问题。F值的中心思想就是：赋予P、R中更低的值更大的权值，因为P、R总是此消彼长，这样就制约两者必须同时保持在较大值才使得F值较高。可以用于<strong>所有有着此消彼长关系的标准的综合评价</strong>。</p><h3 id="7-Using-Large-Data-Sets"><a href="#7-Using-Large-Data-Sets" class="headerlink" title="7. Using Large Data Sets"></a>7. Using Large Data Sets</h3><p>一般来说，增大数据集，可以提高算法的 accuracy，但也不全是这样。比如房价预测，如果我仅仅给你房子的面积，而没有房子在市中心还是偏远地区？房龄多少？等信息，我们是无法进行良好预测的。所以，我们需要知道的前提条件是： </p><p><strong>如果当前的特征已经足够预测，增大数据集的确可以提高准确性 </strong><br>总结： </p><ol><li><p>想要保证bias小，就要保证有足够多的feature，即linear/logistics regression中有很多parameters，neuron networks中应该有很多hidden layer neurons</p></li><li><p>想要保证variance小，就要保证不产生overfit，那么就需要很多data set（只要样本数远远大于特征数，是无法过拟合每个点）。这里需要Jcv和Jtrain都很小，才能使Jtest相对小</p></li></ol><p>综上所述，对数据及进行理性分析的结果是两条： </p><p><strong>首先，x中有足够多的feature，以得到low bias;</strong><br><strong>其次，有足够大的training set，以得到low variance</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (5) Neural Network Part2</title>
    <link href="https://paradoxallen.github.io/53070/"/>
    <id>https://paradoxallen.github.io/53070/</id>
    <published>2017-08-29T16:00:00.000Z</published>
    <updated>2018-06-15T04:54:42.481Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><p>上一篇文章，介绍了<strong>前向传播(forward propagation)</strong>的过程，以及神经网络计算非线性问题的例子(XOR问题)</p><p>这一篇文章，开始介绍，如何来计算神经网络中各种参数的方法：<strong>后向传播(backward propagation)</strong></p><h3 id="1-Cost-Function"><a href="#1-Cost-Function" class="headerlink" title="1. Cost Function"></a>1. Cost Function</h3><p>为了拟合神经网络的各个参数，我们首先需要规定一些变量：<br><img src="https://i.imgur.com/GxSynvL.png" alt=""></p><p>不同分类问题：</p><p>若分为2类，其实用一个神经元作为输出层就可以了，用y=0和y=1区分；</p><p>多类问题，利用以前说过的，分为K类则最终的y∈R^K，例如分为三类的问题输出可选择为<img src="https://i.imgur.com/ariPfuP.png" alt=""></p><p>因为同样是分类问题，我们回想一下逻辑回归的cost function，其实神经网络的相同，只不过是对于分为K类问题的版本而已：<br><img src="https://i.imgur.com/9XJbSNE.png" alt=""></p><p>是不是……太复杂了，如果用 cost function 计算梯度之后梯度直接梯度下降法来计算神经网络参数，明显不如接下来介绍的这种方法简单快捷。</p><h3 id="2-Backpropagation"><a href="#2-Backpropagation" class="headerlink" title="2. Backpropagation"></a>2. Backpropagation</h3><p><strong>后向传播</strong>，是神经网络中用于替代直接计算梯度下降法，来设定各层参数的方法。方法的精髓在于，<strong>训练网络时先根据初始化的参数（一般是随机设定），计算得到最后一层（输出层）的输出，计算与实际网络输出之间的差，再根据当前层的差，反推出上一层的差，逐渐反推到第一层。每一层根据自身层的差，来逼近真实参数</strong>。</p><p>首先我们设定某l层的某j个神经元，与真实的神经元的值，两者的差距为<img src="https://i.imgur.com/Nc3lrrH.png" alt="">，the error of node j in layer l.</p><p>对于输出层的每个神经元来说，可以直接计算：<img src="https://i.imgur.com/3Ds9vuD.png" alt="">，向量化表示为<img src="https://i.imgur.com/cgaxaK5.png" alt=""></p><p>对于其他层的每个神经元而言，就需要依靠上一层神经元的计算结果来反推，<strong>反推的过程可以视为原本 Forwardpropagation 过程中分散到各个下一层神经元的水流沿着同样的道路再次汇聚到上一层的神经元中</strong>：<img src="https://i.imgur.com/9XmWAoL.png" alt=""></p><p>对于g′(t)的计算，这里需要一点点小技巧：<br><img src="https://i.imgur.com/CqMnQoj.png" alt=""></p><p>推导过程见下：<br><img src="https://i.imgur.com/Or2ogvB.png" alt=""></p><p>不存在δ(1)，因为<strong>输入层没有误差</strong></p><p>我们首先不考虑正则化项（最小化各个系数<img src="https://i.imgur.com/AbSeYkS.png" alt="">，得到计算各层各个单元的误差项之和</p><p>因此，我们得到求出神经网络各层参数的方法：<br><img src="https://i.imgur.com/SKUhVtK.png" alt=""></p><h3 id="3-Gradient-Checking"><a href="#3-Gradient-Checking" class="headerlink" title="3. Gradient Checking"></a>3. Gradient Checking</h3><p>当我们需要验证求出的神经网络工作的对不对呢？可以使用 gradient checking，通过check梯度判断我们的code有没有问题</p><p>对于下面这个[θ−J(θ)]图，取θ点左右各一点(θ−ϵ)与(θ+ϵ)，则点θ的梯度近似等于<img src="https://i.imgur.com/xliVzIX.png" alt=""></p><p><img src="https://i.imgur.com/XvkBIeh.png" alt=""></p><p>对于神经网络中的情况，则有：<br><img src="https://i.imgur.com/SPV9RtR.png" alt=""></p><p>由于在backpropagation算法中我们一直能得到J(θ)的导数D（derivative），那么就可以将这个近似值与D进行比较</p><p><strong>Summary: </strong><br>(1) 在backpropagation中计算出J(θ)对θ的导数D并组成vector（Dvec） </p><p>(2) 用numerical gradient checking方法计算大概的梯度<img src="https://i.imgur.com/xa4LZR9.png" alt=""></p><p>(3) 看是否得到相同（or相近）的结果 </p><p>(4) <strong>（非常重要）</strong>停止checking，只用 backpropagation 进行神经网络学习，否则会非常非常慢</p><h3 id="4-Backpropagation-in-Practice"><a href="#4-Backpropagation-in-Practice" class="headerlink" title="4. Backpropagation in Practice"></a>4. Backpropagation in Practice</h3><p>这一节我们来看看实际 octave/MATLAB 编程中的一些技巧，例如对于神经网络如下：<br><img src="https://i.imgur.com/e2uWpzX.png" alt=""><br>当s1=10,s2=10时，则有θ(1)∈R10×11,θ(2)∈R10×11，一般来说，为了方便变形与传递参数，我们是将所有θ展开成一个完整的变量：<br>    thetaVec=[Theta1(:);Theta2(:);Theta3(:)] </p><p>再次展开的时候，例如重组为θ(1)时，取出其中前110个重组就好：<br>    Theta1=reshape(thetaVec(1:110),10,11)</p><p>如何初始化各层的参数呢？这同样是一个需要注意的地方：<strong>不能将各层的各个神经元的参数赋值为相同的数</strong>。</p><p>有兴趣的同学可以计算一下，这样神经网络的某一层内的所有神经元计算都变得相同，这样神经网络的非线性程度就降低了。<strong>一般来说，都是在(+ϵ,−ϵ)之间随机赋值</strong></p><p>我的神经网络需要有多少层呢？同样是个有趣的问题，一般来说，<strong>三层结构（仅仅一个隐藏层）</strong>已经足够来处理大部分非线性情况。如果分类效果不好，可以尝试使用更多的隐藏层，但需要保证每个隐藏层的神经元个数相同，层数越多就越慢，当然一般来说分类效果就更好</p><p>每层神经网络需要多少个神经元？能确定的只有输入层与输出层：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">No. of input units: Dimension of features</span><br><span class="line">No. output units: Number of classes</span><br><span class="line">Other units: who knows… usually the more the better</span><br></pre></td></tr></table></figure></p><h3 id="5-Put-it-together"><a href="#5-Put-it-together" class="headerlink" title="5. Put it together"></a>5. Put it together</h3><p>最终我们回顾一下神经网络的主要步骤： </p><p><strong>randomly initialize weights</strong></p><p><strong>(for 1 to m) forward-propagation</strong></p><p><strong>(for 1 to m) cost function</strong></p><p><strong>(for 1 to m) backward-propogation</strong></p><p><strong>gradient checking, then stop</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (4) Neural Network Part1</title>
    <link href="https://paradoxallen.github.io/21187/"/>
    <id>https://paradoxallen.github.io/21187/</id>
    <published>2017-08-22T16:00:00.000Z</published>
    <updated>2018-06-14T00:26:45.751Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-由XOR-Problem想到"><a href="#1-由XOR-Problem想到" class="headerlink" title="1. 由XOR Problem想到"></a>1. 由XOR Problem想到</h3><p>有一种经典的非线性问题：<strong>XOR，也就是异或</strong>。真值表如下： </p><p>0 0 | 0<br>1 0 | 1<br>1 1 | 0<br>0 1 | 1，| 左侧是输入，| 右侧是输出 </p><p>如果在二维坐标系上绘图，可以看出来仅利用一条直线是无法对这个问题分类的，也就是<strong>线性不可分</strong>问题。</p><p>如果利用逻辑回归的方法，可以枚举各种特征的出现可能，即<br>继续想下去，如果基础特征量更多的话？就会出现过拟合的问题，同时带来极大的计算量。</p><p>例如，计算机视觉中处理一张照片，每个像素都需要一个数值表示。对于100*100像素值的图片，仅仅考虑二次项等级，就有特征数量为5000个（与相同，故除以2）。</p><p>于是，这章介绍的非线性分类器，<strong>神经网络（Neural Network，NN）</strong>就发挥了作用。</p><h3 id="2-人工神经网络"><a href="#2-人工神经网络" class="headerlink" title="2. 人工神经网络"></a>2. 人工神经网络</h3><p>神经网络最初提出的初衷，是因为要模拟人类大脑的结构（很初级的模拟，因为人类对于自己大脑究竟是怎样都没有弄清楚）。<strong>通过多个感知机之间的输入输出，从而完成整体的智能行为</strong>。</p><p>在人工神经网络中，“感知机”就是一个有着输入与输出功能的小单元，接收上一层的输入，将输出传给下一层。</p><p>人工神经网络是层级结构，某一层上的单元之间互相不会有输入输出关系，只和上一层或者下一层的单元产生数据传输关系。至少会有两层：<strong>输入层（input layer）与输出层（output layer）</strong>，但是两层的神经网络可以解决的问题很少，一般都是三层或者三层以上，中间的这些层就称为<strong>“隐藏层（hidden layer）”</strong>，我们来看一个最简单的例子：<br><img src="https://i.imgur.com/U02TDKa.jpg" alt=""></p><p>由<strong>输入层，到隐藏层，最终到输出层</strong>。这是一次 <strong>forward propogation</strong> 过程。类似于逻辑回归，但是神经网络的输入是某个样本的所有基础特征，不需要考虑 这一类新加入的特征。</p><h3 id="3-回到XOR-Problem"><a href="#3-回到XOR-Problem" class="headerlink" title="3. 回到XOR Problem"></a>3. 回到XOR Problem</h3><p>先讲几个基础的利用神经网络进行二进制运算分类的问题：</p><ol><li><p>二进制 AND<br><img src="https://i.imgur.com/5pgAvdw.jpg" alt=""><br>即为，只有1,1时返回值才为1，符合 AND 的操作结果。</p></li><li><p>二进制 OR<br><img src="https://i.imgur.com/vNpszeX.jpg" alt=""><br>即为，只有0,0时返回值才为0，符合 OR 的操作结果。</p></li><li><p>二进制 NOT<br><img src="https://i.imgur.com/8ODYZRw.jpg" alt=""></p></li></ol><p>XOR 问题复杂一些，但是如果我们做了如下转换：</p><p><code>XNOR = NOT XOR = AND OR NOT AND NOT</code></p><p>变换的正确性，很容易通过真值表来验证。大家可以分别计算各个括号中的内容，然后通过 OR 连接起来。<br>我们将 AND 的内容视为 ，NOT AND NOT 的内容视为<br><img src="https://i.imgur.com/tcDGEAE.jpg" alt=""></p><h3 id="4-神经网络多类分类"><a href="#4-神经网络多类分类" class="headerlink" title="4. 神经网络多类分类"></a>4. 神经网络多类分类</h3><p>神经网络处理多类的分类问题是很方便的。举个例子，区分手写数字时，有10个类别：0，1，2，……，9。<br>对于某一个训练样本来说，有着特征组合,这个神经网络的输出层有10个单元。当输出层的10个单元全部取 0，意味着输入不是任何一种数字。</p><p>对于手写数字的识别，一直是业界的研究重点之一。视频中举了一篇经典的利用神经网络处理该问题的Paper，有兴趣的同学可以访问作者的个人主页查看Demo与Paper：<a href="http://yann.lecun.com/exdb/lenet/" target="_blank" rel="noopener">Yann LeCun</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (3) Logistic Regression &amp; Regularization</title>
    <link href="https://paradoxallen.github.io/26112/"/>
    <id>https://paradoxallen.github.io/26112/</id>
    <published>2017-08-15T16:00:00.000Z</published>
    <updated>2018-06-13T04:32:05.451Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Logistic-Regression"><a href="#1-Logistic-Regression" class="headerlink" title="1. Logistic Regression"></a>1. Logistic Regression</h3><p>对于分类问题而言，很容易想到利用<strong>线性回归</strong>方法，拟合之后的<strong>hθ(x)&gt;0.5则为True，其余为False</strong>.</p><p>但是线性回归有一个问题，拟合出的值都是离散的，范围不确定。</p><p>为了方便分析，我们希望将拟合出的值限制在0~1之间。因此，出现了<strong>逻辑回归</strong>。</p><p>逻辑回归的模型是一个<strong>非线性模型</strong>：<strong>sigmoid函数，又称逻辑回归函数</strong>。但它本质上又是一个线性回归模型，因为除去sigmoid映射函数关系，其他的步骤，算法都是线性回归的。</p><p>sigmoid函数（或，逻辑回归函数）：<br><img src="https://i.imgur.com/QLwtN5k.png" alt=""><br>其函数图像为：<br><img src="https://i.imgur.com/wC6xW9Z.png" alt=""></p><p>这个函数的特征非常明显<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">函数值一直在0~1范围内；</span><br><span class="line">经过(0,0.5)点。这个很容易作为区分0，1类的分界线。</span><br></pre></td></tr></table></figure></p><p>逻辑回归中，对于原本线性回归中拟合而成的hypothesis函数，需要经过sigmoid函数的修饰：<br><img src="https://i.imgur.com/ElJujkC.png" alt=""></p><p>此时，hθ(x)的含义发生了变化，<br><img src="https://i.imgur.com/6DX46yY.png" alt=""><br>。成为<br>        <strong>‘’the probability that y=1, given x, parameterized by θ’’</strong></p><p>因此有<br><img src="https://i.imgur.com/SbZ7wQN.png" alt=""></p><p><strong>Decision Boundary</strong>。表示的是 hypothesis 函数确定之后，划分数据分类的界限，并不一定可以百分百区分数据集，只是函数的属性之一。下图蓝色曲线即为某个 Desicision Boundary。<br><img src="https://i.imgur.com/nM4tJjr.jpg" alt=""></p><h3 id="2-Cost-Function"><a href="#2-Cost-Function" class="headerlink" title="2. Cost Function"></a>2. Cost Function</h3><p>回忆线性回归的 cost function，我们在其中插入 cost 函数的概念：<br><img src="https://i.imgur.com/2fhUuge.png" alt=""></p><p>完全照搬线性回归的 cost function 到逻辑回归中，因为sigmoid函数的非线性，会造成J(θ)取值的不断震荡，导致其是一个非凸形函数（non-convex）。表示在“J(θ)—θ”二维图中如下：<br><img src="https://i.imgur.com/YxpdAK6.png" alt=""></p><p>我们需要构造一种新的 cost 函数。出发点为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">当y=1时，若hypothesis函数拟合结果为0，即为“重大失误”，cost 趋于无穷大；</span><br><span class="line">当y=0时，若hypothesis函数拟合结果为1，即为“重大失误”，cost 趋于无穷大；</span><br></pre></td></tr></table></figure></p><p>构造的新 cost 函数：<br><img src="https://i.imgur.com/1AZPras.png" alt=""></p><p>如果进一步合并，可以得到最终逻辑回归的cost函数。并且值得指出的是，代入这个cost函数通过梯度下降法得到的 θ 更新函数依然成立：<br><img src="https://i.imgur.com/F16aq5V.png" alt=""></p><h3 id="3-梯度下降法的优化"><a href="#3-梯度下降法的优化" class="headerlink" title="3. 梯度下降法的优化"></a>3. 梯度下降法的优化</h3><p>对于梯度下降法的优化有很多，但是都需要J(θ)与∂J(θ)/∂θj的代码。</p><p>以此为基础的对于梯度下降法的优化（视频中都没有具体介绍，有兴趣的同学可以点击链接）有：<br><a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method" target="_blank" rel="noopener">共轭梯度法</a><br><a href="https://en.wikipedia.org/w/index.php?title=Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm&amp;redirect=no" target="_blank" rel="noopener">BFGS</a><br><a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" target="_blank" rel="noopener">L-BFGS</a></p><p>这些优化方法的特点也很一致：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">不需要人为选择 α，自适应性</span><br><span class="line">更复杂，更慢</span><br></pre></td></tr></table></figure></p><p>这里提到了两个MATLAB的非线性优化函数：<br><strong>optimset：创建或编辑一个最优化参数选项。</strong>具体调用在MATLAB中 <code>help optimset</code> 命令查看；<br><strong>fminunc：最小值优化。</strong>具体调用在MATLAB中 <code>help fminunc</code>命令查看；</p><p><strong>建议</strong>：Ng在优化这一部分讲的过于简略，基本等于什么都没说……还是要根据这几个方法名称在使用时搜索更多。</p><h3 id="4-one-vs-all-one-vs-rest"><a href="#4-one-vs-all-one-vs-rest" class="headerlink" title="4.one vs. all (one vs. rest)"></a>4.one vs. all (one vs. rest)</h3><p>如果需要进行多类的分类，需要一种精妙的修改，使得两类的分类问题得以适用于多类的分类。 </p><ol><li><p>现已知有n类样本需要区分开（1，2，3，……）；</p></li><li><p>以原1类为新1类，剩余的原2，3，……作为新2类。原本的多类问题变成了二类问题:<br><img src="https://i.imgur.com/TKq60Ef.png" alt=""></p></li><li>以原2类为新1类，剩余的原1，3，……作为新2类。再分类:<br><img src="https://i.imgur.com/EioDwff.png" alt=""></li><li><p><img src="https://i.imgur.com/wK9pOb4.png" alt=""></p></li><li><p>对于任意一个 x 而言，如何分辨是哪一类呢？于是，求出所有的<img src="https://i.imgur.com/7U91HVE.png" alt="">值最大对应的i（<strong>表示y=i的概率最大</strong>）即为x的所属分类</p></li></ol><h3 id="5-Regularization（正则化）"><a href="#5-Regularization（正则化）" class="headerlink" title="5. Regularization（正则化）"></a>5. Regularization（正则化）</h3><p>拟合会产生三种情况： </p><ol><li><p><strong>underfitting（欠拟合）=high bias</strong>，大部分训练样本无法拟合</p></li><li><p><strong>overfitting（过拟合）=high variance</strong>，为了拟合几乎每一个训练样本。导致拟合函数极为复杂，易产生波动，泛化（generalize）能力差，虽然训练样本几乎百分百拟合，但是测试样本很可能因为极大波动而极少拟合成功</p></li><li><p><strong>just right</strong>，对于训练样本，拟合得不多不少刚刚好，并且泛化到测试样本拟合效果同样较好</p></li></ol><p>欠拟合，比较好解决，创造并引入更多的特征即可。例如：对于x,y而言，可以引入x2,y2,xy等等新的特征</p><p>过拟合，则比较复杂。可用的方法有两个： </p><ol><li><p><strong>Reduce number of features, 减少特征量</strong></p></li><li><p><strong>Regularization，正则化</strong>。保持所有的特征数量不变，而去改变特征前的度量单位 θj（若 θj 趋于0，则此特征可视为无影响）</p></li></ol><p>解决过拟合的正则化方法，因此需要引入全新的优化目标到 cost function 中。原先的 cost function 只是希望适合拟合更为接近，现在还需要使得特征前的度量单位 θj 的最小。因此有：<br><img src="https://i.imgur.com/Msp5tw4.png" alt=""></p><p>正则化方法处理之后，∂J(θ)/∂θj发生对应变化，因此我们有：<br><img src="https://i.imgur.com/dmhGxHY.png" alt=""></p><p>若λ非常大（例如10^10），则正则化方法会导致结果 underfitting。这也很好理解，因为优化目标中有使得 <img src="https://i.imgur.com/ctA4muM.png" alt=""> 尽可能小，这样会导致 θ 全部趋于 0。</p><p>一般来说，<strong>α,λ,m&gt;0，所以(1−αλ/m)&lt;1，常见使其取值0.99 左右</strong></p><h3 id="6-Regularization-for-Normal-Equation"><a href="#6-Regularization-for-Normal-Equation" class="headerlink" title="6. Regularization for Normal Equation"></a>6. Regularization for Normal Equation</h3><p>课程视频中缺少证明，因此我们仅需掌握结论使用即可<br>对于 Week 2 中的<strong>Normal Equation</strong>方法，原本需要求解的方程<br><img src="https://i.imgur.com/ZiPg749.png" alt=""><br>做一个小小的改动：<br><img src="https://i.imgur.com/cZwlXun.png" alt=""></p><p>若样本拥有n个特征，则<img src="https://i.imgur.com/Gu1x2Z9.png" alt="">表示的是(n+1) * (n+1)维的对角矩阵，除了(0, 0)取值为 0，其余对角位置取 1。</p><p><strong>non-invertibility</strong>：非不可逆性……好拗口，意思就是对于原本的(xTx)矩阵可能会出现不可逆的情况。但是，对于正则化之后的矩阵<img src="https://i.imgur.com/KB79QV2.png" alt="">一定是可逆的（未提供证明）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (2) Linear Regression</title>
    <link href="https://paradoxallen.github.io/58080/"/>
    <id>https://paradoxallen.github.io/58080/</id>
    <published>2017-08-09T16:00:00.000Z</published>
    <updated>2018-06-13T04:32:05.448Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Multivariate-Linear-Regression"><a href="#1-Multivariate-Linear-Regression" class="headerlink" title="1. Multivariate Linear Regression"></a>1. Multivariate Linear Regression</h3><p>Week 1 讨论仅一个特征，即仅有一个未知量x影响了目标y的取值。如果现在有很多特征？现在我们有x1,x2…xn影响了目标y的取值。</p><p>此时需要区分的是变量标记规则：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xi表示的是第i个特征</span><br><span class="line">x(i)表示的是第i个样本，一个样本是由多个特征组成的列向量</span><br></pre></td></tr></table></figure></p><p>例如：<br><img src="https://i.imgur.com/1VQTtI7.png" alt=""><br>综上，我们有<br><img src="https://i.imgur.com/IN8QuHt.png" alt=""><br>可以视为，每个样本都多出一个特征：x0=1，这样表示有利于之后的矩阵表示</p><h3 id="2-多变量梯度下降法："><a href="#2-多变量梯度下降法：" class="headerlink" title="2. 多变量梯度下降法："></a>2. 多变量梯度下降法：</h3><p><img src="https://i.imgur.com/jqtDLIq.png" alt=""></p><h3 id="3-Feature-Scaling（特征缩放）"><a href="#3-Feature-Scaling（特征缩放）" class="headerlink" title="3. Feature Scaling（特征缩放）"></a>3. Feature Scaling（特征缩放）</h3><p>很简单，就是将每种特征的数据范围限定在同一个数量级。例如<br><img src="https://i.imgur.com/cauqMeB.png" alt=""><br>这样会导致迭代次数过多。这时候，如果我们找到一种mapping方式，使得两者属于同一个数量级的范围内，可以有效减少迭代次数</p><p><strong>注意</strong>：无法降低单次的迭代时间，但是却能有效地降低迭代次数</p><p>其实方法很多，这有一种：<br><img src="https://i.imgur.com/egjFhiQ.png" alt=""><br>其中，mean(x)表示向量每个元素的平均值，max(x)表示向量中最大元素，min(x)表示向量中最小元素</p><h3 id="4-Learning-Rate"><a href="#4-Learning-Rate" class="headerlink" title="4. Learning Rate"></a>4. Learning Rate</h3><p>learning rate 是机器学习中的一个不稳定因素，如何判断选取的 learning rate 是合适的？我们可以看看以下这幅图：<br><img src="https://i.imgur.com/81mq8t6.jpg" alt=""></p><p>如果以迭代次数为横坐标，cost function 结果为纵坐标，绘制的图像是递减的，说明 learning rate 选择的是恰当的。如果碰到下图所显示的三种情况，那就只有一条路：<strong>减小 learning rate </strong><br><img src="https://i.imgur.com/DmExZo6.jpg" alt=""></p><p>但是 learning rate 太小同样会导致一个问题：<strong>学习过慢</strong>。所以，只能靠试：0.001，0.003，0.01，0.03，0.1，0.3……</p><h3 id="5-Polynomial-Regression（多项式回归，不同于多变量线性回归）"><a href="#5-Polynomial-Regression（多项式回归，不同于多变量线性回归）" class="headerlink" title="5. Polynomial Regression（多项式回归，不同于多变量线性回归）"></a>5. Polynomial Regression（多项式回归，不同于多变量线性回归）</h3><p>有时候，我们需要自己创造一些“特征”，来拟合一些非线性分布情况<br>例如：<br><img src="https://i.imgur.com/5F2PbWA.png" alt=""><br>看上去只有一个特征x，但我们完全可以理解为x^2和√x都是单独的新特征</p><p>以后的课程会具体讲述如何选择这些特征</p><h3 id="6-Normal-Equation"><a href="#6-Normal-Equation" class="headerlink" title="6. Normal Equation"></a>6. Normal Equation</h3><p>梯度下降法可以用于寻找函数（cost function）的最小值，想一想，初高中的时候我们使用的是什么方法？最小值点的导数为零，然后解方程</p><p>将导数置为零这种方法即<strong> Normal Equation</strong>。<br><img src="https://i.imgur.com/aqLezpl.png" alt=""></p><p>上文提过，增加一个全1分量x0后得到<br><img src="https://i.imgur.com/N6qtlas.png" alt=""></p><p>可以得到：<br><img src="https://i.imgur.com/LU4vw47.png" alt=""></p><p>matlab编程十分简单：<br><img src="https://i.imgur.com/OVUbKD9.png" alt=""></p><p>Normal Equation 有以下<strong>优缺点</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">不需要 learning rate，也就不需要选择；</span><br><span class="line">不需要迭代，不需要考虑收敛的问题；</span><br><span class="line">当特征非常多的时候，因为涉及求逆操作，会非常慢（注：方阵才有逆矩阵）</span><br></pre></td></tr></table></figure></p><h3 id="7-Octave-Tutorial"><a href="#7-Octave-Tutorial" class="headerlink" title="7. Octave Tutorial"></a>7. Octave Tutorial</h3><p>这一部分十分简单，其实就是MATLAB的使用方法。建议不论是否初学者都去看看，会有收获。 </p><p>谈到一个问题：<strong>如果现有的样本数，小于每个样本所有的特征数怎么办？去除多余的特征（PCA？）</strong>。特征过多，也可能会导致矩阵不可逆的情况。 </p><p>下面记录一些觉得挺有趣的命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">~=：不等于号</span><br><span class="line">xor(0, 1)：异或操作</span><br><span class="line">rand(m, n)：0~1之间的大小为m*n的随机数矩阵；randn：产生均值为0，方差为1的符合正态分布的随机数（有负数）</span><br><span class="line">length(A)：返回A中行、列中更大值</span><br><span class="line">A(:)：将矩阵A变为列向量形式，不论A是向量还是矩阵</span><br><span class="line">sum(A,1)：每列求和得到一个行向量；sum(A,2)：每行求和得到一个列向量</span><br><span class="line">pinv：伪求逆；inv：求逆</span><br><span class="line">imagesc(A)：帅爆！根据矩阵中每个值绘制各种颜色的方块</span><br><span class="line">A.^2 ~= A^2，后者是两个矩阵相乘</span><br></pre></td></tr></table></figure></p><h3 id="8-Submitting-Programming-Assignments"><a href="#8-Submitting-Programming-Assignments" class="headerlink" title="8. Submitting Programming Assignments"></a>8. Submitting Programming Assignments</h3><p>其实看看视频就行了，主要要注意，submit() 时输入的Token，不是Coursera 的密码，而是作业的密码：<br><img src="https://i.imgur.com/a8OsCRt.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (1) Introduction</title>
    <link href="https://paradoxallen.github.io/36971/"/>
    <id>https://paradoxallen.github.io/36971/</id>
    <published>2017-08-04T16:00:00.000Z</published>
    <updated>2018-06-13T04:32:05.450Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Environment-Setup-Instructions"><a href="#1-Environment-Setup-Instructions" class="headerlink" title="1. Environment Setup Instructions"></a>1. Environment Setup Instructions</h3><p>这一章介绍课程一般使用的工具。octave或者matlab即可，这两者本质上没有什么区别，都有着丰富的数学库计算库。</p><h3 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h3><ol><li><strong>机器学习定义：</strong>简单来说，让计算机执行一些行为，但是without explicit programmed。</li><li><p><strong>监督学习（supervised learning）：</strong>通过<strong>已知</strong>的正确信息，得到未知的信息。一般用于解决的问题有两类： </p><ol><li><p><strong>回归问题（regression）</strong><br> 例如：房价预测。已知一百平房子售价，两百平房子售价，三百平房子售价。那么，一百五十平房子的售价呢？</p><p> 很容易想到，<strong>拟合</strong>。那么接下来，是直线拟合？二次曲线拟合？</p><p> 这就是回归问题要解决的，通过已知离散信息，<strong>预测</strong>未知的连续信息。</p></li><li><p><strong>分类问题（classification）</strong><br> 例如：肿瘤分类。已知肿瘤size与肿瘤良性（label: 0）还是恶性（label: 1）分类的关系如下：<br><img src="https://i.imgur.com/MlGVT51.jpg" alt=""></p><p> 更进一步，完全可以用一维坐标图来表示：<br><img src="https://i.imgur.com/xI6gTxQ.jpg" alt=""></p><p> 现在需要对于根据size的对某个肿瘤判断它是良性还是恶性，就是分类问题。</p><p> 再进一步，现在我仅仅把size作为feature之一，如果我们加入考虑肿瘤积水（二维坐标图）？肿瘤硬度（三维坐标图）？原则上可以考虑无限种feature，如何处理？这里卖了个关子，可以使用支持向量机（SVM）。</p></li><li><p><strong>无监督学习（unsupervised learning）/ 聚类算法（clustering）：</strong><br> 对于有监督学习而言，我们已知的信息很多：每个data的label是什么，label可以分为多少类，等等。<strong>但是无监督学习中，这些都是未知的。</strong><br> 最常见的应用：浏览新闻时经常会有的相关新闻，或者搜索时的相关搜索结果，又或是weibo中的推荐分组，等等。</p><p> 可以理解为计算机自己理解后将信息分为了若干类。</p><p> 经典案例：<a href="http://soma.mcmaster.ca/papers/Paper_7.pdf" target="_blank" rel="noopener"><strong>cocktail party problem</strong></a>。简而言之，就是在聚会上摆俩麦克风分别记录声音，需要通过这些信息将不同声源的声音分隔开来。</p><p> 如果考虑声音本身的特征与背景知识，解法会比较复杂，需要大量的先验知识。但站在机器学习的角度来思考，就是一类无监督学习的应用场景。matlab实现仅需几行代码而已。</p></li></ol></li></ol><h3 id="3-Bonus-Course-Wiki-Lecture-Notes"><a href="#3-Bonus-Course-Wiki-Lecture-Notes" class="headerlink" title="3. Bonus: Course Wiki Lecture Notes"></a>3. Bonus: Course Wiki Lecture Notes</h3><p><strong>单一变量线性回归</strong>：即通过线性方程来拟合已知data的feature，与label之间的关系。表达为<br><img src="https://i.imgur.com/zMe7YGr.png" alt=""></p><p>不同的线性方程之间，需要挑出来一个拟合更适合的，这就需要一个判断标准。常用的基于<strong>Mean squared error (MSE)</strong>：<br><img src="https://i.imgur.com/adK8AA2.png" alt=""></p><p>如果让计算机来实现这个过程，怎么办呢？<strong>梯度下降法（Gradient Descent）</strong>是一种常见手段。<br>该方法的核心是：<br><img src="https://i.imgur.com/i2kZYPl.png" alt=""></p><p>其中α是下降步长，人为确定。可以看出来，梯度下降法的目的是使J(θi,θj)越来越小，对于其是否线性，参数的数量，都没有限制。</p><p>最后利用梯度下降法代入导数（不论是链式法则，还是单个变量代入）得到最优的拟合函数参数：<br><img src="https://i.imgur.com/VBHlmsF.jpg" alt=""></p><h3 id="4-Model-and-Cost-Function"><a href="#4-Model-and-Cost-Function" class="headerlink" title="4. Model and Cost Function"></a>4. Model and Cost Function</h3><p>再次回顾一下监督学习，两类：<strong>regression和classification</strong>。<br>简单来说，<strong>regression是predict real-valued output，classification是predict discrete-valued output。</strong><br>注明了以下符号，方便以后沟通：<br><img src="https://i.imgur.com/0Lr7GzM.png" alt=""></p><p>我们设计拟合，目的就是为了使得表示拟合数据与已知真实数据直接差距的cost function最小，这个cost function是啥？就是我们之前提到的判断标准，常见的为：<br><img src="https://i.imgur.com/GiYdN6C.png" alt=""><br>当有两个未知量时，cost function就需要表示为三维图：<br><img src="https://i.imgur.com/DJLSXXG.jpg" alt=""></p><p>为了方便表示，也可以把上图表示为类似地理上的“等高线”。术语称为 <strong>contour plot（轮廓图）</strong>：<br><img src="https://i.imgur.com/ZbnTLxm.jpg" alt=""></p><h3 id="5-Parameter-Learning"><a href="#5-Parameter-Learning" class="headerlink" title="5. Parameter Learning"></a>5. Parameter Learning</h3><p>现在，我们开始具体来看看，第一个机器学习算法：<strong>梯度下降法（gradient descent）</strong></p><p><img src="https://i.imgur.com/SAMHpSX.png" alt=""><br><strong>特别注意</strong>，这里的temp是为了保证更新是同一时间发生的，这才是最正宗的梯度下降法。如果先更新了θ0，再用更新之后的θ0去更新θ1，就违背了梯度下降法的初衷。</p><p>可能你已经发现，<strong>这里的最优值，仅仅是局部的（local）而不是全局的（global）</strong>。因此选取的初值不同，可能会导致算法停留在不同的最优值。因此，这是梯度下降法的一个缺陷。但是对于线性回归而言，cost function 是一个convex function（形状为弓形），仅有一个最优值，局部最优值就是全局最优值。</p><p>到此为止，我们说的其实都是：<strong>batch</strong> gradient descent。也就是我的cost function是所有样本的MSE之和。如果不是计算所有样本，仅仅是计算某个重要子集的MSE之和，这个方法在以后的课程中会提到。</p><p>最后，提到线性代数中的寻找极值的方法：<strong>normal equations</strong>。但是在大规模的数据计算中，还是梯度下降法更为适用。</p><h3 id="6-Linear-Algebra-Review"><a href="#6-Linear-Algebra-Review" class="headerlink" title="6. Linear Algebra Review"></a>6. Linear Algebra Review</h3><p>矩阵：<a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Matrix_(mathematics)</a></p><p>向量：特殊的矩阵，注意是 列数 = 1</p><p>1-index：下标从1开始。0-index：下标从0开始。</p><p>矩阵乘法没有交换律，但是有结合律。</p><p>只有方阵（#col=#row）才有逆矩阵。但仅仅满足方阵这一个条件并不足够，同时需要满足行列式不等于0。没有逆矩阵的矩阵，称之为“奇异（singular）矩阵”或“退化（degenerate）矩阵”。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(5) 常见算法优缺点</title>
    <link href="https://paradoxallen.github.io/65434/"/>
    <id>https://paradoxallen.github.io/65434/</id>
    <published>2017-06-09T16:00:00.000Z</published>
    <updated>2018-06-13T05:09:21.235Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p><a href="https://paradoxallen.github.io/62602/">机器学习(三) 模型结果应用</a></p><p><a href="https://paradoxallen.github.io/21484/">机器学习(四) 常见算法优缺点</a></p><p>文章结构：</p><ul><li><p><strong>什么是感知器分类算法</strong></p></li><li><p><strong>在Python中实现感知器学习算法</strong></p></li></ul><p><em>在iris（鸢尾花）数据集上训练一个感知器模型</em></p><ul><li><strong>自适应线性神经元和融合学习</strong></li></ul><p><em>使用梯度下降方法来最小化损失函数</em></p><p><em>在Python中实现一个自适应的线性神经元</em></p><a id="more"></a><hr><h3 id="什么是感知器分类算法"><a href="#什么是感知器分类算法" class="headerlink" title="什么是感知器分类算法"></a><strong>什么是感知器分类算法</strong></h3><p>设想我们改变逻辑回归算法，“迫使”它只能输出-1或1抑或其他定值。在这种情况下，之前的逻辑函数‍‍g就会变成阈值函数sign：</p><p><img src="https://i.imgur.com/TwrMWwh.png" alt=""></p><p><img src="https://i.imgur.com/pDzWDxS.png" alt=""></p><p>如果我们令假设为hθ(x)=g(θTx)hθ(x)=g(θTx)，将其带入之前的迭代法中：</p><p><img src="https://i.imgur.com/r4P3819.png" alt=""></p><p>至此我们就得出了感知器学习算法。简单地来说，感知器学习算法是神经网络中的一个概念，单层感知器是最简单的神经网络，输入层和输出层直接相连。</p><p><img src="https://i.imgur.com/Nb3JtYy.png" alt=""></p><p>每一个输入端和其上的权值相乘，然后将这些乘积相加得到乘积和，这个结果与阈值相比较（一般为0），若大于阈值输出端就取1，反之，输出端取-1。</p><p>初始权重向量W=[0,0,0]，更新公式W(i)=W(i)+ΔW(i)；ΔW(i)=η<em>(y-y’)</em>X(i)； </p><p>η：学习率，介于[0,1]之间 </p><p>y：输入样本的正确分类 </p><p>y’：感知器计算出来的分类 </p><p>通过上面公式不断更新权值，直到达到分类要求。</p><p><img src="https://i.imgur.com/RlHERhT.jpg" alt=""></p><p>初始化权重向量W，与输入向量做点乘，将结果与阈值作比较，得到分类结果1或-1。</p><hr><h3 id="在Python中实现感知器学习算法"><a href="#在Python中实现感知器学习算法" class="headerlink" title="在Python中实现感知器学习算法"></a><strong>在Python中实现感知器学习算法</strong></h3><p>下面直接贴上实现代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Perceptron(object):</span><br><span class="line">    &quot;&quot;&quot;Perceptron classifier.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ------------</span><br><span class="line">    eta : float</span><br><span class="line">        Learning rate (between 0.0 and 1.0)</span><br><span class="line">    n_iter : int</span><br><span class="line">        Passes over the training dataset.</span><br><span class="line"></span><br><span class="line">    Attributes</span><br><span class="line">    -----------</span><br><span class="line">    w_ : 1d-array</span><br><span class="line">        Weights after fitting.</span><br><span class="line">    errors_ : list</span><br><span class="line">        Number of misclassifications (updates) in each epoch.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, eta=0.01, n_iter=10):</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y):</span><br><span class="line">        &quot;&quot;&quot;Fit training data.</span><br><span class="line"></span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span><br><span class="line">            Training vectors, where n_samples is the number of samples and</span><br><span class="line">            n_features is the number of features.</span><br><span class="line">        y : array-like, shape = [n_samples]</span><br><span class="line">            Target values.</span><br><span class="line"></span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        self : object</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.w_ = np.zeros(1 + X.shape[1])</span><br><span class="line">        self.errors_ = []</span><br><span class="line"></span><br><span class="line">        for _ in range(self.n_iter):</span><br><span class="line">            errors = 0</span><br><span class="line">            for xi, target in zip(X, y):</span><br><span class="line">                update = self.eta * (target - self.predict(xi))</span><br><span class="line">                self.w_[1:] += update * xi</span><br><span class="line">                self.w_[0] += update</span><br><span class="line">                errors += int(update != 0.0)</span><br><span class="line">            self.errors_.append(errors)</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def net_input(self, X):</span><br><span class="line">        &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span><br><span class="line">        return np.dot(X, self.w_[1:]) + self.w_[0]</span><br><span class="line"></span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span><br><span class="line">        return np.where(self.net_input(X) &gt;= 0.0, 1, -1)</span><br></pre></td></tr></table></figure><p><strong>特别说明：</strong></p><p>学习速率η(eta)只有在权重（一般取值0或者很小的数）为非零值的时候，才会对分类结果产生作用。如果所有的权重都初始化为0，学习速率参数eta只影响权重向量的大小，而不影响其方向，为了使学习速率影响分类结果，权重需要初始化为非零值。需要更改的代码中的相应行在下面突出显示:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, eta=0.01, n_iter=50, random_seed=1): # add random_seed=1</span><br><span class="line">    ...</span><br><span class="line">    self.random_seed = random_seed # add this line</span><br><span class="line">def fit(self, X, y):</span><br><span class="line">    ...</span><br><span class="line">    # self.w_ = np.zeros(1 + X.shape[1]) ## remove this line</span><br><span class="line">    rgen = np.random.RandomState(self.random_seed) # add this line</span><br><span class="line">    self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1]) # add this line</span><br></pre></td></tr></table></figure></p><p><strong>在iris（鸢尾）数据集上训练一个感知器模型</strong></p><p><strong>读取iris数据集</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import collections</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(&apos;https://archive.ics.uci.edu/ml/&apos;</span><br><span class="line">        &apos;machine-learning-databases/iris/iris.data&apos;, header=None)</span><br><span class="line">print (df.head())</span><br><span class="line">print (&quot;\n&quot;)</span><br><span class="line">print (df.describe())</span><br><span class="line">print (&quot;\n&quot;)</span><br><span class="line">print (collections.Counter(df[4]))</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/tRDUNXi.jpg" alt=""></p><p><strong>可视化iris数据</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 为了显示中文(这里是Mac的解决方法，其他的大家可以去百度一下)</span><br><span class="line">from matplotlib.font_manager import FontProperties</span><br><span class="line">font = FontProperties(fname=&apos;/System/Library/Fonts/STHeiti Light.ttc&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 选择 setosa and versicolor类型的花</span><br><span class="line">y = df.iloc[0:100, 4].values</span><br><span class="line">y = np.where(y == &apos;Iris-setosa&apos;, -1, 1)</span><br><span class="line"></span><br><span class="line"># 提取它们的特征 （sepal length and petal length）</span><br><span class="line">X = df.iloc[0:100, [0, 2]].values</span><br><span class="line"></span><br><span class="line"># 可视化数据，因为数据有经过处理，总共150行数据，1-50行是setosa花，51-100是versicolor花，101-150是virginica花</span><br><span class="line">plt.scatter(X[:50, 0], X[:50, 1],</span><br><span class="line">            color=&apos;red&apos;, marker=&apos;o&apos;, label=&apos;setosa&apos;)</span><br><span class="line">plt.scatter(X[50:100, 0], X[50:100, 1],</span><br><span class="line">            color=&apos;blue&apos;, marker=&apos;x&apos;, label=&apos;versicolor&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlabel(&apos;sepal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;petal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/wR17A5s.png" alt=""></p><p><strong>训练感知器模型</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Perceptron是我们前面定义的感知器算法函数，这里就直接调用就好</span><br><span class="line">ppn = Perceptron(eta=0.1, n_iter=10)</span><br><span class="line"></span><br><span class="line">ppn.fit(X, y)</span><br><span class="line"></span><br><span class="line">plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=&apos;o&apos;)</span><br><span class="line">plt.xlabel(&apos;迭代次数&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;权重更新次数（错误次数）&apos;,FontProperties=font,fontsize=14)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/e6o2LBT.png" alt=""></p><p><strong>绘制函数决策区域</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib.colors import ListedColormap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot_decision_regions(X, y, classifier, resolution=0.02):</span><br><span class="line"></span><br><span class="line">    # setup marker generator and color map</span><br><span class="line">    markers = (&apos;s&apos;, &apos;x&apos;, &apos;o&apos;, &apos;^&apos;, &apos;v&apos;)</span><br><span class="line">    colors = (&apos;red&apos;, &apos;blue&apos;, &apos;lightgreen&apos;, &apos;gray&apos;, &apos;cyan&apos;)</span><br><span class="line">    cmap = ListedColormap(colors[:len(np.unique(y))])</span><br><span class="line"></span><br><span class="line">    # plot the decision surface</span><br><span class="line">    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),</span><br><span class="line">                           np.arange(x2_min, x2_max, resolution))</span><br><span class="line">    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br><span class="line">    Z = Z.reshape(xx1.shape)</span><br><span class="line">    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)</span><br><span class="line">    plt.xlim(xx1.min(), xx1.max())</span><br><span class="line">    plt.ylim(xx2.min(), xx2.max())</span><br><span class="line"></span><br><span class="line">    # plot class samples</span><br><span class="line">    for idx, cl in enumerate(np.unique(y)):</span><br><span class="line">        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],</span><br><span class="line">                    alpha=0.8, c=cmap(idx),</span><br><span class="line">                    edgecolor=&apos;black&apos;,</span><br><span class="line">                    marker=markers[idx], </span><br><span class="line">                    label=cl)</span><br><span class="line">plot_decision_regions(X, y, classifier=ppn)</span><br><span class="line">plt.xlabel(&apos;sepal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;petal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/AFltDVw.png" alt=""></p><hr><h3 id="自适应线性神经元和融合学习"><a href="#自适应线性神经元和融合学习" class="headerlink" title="自适应线性神经元和融合学习"></a><strong>自适应线性神经元和融合学习</strong></h3><p><strong>使用梯度下降方法来最小化损失函数</strong></p><p>梯度下降的方法十分常见，具体的了解可以参考附录的文章[2]，如今，梯度下降主要用于在神经网络模型中进行权重更新，即在一个方向上更新和调整模型的参数，来最小化损失函数。</p><p><img src="https://i.imgur.com/pYoV9cF.jpg" alt=""><br>图：梯度下降原理过程演示</p><p><strong>在Python中实现一个自适应的线性神经元</strong></p><p>先贴上定义的python函数，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"># 定义神经元函数</span><br><span class="line">class AdalineGD(object):</span><br><span class="line">    &quot;&quot;&quot;ADAptive LInear NEuron classifier.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ------------</span><br><span class="line">    eta : float</span><br><span class="line">        Learning rate (between 0.0 and 1.0)</span><br><span class="line">    n_iter : int</span><br><span class="line">        Passes over the training dataset.</span><br><span class="line"></span><br><span class="line">    Attributes</span><br><span class="line">    -----------</span><br><span class="line">    w_ : 1d-array</span><br><span class="line">        Weights after fitting.</span><br><span class="line">    cost_ : list</span><br><span class="line">        Sum-of-squares cost function value in each epoch.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, eta=0.01, n_iter=50):</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y):</span><br><span class="line">        &quot;&quot;&quot; Fit training data.</span><br><span class="line"></span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span><br><span class="line">            Training vectors, where n_samples is the number of samples and</span><br><span class="line">            n_features is the number of features.</span><br><span class="line">        y : array-like, shape = [n_samples]</span><br><span class="line">            Target values.</span><br><span class="line"></span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        self : object</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.w_ = np.zeros(1 + X.shape[1])</span><br><span class="line">        self.cost_ = []</span><br><span class="line"></span><br><span class="line">        for i in range(self.n_iter):</span><br><span class="line">            net_input = self.net_input(X)</span><br><span class="line">            # Please note that the &quot;activation&quot; method has no effect</span><br><span class="line">            # in the code since it is simply an identity function. We</span><br><span class="line">            # could write `output = self.net_input(X)` directly instead.</span><br><span class="line">            # The purpose of the activation is more conceptual, i.e.,  </span><br><span class="line">            # in the case of logistic regression, we could change it to</span><br><span class="line">            # a sigmoid function to implement a logistic regression classifier.</span><br><span class="line">            output = self.activation(X)</span><br><span class="line">            errors = (y - output)</span><br><span class="line">            self.w_[1:] += self.eta * X.T.dot(errors)</span><br><span class="line">            self.w_[0] += self.eta * errors.sum()</span><br><span class="line">            cost = (errors**2).sum() / 2.0</span><br><span class="line">            self.cost_.append(cost)</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def net_input(self, X):</span><br><span class="line">        &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span><br><span class="line">        return np.dot(X, self.w_[1:]) + self.w_[0]</span><br><span class="line"></span><br><span class="line">    def activation(self, X):</span><br><span class="line">        &quot;&quot;&quot;Compute linear activation&quot;&quot;&quot;</span><br><span class="line">        return self.net_input(X)</span><br><span class="line"></span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span><br><span class="line">        return np.where(self.activation(X) &gt;= 0.0, 1, -1)</span><br></pre></td></tr></table></figure></p><p><strong>查看不同学习率下的错误率随迭代次数的变化情况：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))</span><br><span class="line"></span><br><span class="line"># 可视化W调整的过程中，错误率随迭代次数的变化</span><br><span class="line">ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)</span><br><span class="line">ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker=&apos;o&apos;)</span><br><span class="line">ax[0].set_xlabel(&apos;Epochs&apos;)</span><br><span class="line">ax[0].set_ylabel(&apos;log(Sum-squared-error)&apos;)</span><br><span class="line">ax[0].set_title(&apos;Adaline - Learning rate 0.01&apos;)</span><br><span class="line"></span><br><span class="line">ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)</span><br><span class="line">ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker=&apos;o&apos;)</span><br><span class="line">ax[1].set_xlabel(&apos;Epochs&apos;)</span><br><span class="line">ax[1].set_ylabel(&apos;Sum-squared-error&apos;)</span><br><span class="line">ax[1].set_title(&apos;Adaline - Learning rate 0.0001&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/g6mKKU3.png" alt=""></p><p><strong>iris数据的应用情况：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 标准化特征</span><br><span class="line">X_std = np.copy(X)</span><br><span class="line">X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()</span><br><span class="line">X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()</span><br><span class="line"># 调用函数开始训练</span><br><span class="line">ada = AdalineGD(n_iter=15, eta=0.01)</span><br><span class="line">ada.fit(X_std, y)</span><br><span class="line"># 绘制效果</span><br><span class="line">plot_decision_regions(X_std, y, classifier=ada)</span><br><span class="line">plt.title(&apos;Adaline - Gradient Descent&apos;)</span><br><span class="line">plt.xlabel(&apos;sepal length [standardized]&apos;)</span><br><span class="line">plt.ylabel(&apos;petal length [standardized]&apos;)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"># 可视化W调整的过程中，错误率随迭代次数的变化</span><br><span class="line">plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=&apos;o&apos;)</span><br><span class="line">plt.xlabel(&apos;Epochs&apos;)</span><br><span class="line">plt.ylabel(&apos;Sum-squared-error&apos;)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/kRorVXJ.png" alt=""></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>1）<a href="https://blog.csdn.net/u013719780/article/details/51755409" target="_blank" rel="noopener">机器学习系列：感知器</a><br>2）<a href="https://blog.csdn.net/zyq522376829/article/details/66632699" target="_blank" rel="noopener">机器学习入门系列04，Gradient Descent（梯度下降法）</a><br>3）<a href="https://zhuanlan.zhihu.com/p/27449596?utm_source=weibo&amp;utm_medium=social" target="_blank" rel="noopener">一文看懂各种神经网络优化算法：从梯度下降到Adam方法</a><br>4）<a href="https://blog.csdn.net/huakai16/article/details/77701020" target="_blank" rel="noopener">机器学习与神经网络（三）：自适应线性神经元的介绍和Python代码实现</a><br>5）<a href="http://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb" target="_blank" rel="noopener">《Training Machine Learning Algorithms for Classification》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/62602/&quot;&gt;机器学习(三) 模型结果应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/21484/&quot;&gt;机器学习(四) 常见算法优缺点&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文章结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;什么是感知器分类算法&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;在Python中实现感知器学习算法&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;在iris（鸢尾花）数据集上训练一个感知器模型&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;自适应线性神经元和融合学习&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;使用梯度下降方法来最小化损失函数&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;在Python中实现一个自适应的线性神经元&lt;/em&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="算法" scheme="https://paradoxallen.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(4) 常见算法优缺点</title>
    <link href="https://paradoxallen.github.io/21484/"/>
    <id>https://paradoxallen.github.io/21484/</id>
    <published>2017-06-04T16:00:00.000Z</published>
    <updated>2018-06-13T05:09:08.547Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p><a href="https://paradoxallen.github.io/62602/">机器学习(三) 模型结果应用</a></p><p>机器学习算法我们了解了很多，但是放在一起来比较优缺点是缺少的，本篇文章就一些常见的算法来进行一次优缺点梳理。</p><a id="more"></a><hr><h3 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a><strong>决策树算法</strong></h3><h4 id="一、决策树优点"><a href="#一、决策树优点" class="headerlink" title="一、决策树优点"></a><strong>一、决策树优点</strong></h4><p>1、决策树易于理解和解释，可以可视化分析，容易提取出规则。</p><p>2、可以同时处理标称型和数值型数据。</p><p>3、测试数据集时，运行速度比较快。</p><p>4、决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。</p><h4 id="二、决策树缺点"><a href="#二、决策树缺点" class="headerlink" title="二、决策树缺点"></a><strong>二、决策树缺点</strong></h4><p>1、对缺失数据处理比较困难。</p><p>2、容易出现过拟合问题。</p><p>3、忽略数据集中属性的相互关联。</p><p>4、ID3算法计算信息增益时结果偏向数值比较多的特征。</p><h4 id="三、改进措施"><a href="#三、改进措施" class="headerlink" title="三、改进措施"></a><strong>三、改进措施</strong></h4><p>1、对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。</p><p>2、使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题</p><h4 id="四、常见算法"><a href="#四、常见算法" class="headerlink" title="四、常见算法"></a><strong>四、常见算法</strong></h4><h5 id="一）C4-5算法"><a href="#一）C4-5算法" class="headerlink" title="一）C4.5算法"></a><strong>一）C4.5算法</strong></h5><p>ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定的测试属性。</p><p>C4.5算法核心思想是ID3算法，是ID3算法的改进，改进方面有：</p><ul><li><p>用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；</p></li><li><p>在树构造过程中进行剪枝；</p></li><li><p>能处理非离散的数据；</p></li><li><p>能处理不完整的数据。</p></li></ul><p><strong>优点</strong>：产生的分类规则易于理解，准确率较高。</p><p><strong>缺点</strong>：</p><p>1）在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效；</p><p>2）C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</p><h5 id="二）CART分类与回归树"><a href="#二）CART分类与回归树" class="headerlink" title="二）CART分类与回归树"></a><strong>二）CART分类与回归树</strong></h5><p>是一种决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数<br>据集生成的决策树的拓展形。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。</p><p><strong>优点</strong></p><p>1）非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。</p><p>2）在面对诸如存在缺失值、变量数多等问题时CART 显得非常稳健。</p><hr><h3 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a><strong>分类算法</strong></h3><h4 id="一、KNN算法"><a href="#一、KNN算法" class="headerlink" title="一、KNN算法"></a><strong>一、KNN算法</strong></h4><p><strong>KNN算法的优点</strong> </p><p>1、KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练</p><p>2、KNN理论简单，容易实现</p><p><strong>KNN算法的缺点</strong></p><p>1、对于样本容量大的数据集计算量比较大。</p><p>2、样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。</p><p>3、KNN每一次分类都会重新进行一次全局运算。</p><p>4、k值大小的选择。</p><p><strong>KNN算法应用领域</strong></p><p>文本分类、模式识别、聚类分析，多分类领域</p><h4 id="二、支持向量机（SVM）"><a href="#二、支持向量机（SVM）" class="headerlink" title="二、支持向量机（SVM）"></a><strong>二、支持向量机（SVM）</strong></h4><p>支持向量机是一种基于分类边界的方法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。基于分类边界的分类算法的目标是，通过训练，找到这些分类之间的边界（直线的――称为线性划分，曲线的――称为非线性划分）。对于多维数据（如N维），可以将它们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面（超面比N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。</p><p>支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。</p><p><strong>SVM优点</strong></p><p>1、解决小样本下机器学习问题。<br>2、解决非线性问题。<br>3、无局部极小值问题。（相对于神经网络等算法）<br>4、可以很好的处理高维数据集。<br>5、泛化能力比较强。</p><p><strong>SVM缺点</strong></p><p>1、对于核函数的高维映射解释力不强，尤其是径向基函数。<br>2、对缺失数据敏感。</p><p><strong>SVM应用领域</strong></p><p>文本分类、图像识别、主要二分类领域</p><h4 id="三、朴素贝叶斯算法"><a href="#三、朴素贝叶斯算法" class="headerlink" title="三、朴素贝叶斯算法"></a><strong>三、朴素贝叶斯算法</strong></h4><p><strong>朴素贝叶斯算法优点</strong></p><p>1、对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。<br>2、支持增量式运算。即可以实时的对新增的样本进行训练。<br>3、朴素贝叶斯对结果解释容易理解。</p><p><strong>朴素贝叶斯缺点</strong></p><p>1、由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。</p><p><strong>朴素贝叶斯应用领域</strong></p><p>文本分类、欺诈检测中使用较多</p><h4 id="四、Logistic回归算法"><a href="#四、Logistic回归算法" class="headerlink" title="四、Logistic回归算法"></a><strong>四、Logistic回归算法</strong></h4><p><strong>logistic回归优点</strong></p><p>1、计算代价不高，易于理解和实现</p><p><strong>logistic回归缺点</strong></p><p>1、容易产生欠拟合。</p><p>2、分类精度不高。</p><p><strong>logistic回归应用领域</strong></p><p>用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。</p><p>Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。</p><hr><h3 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a><strong>聚类算法</strong></h3><h4 id="一、K-means-算法"><a href="#一、K-means-算法" class="headerlink" title="一、K means 算法"></a><strong>一、K means 算法</strong></h4><p>是一个简单的聚类算法，把n的对象根据他们的属性分为k个分割，k&lt; n。 算法的核心就是要优化失真函数J,使其收敛到局部最小值但不是全局最小值。<br>其中N为样本数，K是簇数，rnk b表示n属于第k个簇，uk 是第k个中心点的值。然后求出最优的uk</p><p><strong>优点</strong>：算法速度很快</p><p><strong>缺点</strong>：分组的数目k是一个输入参数，不合适的k可能返回较差的结果。</p><h4 id="二、EM最大期望算法"><a href="#二、EM最大期望算法" class="headerlink" title="二、EM最大期望算法"></a><strong>二、EM最大期望算法</strong></h4><p>EM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。</p><p>EM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。</p><hr><h3 id="集成算法（AdaBoost算法）"><a href="#集成算法（AdaBoost算法）" class="headerlink" title="集成算法（AdaBoost算法）"></a><strong>集成算法（AdaBoost算法）</strong></h3><h4 id="一、-AdaBoost算法优点"><a href="#一、-AdaBoost算法优点" class="headerlink" title="一、  AdaBoost算法优点"></a><strong>一、  AdaBoost算法优点</strong></h4><p>1、很好的利用了弱分类器进行级联。</p><p>2、可以将不同的分类算法作为弱分类器。</p><p>3、AdaBoost具有很高的精度。</p><p>4、相对于bagging算法和Random Forest算法，AdaBoost充分考虑的每个分类器的权重。</p><h4 id="二、Adaboost算法缺点"><a href="#二、Adaboost算法缺点" class="headerlink" title="二、Adaboost算法缺点"></a><strong>二、Adaboost算法缺点</strong></h4><p>1、AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。</p><p>2、数据不平衡导致分类精度下降。</p><p>3、训练比较耗时，每次重新选择当前分类器最好切分点。</p><h4 id="三、AdaBoost应用领域"><a href="#三、AdaBoost应用领域" class="headerlink" title="三、AdaBoost应用领域"></a><strong>三、AdaBoost应用领域</strong></h4><p>模式识别、计算机视觉领域，用于二分类和多分类场景</p><hr><h3 id="人工神经网络算法"><a href="#人工神经网络算法" class="headerlink" title="人工神经网络算法"></a><strong>人工神经网络算法</strong></h3><h4 id="一、神经网络优点"><a href="#一、神经网络优点" class="headerlink" title="一、神经网络优点"></a><strong>一、神经网络优点</strong></h4><p>1、分类准确度高，学习能力极强。</p><p>2、对噪声数据鲁棒性和容错性较强。</p><p>3、有联想能力，能逼近任意非线性关系。</p><h4 id="二、神经网络缺点"><a href="#二、神经网络缺点" class="headerlink" title="二、神经网络缺点"></a><strong>二、神经网络缺点</strong></h4><p>1、神经网络参数较多，权值和阈值。</p><p>2、黑盒过程，不能观察中间结果。</p><p>3、学习过程比较长，有可能陷入局部极小值。</p><h4 id="三、人工神经网络应用领域"><a href="#三、人工神经网络应用领域" class="headerlink" title="三、人工神经网络应用领域"></a><strong>三、人工神经网络应用领域</strong></h4><p>目前深度神经网络已经应用与计算机视觉，自然语言处理，语音识别等领域并取得很好的效果。</p><hr><h3 id="排序算法（PageRank）"><a href="#排序算法（PageRank）" class="headerlink" title="排序算法（PageRank）"></a><strong>排序算法（PageRank）</strong></h3><p>PageRank是google的页面排序算法，是基于从许多优质的网页链接过来的网页，必定还是优质网页的回归关系，来判定所有网页的重要性。（也就是说，一个人有着越多牛X朋友的人，他是牛X的概率就越大。）</p><h4 id="一、PageRank优点"><a href="#一、PageRank优点" class="headerlink" title="一、PageRank优点"></a><strong>一、PageRank优点</strong></h4><p>完全独立于查询，只依赖于网页链接结构，可以离线计算。</p><h4 id="二、PageRank缺点"><a href="#二、PageRank缺点" class="headerlink" title="二、PageRank缺点"></a><strong>二、PageRank缺点</strong></h4><p>1）PageRank算法忽略了网页搜索的时效性。</p><p>2）旧网页排序很高，存在时间长，积累了大量的in-links，拥有最新资讯的新网页排名却很低，因为它们几乎没有in-links。</p><hr><h3 id="关联规则算法（Apriori算法）"><a href="#关联规则算法（Apriori算法）" class="headerlink" title="关联规则算法（Apriori算法）"></a><strong>关联规则算法（Apriori算法）</strong></h3><p>Apriori算法是一种挖掘关联规则的算法，用于挖掘其内含的、未知的却又实际存在的数据关系，其核心是基于两阶段频集思想的递推算法 。</p><p><strong>Apriori算法分为两个阶段：</strong></p><p>1）寻找频繁项集</p><p>2）由频繁项集找关联规则</p><p><strong>算法缺点：</strong></p><p>1）在每一步产生侯选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；</p><p>2） 每次计算项集的支持度时，都对数据库中    的全部记录进行了一遍扫描比较，需要很大的I/O负载。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1）Jason Brownlee  《How To Use Machine Learning Results》</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/62602/&quot;&gt;机器学习(三) 模型结果应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;机器学习算法我们了解了很多，但是放在一起来比较优缺点是缺少的，本篇文章就一些常见的算法来进行一次优缺点梳理。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="算法" scheme="https://paradoxallen.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
