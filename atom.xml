<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RP&#39;s Blog</title>
  
  <subtitle>学习总结  思考感悟</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://paradoxallen.github.io/"/>
  <updated>2018-06-13T04:55:26.430Z</updated>
  <id>https://paradoxallen.github.io/</id>
  
  <author>
    <name>LRP</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>关于机器学习在大气科学的应用</title>
    <link href="https://paradoxallen.github.io/21048/"/>
    <id>https://paradoxallen.github.io/21048/</id>
    <published>2018-05-06T16:00:00.000Z</published>
    <updated>2018-06-13T04:55:26.430Z</updated>
    
    <content type="html"><![CDATA[<p>前阵子阅读了院里一位博士研究生师兄的一篇有关基于神经网络算法对北京近五年的常规探空数据进行自组织分类，并揭示出大气污染物在不同边界层结构下的演变规律和相关机制的文章<a href="https://www.atmos-chem-phys.net/18/6771/2018/" target="_blank" rel="noopener">《Self-organized classification of boundary layer meteorology and associated characteristics of air quality in Beijing》</a>，看完顿时心生膜拜之情；</p><p>然后恰巧也是那个时候吕教授在院群上也转发了一篇关于机器学习预测火势甚至天气的公众号文章<a href="https://mp.weixin.qq.com/s?__biz=MjM5ODE1NDYyMA==&amp;mid=2653384819&amp;idx=2&amp;sn=523f27cb9442ab4af27137edd1280248&amp;chksm=bd1cc8608a6b4176d7cae939f794e082cf86b5bf0ac0deb53790f507f563f588b2148687957c&amp;mpshare=1&amp;scene=1&amp;srcid=0517RYcQWk5dWJCcqoI7jLCe#rd" target="_blank" rel="noopener">《机器学习成功解决“蝴蝶效应”！以后你终于可以相信天气预报了》</a>。</p><p>加之自己报名了一个<a href="https://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;mid=2247487345&amp;idx=1&amp;sn=4acb8978a2d95f0a1926c0e07021bec3&amp;chksm=e89455fcdfe3dcea645499d9321177a99ea713ffe06fe4af9d18c8506dd285755be2a1640539&amp;mpshare=1&amp;scene=1&amp;srcid=04151pmRgGqnfZDpvBL9EKKk#rd" target="_blank" rel="noopener">“预测北京和伦敦两个城市的空气质量”的KDD Cup 2018</a>但是因为自己报名太晚，组队不成（其实更深层的是之前关于机器学习的内容已经忘得差不多了。。。）</p><p>如此的机缘巧合，感觉将机器学习应用于大气科学将前途无量。我自己也想在这一方向进行深入了解，接下来我会进行相关内容的学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前阵子阅读了院里一位博士研究生师兄的一篇有关基于神经网络算法对北京近五年的常规探空数据进行自组织分类，并揭示出大气污染物在不同边界层结构下的演变规律和相关机制的文章&lt;a href=&quot;https://www.atmos-chem-phys.net/18/6771/2018/&quot;
      
    
    </summary>
    
      <category term="个人随笔" scheme="https://paradoxallen.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="大气科学" scheme="https://paradoxallen.github.io/tags/%E5%A4%A7%E6%B0%94%E7%A7%91%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>统计推断(零) 章节简介</title>
    <link href="https://paradoxallen.github.io/5451/"/>
    <id>https://paradoxallen.github.io/5451/</id>
    <published>2018-03-31T16:00:00.000Z</published>
    <updated>2018-06-03T16:17:44.275Z</updated>
    
    <content type="html"><![CDATA[<p>《统计推断(翻译版·原书第2版)》从概率论的基础开始，通过例子与习题的旁征博引，引进了大量近代统计处理的新技术和一些国内同类教材中不常见而又广为使用的分布。</p><p>其内容既包括工科概率入门、经典统计和现代统计的基础，又加进了不少近代统计中数据处理的实用方法和思想，例如：Bootstrap再抽样法、刀切(Jackkrlife)估计、EM算法、Logistic回归、稳健(Robest)回归、Markov链、Monte Carlo方法等。</p><p>它的统计内容与国内流行的教材相比，理论较深，模型较多，案例的涉及面要广，理论的应用面要丰富，统计思想的阐述与算法更为具体。</p><p>《统计推断(翻译版·原书第2版)》可作为工科、管理类学科专业本科生、研究生的教材或参考书，也可供教师、工程技术人员自学之用。</p><a id="more"></a><hr><h3 id="章节简介"><a href="#章节简介" class="headerlink" title="章节简介"></a><strong>章节简介</strong></h3><p><strong>出版说明</strong><br><strong>第2版序</strong><br><strong>第1版序</strong><br><strong>译后序</strong><br><strong>第1章 概率论</strong><br>1.1 集合论<br>1.2 概率论基础<br>1.2.1 公理化基础<br>1.2.2 概率演算<br>1.2.3 计数<br>1.2.4 枚举结果<br>1.3 条件概率与独立性<br>1.4 随机变量<br>1.5 分布函数<br>1.6 概率密度函数和概率质量函数<br>1.7 习题<br>1.8 杂录<br><strong>第2章 变换和期望</strong><br>2.1 随机变量函数的分布<br>2.2 期望<br>2.3 矩和矩母函数<br>2.4 积分号下的求导<br>2.5 习题<br>2.6 杂录<br>2.6.1 矩列的唯一性<br>2.6.2 其他母函数<br>2.6.3 矩母函数能否唯一地确定分布？<br><strong>第3章 常见分布族</strong><br>3.1 引言<br>3.2 离散分布<br>3.3 连续分布<br>3.4 指数族<br>3.5 位置与尺度族<br>3.6 不等式与恒等式<br>3.6.1 概率不等式<br>3.6.2 恒等式<br>3.7 习题<br>3.8 杂录<br>3.8.1 Poisson假设<br>3.8.2 Chebychev不等式及其改进<br>3.8.3 再谈指数族<br><strong>第4章 多维随机变量</strong><br>4.1 联合分布与边缘分布<br>4.2 条件分布与独立性<br>4.3 二维变换<br>4.4 多层模型与混合分布<br>4.5 协方差与相关<br>4.6 多维分布<br>4.7 不等式<br>4.7.1 数值不等式<br>4.7.2 函数不等式<br>4.8 习题<br>4.9 杂录<br>4.9.1 交换悖论<br>4.9.2 算术－几何－调和平均值不等式<br>8.3.1 错误概率与功效函数<br>8.3.2 最大功效检验<br>8.3.3 并－检验与交－并检验的真实水平<br>8.3.4 P-值<br>8.3.5 损失函数最优性<br>8.4 习题<br>8.5 杂录<br>8.5.1 单调功效函数<br>8.5.2 似然比作为证据<br>8.5.3 P-值和后验概率<br>8.5.4 置信集P-值<br><strong>第9章 区间估计</strong><br>9.1 引言<br>9.2 区间估计量的求法<br>9.2.1 反转一个检验统计量<br>9.2.2 枢轴量<br>9.2.3 枢轴化累积分布函数<br>9.2.4 Bayes区间<br>9.3 区间估计量的评价方法<br>9.3.1 尺寸和覆盖概率<br>9.3.2 与检验相关的最优性<br>9.3.3 Bayes最优<br>9.3.4 损失函数最优<br>9.4 习题<br>9.5 杂录<br>9.5.1 置信方法<br>9.5.2 离散分布中的置信区间<br>9.5.3 Fieller定理<br>9.5.4 其他区间如何?<br><strong>第10章 渐近评价</strong><br>10.1 点估计<br>10.1.1 相合性<br>10.1.2 有效性<br>10.1.3 计算与比较<br>10.1.4 自助法标准误差<br>10.2 稳健性<br>10.2.1 均值和中位数<br>10.2.2 M_估计量<br>10.3 假设检验<br>10.3.1 LRT的渐近分布<br>10.3.2 其他大样本检验<br>10.4 区间估计<br>10.4.1 近似极大似然区间<br>10.4.2 其他大样本区间<br>10.5 习题<br>10.6 杂录<br>10.6.1 超有效性<br>10.6.2 适当的正则性条件<br>10.6.3 再谈自助法<br>10.6.4 影响函数<br>10.6.5 自助法区间<br>10.6.6 稳健区间<br><strong>第11章 方差分析和回归分析</strong><br>11.1 引言<br>11.2 一种方式分组的方差分析<br>11.2.1 模型和分布假定<br>11.2.2 经典的ANOVA假设<br>11.2.3 均值的线性组合的推断<br>11.2.4 ANOVAF检验<br>11.2.5 对比的同时估计<br>11.2.6 平方和的分解<br>11.3 简单线性回归<br>11.3.1 最小二乘：数学解<br>11.3.2 最佳线性无偏估计：统计解<br>11.3.3 模型和分布假定<br>11.3.4 正态误差下的估计和检验<br>11.3.5 在给定点x=x0处的估计和预测<br>11.3.6 同时估计和置信带<br>11.4 习题<br>11.5 杂录<br>11.5.1 Cochran定理<br>11.5.2 多重比较<br>11.5.3 随机化完全区组设计<br>11.5.4 其他类型的方差分析<br>11.5.5 置信带的形状<br>11.5.6 Stein悖论<br><strong>第12章 回归模型</strong><br>12.1 引言<br>12.2 变量有误差时的回归<br>12.2.1 函数关系和结构关系<br>12.2.2 最小二乘解<br>12.2.3 极大似然估计<br>12.2.4 置信集<br>12.3 罗吉斯蒂克回归<br>12.3.1 模型<br>12.3.2 估计<br>12.4 稳健回归<br>12.5 习题<br>12.6 杂录<br>12.6.1 函数和结构的意义<br>12.6.2 EIV模型中常规最小乘的相合性<br>12.6.3 EIV模型中的工具变量<br>12.6.4 罗吉斯蒂克似然方程<br>12.6.5 再谈稳健回归<br><strong>附录 计算机代数</strong><br><strong>常用分布表</strong><br><strong>参考文献</strong><br><strong>作者索引</strong><br><strong>名词索引</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《统计推断(翻译版·原书第2版)》从概率论的基础开始，通过例子与习题的旁征博引，引进了大量近代统计处理的新技术和一些国内同类教材中不常见而又广为使用的分布。&lt;/p&gt;
&lt;p&gt;其内容既包括工科概率入门、经典统计和现代统计的基础，又加进了不少近代统计中数据处理的实用方法和思想，例如：Bootstrap再抽样法、刀切(Jackkrlife)估计、EM算法、Logistic回归、稳健(Robest)回归、Markov链、Monte Carlo方法等。&lt;/p&gt;
&lt;p&gt;它的统计内容与国内流行的教材相比，理论较深，模型较多，案例的涉及面要广，理论的应用面要丰富，统计思想的阐述与算法更为具体。&lt;/p&gt;
&lt;p&gt;《统计推断(翻译版·原书第2版)》可作为工科、管理类学科专业本科生、研究生的教材或参考书，也可供教师、工程技术人员自学之用。&lt;/p&gt;
    
    </summary>
    
      <category term="应用统计" scheme="https://paradoxallen.github.io/categories/%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="统计" scheme="https://paradoxallen.github.io/tags/%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>博客搭建——主题Light修改教程</title>
    <link href="https://paradoxallen.github.io/46499/"/>
    <id>https://paradoxallen.github.io/46499/</id>
    <published>2018-01-22T16:00:00.000Z</published>
    <updated>2018-06-16T16:25:02.991Z</updated>
    
    <content type="html"><![CDATA[<p>这是部署了的我的博客：<a href="https://paradoxallen.github.io/">RP’s Blog</a>可以点击预览效果</p><hr><h3 id="添加“多说”评论与“页面导航”"><a href="#添加“多说”评论与“页面导航”" class="headerlink" title="添加“多说”评论与“页面导航”"></a>添加“多说”评论与“页面导航”</h3><p>在多说 进行注册，我用的是我的QQ号，获取通用代码。</p><p>将通用代码粘贴到 <code>themes\light\layout\_partial\comment.ejs</code> 里边，<br>其中<code>var duoshuoQuery = {short_name:&quot;paradoxallen&quot;}</code>里的paradoxallen是我的账号。</p><p>如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;% if (page.comments)&#123; %&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;nav id=&quot;pagination&quot; &gt;</span><br><span class="line">    &lt;% if (page.prev) &#123; %&gt;</span><br><span class="line">    &lt;a href=&quot;&lt;%- config.root %&gt;&lt;%- page.prev.path %&gt;&quot; class=&quot;alignleft prev&quot; &gt;&lt;%= __(&apos;prev&apos;) %&gt;&lt;/a&gt;</span><br><span class="line">    &lt;% &#125; %&gt;</span><br><span class="line">    &lt;% if (page.next) &#123; %&gt;</span><br><span class="line">    &lt;a href=&quot;&lt;%- config.root %&gt;&lt;%- page.next.path %&gt;&quot; class=&quot;alignright next&quot; &gt;&lt;%= __(&apos;next&apos;) %&gt;&lt;/a&gt;</span><br><span class="line">    &lt;% &#125; %&gt;</span><br><span class="line">    &lt;div class=&quot;clearfix&quot;&gt;&lt;/div&gt;</span><br><span class="line">&lt;/nav&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 多说评论框 start --&gt;</span><br><span class="line"> &lt;div class=&quot;ds-thread&quot; data-thread-key=&quot;&lt;%- config.root %&gt;&lt;%- item.path%&gt;&quot; data-title=&quot;&lt;%- item.title %&gt;&quot; data-url=&quot;&lt;%- item.permalink %&gt;&quot;&gt;&lt;/div&gt;</span><br><span class="line">&lt;!-- 多说评论框 end --&gt;</span><br><span class="line">&lt;!-- 多说公共JS代码 start (一个网页只需插入一次) --&gt;</span><br><span class="line">&lt;script type=&quot;text/javascript&quot;&gt;</span><br><span class="line">var duoshuoQuery = &#123;short_name:&quot;paradoxallen&quot;&#125;;</span><br><span class="line">  (function() &#123;</span><br><span class="line">    var ds = document.createElement(&apos;script&apos;);</span><br><span class="line">    ds.type = &apos;text/javascript&apos;;ds.async = true;</span><br><span class="line">    ds.src = (document.location.protocol == &apos;https:&apos; ? &apos;https:&apos; : &apos;http:&apos;) + &apos;//static.duoshuo.com/embed.js&apos;;</span><br><span class="line">    ds.charset = &apos;UTF-8&apos;;</span><br><span class="line">    (document.getElementsByTagName(&apos;head&apos;)[0] </span><br><span class="line">     || document.getElementsByTagName(&apos;body&apos;)[0]).appendChild(ds);</span><br><span class="line">  &#125;)();</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">&lt;!-- 多说公共JS代码 end --&gt;</span><br><span class="line">&lt;section id=&quot;comment&quot;&gt;</span><br><span class="line">&lt;/section&gt;</span><br><span class="line">&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure></p><h3 id="添加小图标"><a href="#添加小图标" class="headerlink" title="添加小图标"></a>添加小图标</h3><p>在<code>themes/light/layout/_partial/head.ejs</code>里将<code>&lt;link href=&quot;&lt;%- config.root %&gt;favicon.png&quot; rel=&quot;icon&quot;&gt;</code><br>替换为<code>&lt;link href=&quot;&lt;%- config.root %&gt;favicon.ico&quot; rel=&quot;icon&quot; type=&quot;image/x-ico&quot;&gt;</code>将favicon.ico图标文件放在source目录下。</p><h3 id="添加分类、标签云widget"><a href="#添加分类、标签云widget" class="headerlink" title="添加分类、标签云widget"></a>添加分类、标签云widget</h3><p>很简单，在<code>themes/light/_config.yml</code>中，添加如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">widgets:</span><br><span class="line">- category</span><br><span class="line">- tagcloud</span><br><span class="line">```   </span><br><span class="line">### 添加友情链接widget ###</span><br><span class="line">在`themes/light/layout/_widget`中新建名为`blogroll.ejs`的文件，编辑内容如下：</span><br></pre></td></tr></table></figure></p><p><div class="widget tag"></div></p><p></p><h3 class="title">友情链接</h3><p></p><p><ul class="entry"></ul></p><p><li><a href="http://paradoxallen.pw/" title="paradox的博客" target="_blank" rel="noopener">paradix</a></li></p><p><li><a href="http://paradoxallen.tk/" title="Hexo博客" target="_blank" rel="noopener">Hexo博客</a></li><br><br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 添加新浪微博widget(微博秀) ###</span><br><span class="line">去[新浪微博开放平台](http://app.weibo.com/tool/weiboshow)设置和生成微博秀代码。我用的139邮箱注册新浪。</span><br><span class="line">在`themes/light/layout/_widget`中新建名为`weibo.ejs`的文件，将刚才的代码直接保存到这里。</span><br><span class="line">在`themes/light/_config.yml`中，添加如下：</span><br></pre></td></tr></table></figure></p><p> widgets:  #站点右边栏，暂时默认，后面介绍修改和添加</p><ul><li>search<ul><li>category</li><li>tagcloud  #标签云</li><li>intro</li><li>weibo    #新浪微博秀</li><li>blogroll    #友情链接<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 主页文章显示摘要 ###</span><br><span class="line">编辑md文件的时候，在要作为摘要的文字后面添加``即可。</span><br><span class="line"></span><br><span class="line">### 修改背景图片、文字颜色等 ###</span><br><span class="line">找到`hexo\themes\light\source\css\_base\layout.styl`，在`body`下面增加一条`background-image url(&apos;/imgs/noise.png&apos;)`</span><br><span class="line">至于背景图片`\themes\light\source\imgs\bozhu.png`，用自己喜欢的即可。</span><br><span class="line">然后在`hexo\themes\light\source\css\_base\variable.styl`中修改`color`区块中的属性，就可以改变网站不同元素的颜色了。</span><br><span class="line"></span><br><span class="line">### 重头戏来了，我修改的themes\light\_config.yml文件 ###</span><br></pre></td></tr></table></figure></li></ul></li></ul><p>menu: #站点右上角导航栏，暂时默认<br>  首页: /<br>  归档: /archives<br>  关于: /about</p><p>widgets:  #站点右边栏</p><ul><li>search</li><li>category</li><li>tagcloud  #标签云</li><li>intro</li><li>weibo    #新浪微博秀</li><li>blogroll    #友情链接</li></ul><p>excerpt_link: 阅读全文 #替换为中文</p><p>plugins: </p><ul><li>hexo-generator-feed</li></ul><p>twitter: #右边栏要显示twitter展示的话，需要在此设置<br>  username:<br>  show_replies: false<br>  tweet_count: 5</p><p>addthis: #SNS分享，身在天朝，当然用“百度分享”，暂时默认<br>  enable: true<br>  pubid:<br>  facebook: true<br>  twitter: true<br>  google: true<br>  pinterest: true</p><p>fancybox: true #图片效果，默认<br>google_analytics: #要使用google_analytics进行统计的话，这里需要配置ID<br>rss: /atom.xml #生成RSS，需要配置路径<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 根目录下的_config.yml文件设置 ###</span><br></pre></td></tr></table></figure></p><h1 id="Hexo-Configuration"><a href="#Hexo-Configuration" class="headerlink" title="Hexo Configuration"></a>Hexo Configuration</h1><h2 id="Docs-http-hexo-io-docs-configuration-html"><a href="#Docs-http-hexo-io-docs-configuration-html" class="headerlink" title="Docs: http://hexo.io/docs/configuration.html"></a>Docs: <a href="http://hexo.io/docs/configuration.html" target="_blank" rel="noopener">http://hexo.io/docs/configuration.html</a></h2><h2 id="Source-https-github-com-hexojs-hexo"><a href="#Source-https-github-com-hexojs-hexo" class="headerlink" title="Source: https://github.com/hexojs/hexo/"></a>Source: <a href="https://github.com/hexojs/hexo/" target="_blank" rel="noopener">https://github.com/hexojs/hexo/</a></h2><h1 id="Site"><a href="#Site" class="headerlink" title="Site"></a>Site</h1><p>title: RP’s Blog<br>subtitle:<br>description: 学习总结 思考感悟  # 网站描述<br>author: paradox<br>email: <a href="mailto:paradoxallen@gmail.com" target="_blank" rel="noopener">paradoxallen@gmail.com</a><br>language: zh-CN</p><h1 id="URL"><a href="#URL" class="headerlink" title="URL"></a>URL</h1><h2 id="If-your-site-is-put-in-a-subdirectory-set-url-as-‘http-yoursite-com-child-39-and-root-as-‘-child-‘"><a href="#If-your-site-is-put-in-a-subdirectory-set-url-as-‘http-yoursite-com-child-39-and-root-as-‘-child-‘" class="headerlink" title="If your site is put in a subdirectory, set url as ‘http://yoursite.com/child&#39; and root as ‘/child/‘"></a>If your site is put in a subdirectory, set url as ‘<a href="http://yoursite.com/child&#39;" target="_blank" rel="noopener">http://yoursite.com/child&#39;</a> and root as ‘/child/‘</h2><p>url: <a href="http://paradoxallen.github.io">http://paradoxallen.github.io</a><br>root: /<br>permalink: :year/:month/:day/:title/<br>tag_dir: tags<br>archive_dir: archives<br>category_dir: categories<br>code_dir: downloads/code<br>permalink_defaults:</p><h1 id="Directory"><a href="#Directory" class="headerlink" title="Directory"></a>Directory</h1><p>source_dir: source<br>public_dir: public</p><h1 id="Writing-文章布局、写作格式的定义，不修改"><a href="#Writing-文章布局、写作格式的定义，不修改" class="headerlink" title="Writing 文章布局、写作格式的定义，不修改"></a>Writing 文章布局、写作格式的定义，不修改</h1><p>new_post_name: :title.md # File name of new posts<br>default_layout: post<br>titlecase: false # Transform title into titlecase<br>external_link: true # Open external links in new tab<br>filename_case: 0<br>render_drafts: false<br>post_asset_folder: false<br>relative_link: false<br>highlight:<br>  enable: true<br>  line_number: true<br>  tab_replace:</p><h1 id="Category-amp-Tag"><a href="#Category-amp-Tag" class="headerlink" title="Category &amp; Tag"></a>Category &amp; Tag</h1><p>default_category: uncategorized<br>category_map:<br>tag_map:</p><h1 id="Archives"><a href="#Archives" class="headerlink" title="Archives"></a>Archives</h1><h2 id="2-Enable-pagination"><a href="#2-Enable-pagination" class="headerlink" title="2: Enable pagination"></a>2: Enable pagination</h2><h2 id="1-Disable-pagination"><a href="#1-Disable-pagination" class="headerlink" title="1: Disable pagination"></a>1: Disable pagination</h2><h2 id="0-Fully-Disable"><a href="#0-Fully-Disable" class="headerlink" title="0: Fully Disable"></a>0: Fully Disable</h2><p>archive: 1<br>category: 1<br>tag: 1</p><h1 id="Server"><a href="#Server" class="headerlink" title="Server"></a>Server</h1><h2 id="Hexo-uses-Connect-as-a-server"><a href="#Hexo-uses-Connect-as-a-server" class="headerlink" title="Hexo uses Connect as a server"></a>Hexo uses Connect as a server</h2><h2 id="You-can-customize-the-logger-format-as-defined-in"><a href="#You-can-customize-the-logger-format-as-defined-in" class="headerlink" title="You can customize the logger format as defined in"></a>You can customize the logger format as defined in</h2><h2 id="http-www-senchalabs-org-connect-logger-html"><a href="#http-www-senchalabs-org-connect-logger-html" class="headerlink" title="http://www.senchalabs.org/connect/logger.html"></a><a href="http://www.senchalabs.org/connect/logger.html" target="_blank" rel="noopener">http://www.senchalabs.org/connect/logger.html</a></h2><p>port: 4000<br>server_ip: localhost<br>logger: false<br>logger_format: dev</p><h1 id="Date-Time-format"><a href="#Date-Time-format" class="headerlink" title="Date / Time format"></a>Date / Time format</h1><h2 id="Hexo-uses-Moment-js-to-parse-and-display-date"><a href="#Hexo-uses-Moment-js-to-parse-and-display-date" class="headerlink" title="Hexo uses Moment.js to parse and display date"></a>Hexo uses Moment.js to parse and display date</h2><h2 id="You-can-customize-the-date-format-as-defined-in"><a href="#You-can-customize-the-date-format-as-defined-in" class="headerlink" title="You can customize the date format as defined in"></a>You can customize the date format as defined in</h2><h2 id="http-momentjs-com-docs-displaying-format"><a href="#http-momentjs-com-docs-displaying-format" class="headerlink" title="http://momentjs.com/docs/#/displaying/format/"></a><a href="http://momentjs.com/docs/#/displaying/format/" target="_blank" rel="noopener">http://momentjs.com/docs/#/displaying/format/</a></h2><p>date_format: YYYY-MM-D<br>time_format: H:mm:ss</p><h1 id="Pagination"><a href="#Pagination" class="headerlink" title="Pagination"></a>Pagination</h1><h2 id="Set-per-page-to-0-to-disable-pagination"><a href="#Set-per-page-to-0-to-disable-pagination" class="headerlink" title="Set per_page to 0 to disable pagination"></a>Set per_page to 0 to disable pagination</h2><p>per_page: 5 #每页5篇文章<br>pagination_dir: page</p><h1 id="Disqus-社会化评论disqus，我使用多说，在主题中配置"><a href="#Disqus-社会化评论disqus，我使用多说，在主题中配置" class="headerlink" title="Disqus #社会化评论disqus，我使用多说，在主题中配置"></a>Disqus #社会化评论disqus，我使用多说，在主题中配置</h1><p>disqus_shortname:</p><h1 id="Extensions"><a href="#Extensions" class="headerlink" title="Extensions"></a>Extensions</h1><h2 id="Plugins-https-github-com-hexojs-hexo-wiki-Plugins"><a href="#Plugins-https-github-com-hexojs-hexo-wiki-Plugins" class="headerlink" title="Plugins: https://github.com/hexojs/hexo/wiki/Plugins"></a>Plugins: <a href="https://github.com/hexojs/hexo/wiki/Plugins" target="_blank" rel="noopener">https://github.com/hexojs/hexo/wiki/Plugins</a></h2><h2 id="Themes-https-github-com-hexojs-hexo-wiki-Themes"><a href="#Themes-https-github-com-hexojs-hexo-wiki-Themes" class="headerlink" title="Themes: https://github.com/hexojs/hexo/wiki/Themes"></a>Themes: <a href="https://github.com/hexojs/hexo/wiki/Themes" target="_blank" rel="noopener">https://github.com/hexojs/hexo/wiki/Themes</a></h2><p>theme: light<br>exclude_generator:</p><h1 id="Deployment-站点部署到github要配置"><a href="#Deployment-站点部署到github要配置" class="headerlink" title="Deployment 站点部署到github要配置"></a>Deployment 站点部署到github要配置</h1><p>deploy:<br>  type: github<br>  repository: <a href="mailto:git@github.com" target="_blank" rel="noopener">git@github.com</a>:paradoxallen/paradoxallen.github.io.git<br>  branch: master<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### 添加RSS ###</span><br><span class="line">hexo提供了RSS的生成插件，需要手动安装和设置。步骤如下：</span><br><span class="line">安装RSS插件到本地：`npm install hexo-generator-feed`</span><br><span class="line">开启RSS功能：编辑`hexo/_config.yml`，添加如下代码：</span><br></pre></td></tr></table></figure></p><p>plugins:</p><ul><li>hexo-generator-feed<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">在站点添加链接：</span><br><span class="line">在`themes/light/_config.yml`中，编辑 `rss: /atom.xml`</span><br><span class="line">在`themes/light/layout/_partial/header.ejs中，&lt;ul&gt;&lt;/ul&gt;`之间，添加一样代码</span><br><span class="line">`&lt;li&gt; &lt;a href=&quot;/atom.xml&quot;&gt;RSS&lt;/a&gt; &lt;/li&gt;`</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### 文章中插入图片 ###</span><br><span class="line">使用markdown写文章，插入图片的格式为`![图片名称](链接地址)`，这里要说的是链接地址怎么写。对于hexo，有两种方式：</span><br><span class="line">使用本地路径：在`hexo/source`目录下新建一个`img`文件夹，将图片放入该文件夹下，插入图片时链接即为`/img/图片名称`。</span><br><span class="line">使用[七牛](https://sso.qiniu.com/)，有图片的链接地址。格式为`![图片名称](链接地址)`我用的是139邮箱注册的七牛。</span><br><span class="line"></span><br><span class="line">### 加入左上角「fork me on github」 ###</span><br><span class="line">这里有[github](https://blog.github.com/2008-12-19-github-ribbons/)给出的教程，把代码插入到任意一个全局的模板文件中就行，比如`layout.ejs`的末尾。</span><br><span class="line">我的`themes\light\layout\layout.ejs`文件如下：</span><br></pre></td></tr></table></figure></li></ul><p>&lt;%<br>if(page.layout !== ‘false’){<br>%&gt;</p><p>&lt;%- partial(‘_partial/head’) %&gt;</p><p><body><br>  <header id="header" class="inner">&lt;%- partial(‘_partial/header’) %&gt;</header><br>  <div id="content" class="inner"><br>    <div id="main-col" class="alignleft"><div id="wrapper">&lt;%- body %&gt;</div></div><br>    <aside id="sidebar" class="alignright">&lt;%- partial(‘_partial/sidebar’) %&gt;</aside><br>    <div class="clearfix"></div><br>  </div><br>  <footer id="footer" class="inner">&lt;%- partial(‘_partial/footer’) %&gt;</footer><br>  &lt;%- partial(‘_partial/after_footer’) %&gt;<br></body><br></p><p>&lt;%}else{ %&gt;</p><p>&lt;%- page.content %&gt;<br>&lt;%};%&gt;<br><a href="https://github.com/paradoxallen" target="_blank"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_white_ffffff.png" alt="Fork me on GitHub"></a><br><code>`</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这是部署了的我的博客：&lt;a href=&quot;https://paradoxallen.github.io/&quot;&gt;RP’s Blog&lt;/a&gt;可以点击预览效果&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;添加“多说”评论与“页面导航”&quot;&gt;&lt;a href=&quot;#添加“多说”评论与“页面导航”&quot; c
      
    
    </summary>
    
      <category term="博客开发" scheme="https://paradoxallen.github.io/categories/%E5%8D%9A%E5%AE%A2%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="hexo" scheme="https://paradoxallen.github.io/tags/hexo/"/>
    
      <category term="博客" scheme="https://paradoxallen.github.io/tags/%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
  <entry>
    <title>Python中的按位运算</title>
    <link href="https://paradoxallen.github.io/1340/"/>
    <id>https://paradoxallen.github.io/1340/</id>
    <published>2017-09-29T16:00:00.000Z</published>
    <updated>2018-06-16T16:25:02.993Z</updated>
    
    <content type="html"><![CDATA[<p>位操作是程序设计中对位模式或二进制数的一元和二元操作. 在许多古老的微处理器上, 位运算比加减运算略快, 通常位运算比乘除法运算要快很多. 在现代架构中, 情况并非如此:位运算的运算速度通常与加法运算相同(仍然快于乘法运算).</p><p>简单来说，<strong>按位运算就把数字转换为机器语言——二进制的数字来运算的一种运算形式</strong>。在计算机系统中，数值一律用补码来表示(存储)。</p><p>Python中的按位运算符有：<strong>左移运算符（&lt;&lt;），右移运算符（&gt;&gt;）,按位与（&amp;），按位或（|），按位翻转（～）</strong>。这些运算符中只有按位翻转运算符是单目运算符，其他的都是双目运算符。</p><h4 id="按位与-amp"><a href="#按位与-amp" class="headerlink" title="按位与    &amp;"></a>按位与    &amp;</h4><p><strong>举例：</strong><br><strong>3&amp;5</strong><br>解法：3的二进制补码是 11,  5的是101, 3&amp;5也就是011&amp;101,先看百位(其实不是百位,这样做只是便于理解) 一个0一个1,根据(1&amp;1=1，1&amp;0=0，0&amp;0=0，0&amp;1=0)可知百位应该是1,同样十位上的数字1&amp;0=0,个位上的数字1&amp;1=1,因此最后的结果是1.(这之后本来应该还有一步,因为我们现在得到的数值只是所求答案的补码,但是因为正数的补码即是它本身,所以就省略了。不过,下面的例子就不能省略最后这一步了).    </p><p><strong>-1&amp;-2</strong><br>解法:-1的补码是11111111,  -2的补码是11111110, 11111111&amp;11111110得到的结果是:11111110,这个是补码,再转化位原码为100000010 (负数转换位原码的方法是减一取反),最后转换为十进制是 -2.</p><p><strong>-2&amp;6</strong><br>解法:-2的补码是11111110,  6的补码是110,   11111110&amp;110,也就是11111110&amp;00000110(这样写的目的是让初学者能够更好理解按位运算),按照上面的方法得到的结果是:110,转化位十进制就是6.</p><p><strong>小技巧</strong>：利用按位与可以将任意二进制数的最后一位变为0,即就是X&amp;0.</p><h4 id="按位并"><a href="#按位并" class="headerlink" title="按位并    |"></a>按位并    |</h4><p><strong>举例： </strong></p><p><strong>4|7</strong><br>解法：按位并的计算规律和按位与的很相似，只不过换了逻辑运算符，并的规律是： 1|1=1 ,1 |0=1, 0|0=0.   4|7转换位二进制就是:100|111=111.  二进制111即为十进制的7.<br><strong>小技巧</strong>：利用按位并可以将任意二进制数的最后一位变为1,即就是X|1.</p><h4 id="按位异或"><a href="#按位异或" class="headerlink" title="按位异或    ^"></a>按位异或    ^</h4><p><strong>方法</strong>:  对位相加,<strong>特别要注意的是不进位. </strong></p><p><strong>举例：</strong> </p><p><strong>2^5</strong><br>解法:10^101=111,二进制111得到十进制的结果是7.</p><p><strong>1^1</strong><br>解法:1+1=0.(本来二进制1+1=10,但不能进位,所以结果是0) </p><p><strong>-3^4  </strong><br>解法: -3的补码是11111101,4的补码是100 (也即00000100),11111101^00000100=11111101,补码                               11111101转为原码是1000111,即十进制的-7.</p><h4 id="按位翻转"><a href="#按位翻转" class="headerlink" title="按位翻转  ~"></a>按位翻转  ~</h4><p><strong>方法:</strong>   将二进制数+1之后乘以-1,x的按位翻转是-(x+1) . 注意,按位运算符是单目运算符.  -9 ,  1+~4是正确的,5~3就不对了.</p><p><strong>举例:</strong><br><strong>~3</strong><br>解法:3的二进制是11, -(11+1)=-100B=-4D. (注:B和D分别表示二进制和十进制).<br><strong>~-2</strong><br>解法:   -  (-10+1)  =1</p><h4 id="左移运算符-lt-lt"><a href="#左移运算符-lt-lt" class="headerlink" title="左移运算符  &lt;&lt;"></a>左移运算符  &lt;&lt;</h4><p><strong>方法: </strong>   X&lt;&lt;N 将一个数字X所对应的二进制数向左移动N位.</p><p><strong>举例:</strong></p><p><strong>3&lt;&lt;2</strong><br>解法:11向左移动两位变为1100,即12 .</p><h4 id="右移动运算符-gt-gt"><a href="#右移动运算符-gt-gt" class="headerlink" title="右移动运算符  &gt;&gt;"></a>右移动运算符  &gt;&gt;</h4><p><strong>方法:</strong>    X&gt;&gt;N 将一个数字X所对应的二进制数向右移动N位.</p><p><strong>举例: </strong></p><p><strong>3&gt;&gt;2</strong><br>解法:11向右移动两位变为0.</p><p><strong>10&gt;&gt;1</strong><br>解法:10的二进制是1010,向右边移动一位是101,即5.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;位操作是程序设计中对位模式或二进制数的一元和二元操作. 在许多古老的微处理器上, 位运算比加减运算略快, 通常位运算比乘除法运算要快很多. 在现代架构中, 情况并非如此:位运算的运算速度通常与加法运算相同(仍然快于乘法运算).&lt;/p&gt;
&lt;p&gt;简单来说，&lt;strong&gt;按位运
      
    
    </summary>
    
      <category term="计算机综合" scheme="https://paradoxallen.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%BC%E5%90%88/"/>
    
    
      <category term="python" scheme="https://paradoxallen.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (9) Anomaly Detection</title>
    <link href="https://paradoxallen.github.io/39500/"/>
    <id>https://paradoxallen.github.io/39500/</id>
    <published>2017-09-18T16:00:00.000Z</published>
    <updated>2018-06-19T16:59:12.105Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-异常检测-amp-高斯分布"><a href="#1-异常检测-amp-高斯分布" class="headerlink" title="1. 异常检测 &amp; 高斯分布"></a>1. 异常检测 &amp; 高斯分布</h3><p><strong>异常检测是一种介于监督学习与非监督学习之间的机器学习方式</strong>。一般用于检查大规模正品中的<strong>小规模</strong>次品。</p><p>根据单个特征量的概率分布，从而求出某个样本正常的概率，若正常的概率小于阈值，即 p(x)&lt;ϵ 视其为异常（次品）。正品与次品的 label 值 y 定义为：<br><img src="https://i.imgur.com/o9Rt4yM.png" alt=""></p><p>如果某个样本由x1,x2两个变量决定，如下图红色叉所示：<br><img src="https://i.imgur.com/ylEk8oe.png" alt=""></p><p>同一个圆圈内部，表示的是成为正品的概率相同。越中心的圆圈内部正品率越高。越外层的圆圈内正品率越低。</p><p>异常检测一般<strong>将每个特征量的分布假设为正态分布</strong>（如果特征量与正态分布差距很大，之后我们会提到方法对其进行修正）。</p><p>为什么是正态分布？因为在生产与科学实验中发现，很多随机变量的概率分布都可以近似地用正态分布来描述（猜测正确的概率更大）。因</p><p>此，以下稍微介绍一下正态分布的基础知识，如果很熟悉的同学可以略过这部分。</p><p><strong>正态分布（高斯分布）</strong>，包含两个参数：均值μ（分布函数取峰值时所对应横坐标轴的值），与方差σ^2（标准差为σ，控制分布函数的“胖瘦”）。</p><p>如果变量 x 满足于正态分布，将其记为 x∼N(μ,σ^2)。而取某个 x 的对应正品概率为：<img src="https://i.imgur.com/RhxicSW.png" alt=""></p><p>均值 <img src="https://i.imgur.com/bzn9fbc.png" alt="">，方差<img src="https://i.imgur.com/88vXqR7.png" alt=""></p><p>正态分布曲线与坐标轴之间的面积（即函数积分）恒定为 1，因此<strong>“高”曲线必然“瘦”，“矮”曲线必然“胖”</strong>：<br><img src="https://i.imgur.com/QUEyjr8.png" alt=""></p><p>由图可知，标准差σ控制着分布函数的“胖瘦”。原因是因为<strong>标准差有关的取值范围，有着固定的分布概率（积分）</strong>：<br><img src="https://i.imgur.com/GPZPvbN.png" alt=""></p><h3 id="2-异常检测算法流程："><a href="#2-异常检测算法流程：" class="headerlink" title="2. 异常检测算法流程："></a>2. 异常检测算法流程：</h3><p>我们拥有一组训练数据：x(1),x(2),…,x(m)，每个样本有着 m 个特征量 x1,x2,…,xn </p><p>将每个样本投影到不同的特征的坐标轴上，基于样本得到各个特征的概率正态分布曲线</p><p>假设各个特征的概率是独立的，因此单个样本的异常概率为<br><img src="https://i.imgur.com/B4JPDaw.png" alt=""></p><p>各个特征的均值为<img src="https://i.imgur.com/v3TxnSr.png" alt="">，方差为 <img src="https://i.imgur.com/oO4VpDF.png" alt=""></p><p>如果我们有着 10000 个正品样本，以及 20 个次品样本，我们应该这样区分<strong>训练集、交叉验证集，与测试集</strong>： </p><p><strong>训练集</strong>：6000个正品作为训练集（不包括次品样本）</p><p><strong>交叉验证集</strong>：2000 个正品样本 + 10 个次品样本。用以确定次品概率的阈值 ϵ</p><p><strong>测试集</strong>：2000 个正品样本 + 10 个次品样本。用以判断算法的检测效果</p><p>特别注意，因为使用异常检测的样本集合一般都是偏斜严重的（正品样本远远多于次品样本）。</p><p>因此，需要在<a href="https://paradoxallen.github.io/9059/">Machine Learning Advice</a>中提到的 <strong>precision/recall/F-score</strong> 来进行判断算法的检测效果。</p><h3 id="3-异常检测-VS-监督学习"><a href="#3-异常检测-VS-监督学习" class="headerlink" title="3. 异常检测 VS. 监督学习"></a>3. 异常检测 VS. 监督学习</h3><p>监督学习方法与异常检测类似，处理对象都是一堆有 label 的样本，并且目标都是预测新样本的类别。那么什么时候使用监督学习的方法？什么时候使用异常检测的方法？</p><p>大体上，区别如下：<br>样本比例：异常检测适用于<strong>正样本（y=1，即次品）个数远远小于负样本</strong>的个数的情况；监督学习适用于<strong>正负样本个数都非常多</strong>的情况</p><p>异常规律：如果<strong>正样本（y=1，即次品）有着难以预测的模式，引起正样本的原因有很多很多</strong>，适用于异常检测；但是如果<strong>正样本有着固定的规律</strong>，比如感冒（病因已被研究透彻），可以尝试基于大量的样本使用监督学习的方法建立模式进行判断</p><h3 id="4-特征选择"><a href="#4-特征选择" class="headerlink" title="4. 特征选择"></a>4. 特征选择</h3><p>绝大多数情况下，特征量符合正态分布的分布情况。但如果特征的分布极端不符合，我们只能对其进行一些处理，以产生全新的特征来适用于异常检测算法。例如：<br><img src="https://i.imgur.com/LuwgqTb.png" alt=""></p><p>此时，我们有着变换后的特征变量：<strong>xnew=log(x1)</strong></p><p>或者，一般情况下我们希望<strong>正品的 p(x) 很大，次品的 p(x) 很小</strong>。也就是说，<strong>在异常情况下某些特征应该变得极大或极小</strong>（正态分布中对于极大值或者极小值的对应概率都是极小的，所以整个样本的正品概率相乘会很容易满足 p(x)&lt;ϵ）：例如创建新变量<img src="https://i.imgur.com/qP5XSx6.png" alt="">，进一步放大了值增大或减小的程度。</p><h3 id="5-Multivariate-Gaussion"><a href="#5-Multivariate-Gaussion" class="headerlink" title="5. Multivariate Gaussion"></a>5. Multivariate Gaussion</h3><p>如果一个样本有着多种特征，那么整体的正品概率可以按照以上提到的，视每个变量为互相独立然后各自概率相乘进行求解（我们称之为 <strong>original model</strong>）。</p><p>但是，如果出现了下图这种正相关（负相关）极强的特征量，同心圆内部的正品概率必然不同，显然不合适了：<br><img src="https://i.imgur.com/PQ9GGVM.png" alt=""></p><p>我们希望原本的同心圆可以更扁，可以变换方向，例如上图的蓝色椭圆。</p><p>此时，我们可以利用协方差矩阵，构造全新的多变量正态分布公式。此时我们用到的不再是方差 σ2，而是<strong>协方差矩阵</strong><img src="https://i.imgur.com/TOlEvpS.png" alt="">。<strong>多变量正态分布的概率公式</strong>为：<br><img src="https://i.imgur.com/qyUGsLg.png" alt="">，其中|Σ|表示协方差矩阵的行列式。</p><p>协方差矩阵与均值，对概率分布图的影响如下：<br><img src="https://i.imgur.com/QQAdk48.png" alt=""><br><img src="https://i.imgur.com/edL5bda.png" alt=""></p><h3 id="6-original-model-VS-multivariate-Gaussian"><a href="#6-original-model-VS-multivariate-Gaussian" class="headerlink" title="6. original model VS. multivariate Gaussian"></a>6. original model VS. multivariate Gaussian</h3><p>如果一个样本有着多种特征，那我们究竟是应该使用 original model，还是 multivariate Gaussian？</p><p>大体上，区别如下：<br><strong>特征选择：</strong>original model 中的各个单个特征（或创造出的新特征），应该尽量满足在异常情况下产生概率极小的特性；而如果特征之间，发现了正相关或负相关的关系，应该用 multivariate Gaussian</p><p><strong>计算效率：</strong>original model 仅仅乘法，效率较高；multivariate Gaussian 需要计算协方差的逆矩阵，效率较低</p><p><strong>样本数目：</strong>original model 在训练集极小的情况下也可以计算；</p><p>multivariate Gaussian 至少需要训练集样本数目大于特征数目，否则协方差矩阵无法求逆</p><p>协方差矩阵无法求逆（奇异矩阵）的情况极少发生，但是一旦发生，可能有以下几种原因：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">特征的数量大于训练集的样本个数</span><br><span class="line">冗余的特征变量（x1≈x2 或者 x3=x4+x5 这样的高度冗余情况）</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (8) PCA</title>
    <link href="https://paradoxallen.github.io/22819/"/>
    <id>https://paradoxallen.github.io/22819/</id>
    <published>2017-09-14T16:00:00.000Z</published>
    <updated>2018-06-19T14:30:00.421Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Motivation-for-Dimensionality-Reduction"><a href="#1-Motivation-for-Dimensionality-Reduction" class="headerlink" title="1. Motivation for Dimensionality Reduction"></a>1. Motivation for Dimensionality Reduction</h3><p>为什么要数据降维？目的性一般来说有三个：</p><p><strong>（1）加速计算，要计算的维度更少；</strong></p><p><strong>（2）节省空间；</strong></p><p><strong>（3）可视化</strong></p><p>因为现在的可视化只能对于2D或者3D有较好的处理。</p><p>那么，数据为什么可以降维呢？举个例子，我用一幅图表示厘米与英尺的关系，这时特征有（厘米，英尺）。但其实我存储的时候，<strong>只需要其中任意一个，另外一个是极强相关度</strong>。此时只有一维特征。</p><h3 id="2-Principal-Component-Analysis-PCA-简介"><a href="#2-Principal-Component-Analysis-PCA-简介" class="headerlink" title="2. Principal Component Analysis (PCA) 简介"></a>2. Principal Component Analysis (PCA) 简介</h3><p><strong>主成分分析（Principal Component Analysis）</strong>是数据降维的一种极为有效的方法。</p><p>通过将高维数据投影到低维空间，以求通过<strong>牺牲极少的数据精度来换取更低维度的数据</strong>。</p><p>先来举个例子，例如下图中的5个点：<br><img src="https://i.imgur.com/bAi9DAY.png" alt=""></p><p>如果精确描述这5个点，那么需要2D的特征(x1,x2)。但是，如果我们以图中的直线作为唯一的1D特征，将每个点投影到直线上，是不是基本可以区分出这5个点？这肯定丧失了一部分精度。</p><p>注意：此时的特征正方向（1D空间的基向量），方向沿着直线的任意两方都正确，我们<strong>关心的只是直线而不是方向</strong>。</p><p>上图的例子中，PCA的任务就是找到这条直线，然后投影到直线上去。而这条直线，必须满足<strong>每个点到直线欧氏距离的平方和最小，即丧失的精度最少</strong>：<img src="https://i.imgur.com/KPVsso6.png" alt=""></p><p>既然都是找直线，那肯定有人自然而然想到了线性回归。但是两者之间有着极大不同。首先就是优化目标，见下图：<br><img src="https://i.imgur.com/lySevWa.png" alt=""></p><p>左图是线性回归所需要最小化的距离（蓝色线段和），右图是2D空间PCA所需要最小化的距离（蓝色线段和）。除此之外，线性回归是为了拟合特殊的变量y，而PCA是一种无监督学习，目的性并不明确，每个变量都会降维。</p><h3 id="3-Principal-Component-Analysis-PCA-流程"><a href="#3-Principal-Component-Analysis-PCA-流程" class="headerlink" title="3. Principal Component Analysis (PCA) 流程"></a>3. Principal Component Analysis (PCA) 流程</h3><p>述说流程之前，希望大家可以看看这篇分析（本课程缺少理论分析）：<a href="http://sebastianraschka.com/Articles/2014_kernel_pca.html" target="_blank" rel="noopener">Kernel tricks and nonlinear dimensionality reduction via RBF kernel PCA</a></p><p>老规矩，首先要 <strong>mean normalization / feature scaling</strong>。方法就跟之前课程提到的一样，对每个特征 j 求均值<img src="https://i.imgur.com/tfWfqOH.png" alt=""></p><p>然后更新特征的值<img src="https://i.imgur.com/nXB3It5.png" alt="">其中Sj是特征值的范围，可以是最大值减去最小值。</p><p>计算协方差矩阵：<img src="https://i.imgur.com/BTVkiWi.png" alt="">，协方差矩阵一般是<strong>对称正定矩阵</strong>，一定会求出特征值与特征向量。</p><p>计算特征值与特征向量，matlab中写作：<code>[U, S, V] = svd(simga)</code>. </p><p>U 是特征向量集合，<img src="https://i.imgur.com/BpkU4CG.png" alt=""></p><p>S是特征值集合<img src="https://i.imgur.com/USdq9tK.png" alt=""></p><p>U∈Rn×n，降维之后取U的前k个特征向量u(1),u(2)…u(k)作为子空间的基向量，构造<img src="https://i.imgur.com/pfb6Kwe.png" alt="">。</p><p>此时x∈Rn→z∈Rk <img src="https://i.imgur.com/auNcI4d.png" alt=""><br>其中z是k×1的列向量，x是n×1的列向量，UTreduce 是k×n的矩阵</p><p>降维之后，进行计算获得结果之后，总是需要升维到原空间中。这个时候的升维精确度已经有所下降，但已经是尽量减少损耗。具体的降维转换公式为<img src="https://i.imgur.com/Wp14DBC.png" alt=""></p><p>也就是下图中的绿色交叉点返回到2D坐标系中表示。<br><img src="https://i.imgur.com/RBzzKVQ.png" alt=""></p><h3 id="4-Principal-Component-Analysis-PCA-其他问题"><a href="#4-Principal-Component-Analysis-PCA-其他问题" class="headerlink" title="4. Principal Component Analysis (PCA) 其他问题"></a>4. Principal Component Analysis (PCA) 其他问题</h3><p>OK，知道怎么使用PCA，那么我们应该选择降维k=？我们应该有评判标准。我们大体目标是<img src="https://i.imgur.com/HuLOWKM.png" alt=""></p><p>因此我们选择标准为：<br><img src="https://i.imgur.com/YjJRuyL.png" alt="">，表示意义为“<strong>保留99%的差异性</strong>”。 </p><p>如果使用特征值来表示就是，<img src="https://i.imgur.com/DTImTvI.png" alt=""></p><p>PCA 中，无监督学习的唯一学习参数是 Ureduce。<strong>Only Training Data！</strong>仅仅是由训练集训练出来的参数。</p><p>可以用<strong>Cross-Validation data 和 test Data进行检验</strong>，但是选择主分量的时候<strong>只应用training data</strong>。</p><p>有时候 PCA 被用于解决 overfitting ，但这样做其实非常不好。PCA 降低了数据维度，但同时损失了数据原有的标记信息。而之前的课程中提到的<strong>regularization 技术</strong>才是解决过拟合的正途。</p><p>使用 PCA 之前，一定要利用原维度的数据进行计算。仅仅当计算过程出现了<strong>（1）计算过慢；（2）占用内存过多</strong>；这两个问题之后，才应该考虑 PCA。盲目的使用，是不可取的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (7) K-Means</title>
    <link href="https://paradoxallen.github.io/48762/"/>
    <id>https://paradoxallen.github.io/48762/</id>
    <published>2017-09-10T16:00:00.000Z</published>
    <updated>2018-06-18T15:56:13.732Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Unsupervised-Learning"><a href="#1-Unsupervised-Learning" class="headerlink" title="1. Unsupervised Learning"></a>1. Unsupervised Learning</h3><p>之前课程中说到的学习，都是监督学习，即有一个label，明确告诉你这个样本，属于哪个类型，或者导致的值是多少。但是，如果我碰到没有label，或者我也不知道label是怎样的情况，但是我还是想要分成若干类。这样的问题，就是一种<strong>无监督</strong>问题。</p><p><strong>聚类（clustering） </strong></p><p>聚类是一种典型的无监督学习例子，但是<strong>聚类不等同于无监督学习</strong>，密度估计同样是一个典型的无监督学习例子。回到聚类，例如有下图：<br><img src="https://i.imgur.com/RWM0hdQ.png" alt=""></p><p>每种样本（蓝色圆圈）都没有label指定类别，但是人眼一看就知道分成两类比较合适。如何让机器也知道如何分类呢？这就是聚类问题。</p><h3 id="2-K-Means-Algorithm"><a href="#2-K-Means-Algorithm" class="headerlink" title="2. K-Means Algorithm"></a>2. K-Means Algorithm</h3><p><strong>K-Means 算法是解决无监督学习的有效算法之一</strong>。K（大写）表示将样本分为K个类型。算法具体的过程通俗易懂，如下图所示：<br><img src="https://i.imgur.com/XXqO0uo.png" alt=""><br><img src="https://i.imgur.com/mTXlNHO.png" alt=""></p><p>配合上图，再作一些简单的解释：<br><img src="https://i.imgur.com/CJB9nyT.png" alt=""></p><p>K-Means 算法进行一段时间后。<strong>可能会出现某个中心点没有分配到任何一个样本点的情况，这个时候可以直接去掉这个分类变为 (K-1) 簇</strong>。</p><p>但是在某些情况我们很执着的要分为 K 簇，这种情况下需要随机确定一个新的聚类中心替代当前中心点，之后再次迭代。</p><h4 id="Optimization-Objective"><a href="#Optimization-Objective" class="headerlink" title="Optimization Objective"></a>Optimization Objective</h4><p>K-Means 算法同样有着 cost function，目标同样是为了最小化 cost。cost 记为：<img src="https://i.imgur.com/Uvbvz66.png" alt=""></p><p>分析K-Means的两个主要步骤，簇分配是以μ为常量，c为变量；移动聚类中心是以c为常量，μ为变量。通过不断修改，减小 cost。</p><p>不同于回归问题，因为学习率过大导致随着迭代次数增加竟然还会增大 cost。聚类问题中不会出现这种问题（就没有学习率），聚类中随着迭代次数增加 <strong>cost 一定是逐渐下降的</strong>。</p><h4 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h4><p>之前只是说，随机化取得K个聚类中心。具体究竟如何执行呢？ </p><p><strong>（1）在m个样本中，随机取得K个样本 (K&lt;m) </strong></p><p><strong>（2）令μ1,…,μK等同于所取到的K个样本</strong> </p><p>但是，有的时候因为初始化的随机性，会陷入局部最大值的情况。例如下图所示：<br><img src="https://i.imgur.com/xKhfaFg.png" alt=""></p><p>这样即使 K-Means 算法停止了，所取得的依然不是较好的分类方法，cost 依然十分大。怎么解决？<strong>多随机初始化几次！一般是运行50~1000次 K-Means 算法，取其中 cost 最小一次的结果</strong>。</p><h4 id="Choosing-the-numbers-of-clusters"><a href="#Choosing-the-numbers-of-clusters" class="headerlink" title="Choosing the numbers of clusters"></a>Choosing the numbers of clusters</h4><p>好吧，最后一个问题来了，究竟应该分成几类？K=？ </p><p>这个问题没有标准解，依照可视化图来观察是个不错的主意。有一种特殊情况，如果K−J(cost)图如下图左边所示，圆圈所指位置为<strong>“肘区”</strong>，一般我们取“肘区”所对应的 K 值。这种方法也称为 Elbow method.</p><p>但是绝大多数时候，我们看到的是如下图右边的情况，这种时候……好吧看你的心情以及实际需求（最多只能承受多少类的负担）取值吧。<br><img src="https://i.imgur.com/mUOx6Rc.png" alt=""></p><p>如果出现K−J(cost)图随着 K 值的增加上下起伏，那说明出现了“局部最大值”的问题。这个时候就应该使用上一小节提到的，多进行几次 K-Means 算法，求出一个J(cost)最小时的 cost 作为当前 K 值的 cost.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (6) Machine Learning Advice</title>
    <link href="https://paradoxallen.github.io/9059/"/>
    <id>https://paradoxallen.github.io/9059/</id>
    <published>2017-09-04T16:00:00.000Z</published>
    <updated>2018-06-15T16:45:25.622Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Evaluating-a-Learning-Algorithm"><a href="#1-Evaluating-a-Learning-Algorithm" class="headerlink" title="1. Evaluating a Learning Algorithm"></a>1. Evaluating a Learning Algorithm</h3><p>如果一个机器学习方法的结果不令人满意，可能有各种方法来解决。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">获取更多的训练样本（最为耗时，不作为优先考虑）</span><br><span class="line">尝试更少 / 更多的特征</span><br><span class="line">尝试引入多项式特征</span><br><span class="line">增加 / 减少正则化系数 λ</span><br></pre></td></tr></table></figure></p><p>究竟应该使用哪种方法来解决问题，需要一个诊断过程，称为 <strong>Maching Learning Diagnostic</strong>。为了更好的阐述，我们先引入两个名词：<strong>欠拟合、过拟合 </strong></p><p><strong>欠拟合：</strong>对于训练集，hypothesis 得到的结果，与真实的结果差距较大，并不能对样本集有效拟合；</p><p><strong>过拟合：</strong>对于训练集，hypothesis 得到的结果，与真实的结果差距较小；但是对于测试集，hypothesis 得到的结果，与真实的结果差距较大。这说明 hypothesis 的泛化能力较差，只是在训练集上得到的效果较好</p><p>通常情况下，我们会得到一组数据而不是区分好的训练集与测试集。这时就需要我们做一些<strong>处理</strong>：</p><p>首先打乱数据的次序，然后将其之前大约70%的部分来作为训练集，训练样本总数记为m，训练样本记为<img src="https://i.imgur.com/S04RXiX.png" alt=""></p><p>剩下的30%部分作为测试集，测试样本总数记为mtest，测试样本记为<img src="https://i.imgur.com/rluZ3Dt.png" alt=""></p><p>在测试集上，我们会计算 hypothesis 与真实数据的偏差。对于线性回归与逻辑回归，有一些不同。这些在之前的系列中都有提及： </p><p><strong>线性回归</strong>：<img src="https://i.imgur.com/BmXcDfD.png" alt=""><br><strong>逻辑回归</strong>：<img src="https://i.imgur.com/g2yXaRe.png" alt=""></p><p>我们在训练集、测试集的基础上，再次添加一种数据集：<strong>交叉验证集（cross validation set）</strong>。</p><p>比例大约是训练集60%，测试集20%，交叉验证集20%，交叉验证样本总数记为mcv，交叉验证样本记为<img src="https://i.imgur.com/B6gENOi.png" alt="">。而Jcv(θ)的计算方式与Jtest(θ)相同，只是适用的数据集范围不同。 </p><p>那么问题来了，为什么要区分这三个集合？以及它们的区别是什么呢？</p><p>这三个名词在机器学习领域的文章中极其常见，但很多人对他们的概念并不是特别清楚，尤其是后两个经常被人混用。Ripley, B.D（1996）在他的经典专著《Pattern Recognition and Neural Networks》中给出了这三个词的定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Training set: A set of examples used for learning, which is to fit the parameters [i.e., weights] of the classifier.</span><br><span class="line">Validation set: A set of examples used to tune the parameters [i.e., architecture, not weights] of a classifier, for example to choose the number of hidden units in a neural network.</span><br><span class="line">Test set: A set of examples used only to assess the performance [generalization] of a fully specified classifier.</span><br></pre></td></tr></table></figure></p><p>显然，<strong>training set是用来训练模型或确定模型参数的</strong>，如ANN中权值等； <strong>validation set是用来做模型选择（model selection），即做模型的最终优化及确定的</strong>，如ANN的结构；而<strong> test set则纯粹是为了测试已经训练好的模型的推广能力</strong>。</p><p>当然，test set这并不能保证模型的正确性，他只是说相似的数据用此模型会得出相似的结果。但实际应用中，一般只将数据集分成两类，即training set 和test set，大多数文章并不涉及validation set。<br>Ripley还谈到了<strong>Why separate test and validation sets?</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The error rate estimate of the final model on validation data will be biased (smaller than the true error rate) since the validation set is used to select the final model.</span><br><span class="line">After assessing the final model with the test set, YOU MUST NOT tune the model any further.</span><br></pre></td></tr></table></figure></p><p>对于过拟合，如何选择 model？使用多高次方的多项式拟合？答案是<strong>遍历尝试</strong>，如下图所示。分别在训练集上计算得到各个θ的值之后，再在交叉验证集（而不是测试集）上计算Jcv(θ)，选择Jcv(θ)最小的一个model作为hypothesis.<br><img src="https://i.imgur.com/mOk3wc8.png" alt=""></p><p>如此一来，测试集上的Jtest(θ)就可以用于其他评估方向，而依靠交叉验证集来决定多项式拟合的次数。</p><h3 id="2-Bias-vs-Variance"><a href="#2-Bias-vs-Variance" class="headerlink" title="2. Bias vs. Variance"></a>2. Bias vs. Variance</h3><p><strong>Bias 与 Variance </strong>并非是通常意义上的名词偏差、方差。而是用于描述两种机器学习中出现的问题。</p><p><strong>high bias意味着欠拟合，high variance意味着过拟合。</strong>下图可以直观看出来：<br><img src="https://i.imgur.com/CzSF40I.png" alt=""></p><p>接下来，我们从另一些角度来看看 Bias 与 Variance 的表现<br>以 hypothesis 的多项式次数 d 为横轴，error 为纵轴，我们可以得到在训练集与交叉验证集上的误差曲线为：<br><img src="https://i.imgur.com/o40hQlt.png" alt=""></p><p>对于 Bias 与 Variance 也有了各自的概念： </p><p><strong>Bias：</strong>Jtrain(θ)较大，Jcv(θ)较大，Jtrain(θ)≈Jcv(θ)。一般是 d（维度）较小的时候，才会产生 bias，欠拟合阶段；</p><p><strong>Variance：</strong>Jtrain(θ)较小，Jcv(θ)较大，Jtrain(θ)≪Jcv(θ)。一般是 d（维度）较高的时候，才会产生 variance，过拟合阶段</p><p>以 hypothesis 的正则化系数 λ 为横轴，error 为纵轴，我们可以得到在训练集与交叉验证集上的误差曲线与上图左右反转：<br><img src="https://i.imgur.com/tT6dRPD.png" alt=""></p><p><strong>Bias：</strong>Jtrain(θ)较大，Jcv(θ)较大，Jtrain(θ)≈Jcv(θ)。一般是 λ 较大的时候，才会产生 bias，欠拟合阶段；</p><p><strong>Variance：</strong>Jtrain(θ)较小，Jcv(θ)较大，Jtrain(θ)≪Jcv(θ)。一般是 λ 较小的时候，才会产生 variance，过拟合阶段</p><p>那么如何选择 λ 呢？其实类似于选择多项式的维度 d，依次尝试。在确定了多项式维度之后，依次增大 λ 的值后再次计算 Jcv(θ) 的值，找到使得 Jcv(θ) 最小的 λ 值。增大的幅度可以较大，例如 λ=0,0.01,0.02,0.04,0.08,…</p><h3 id="3-Learning-Curve"><a href="#3-Learning-Curve" class="headerlink" title="3. Learning Curve"></a>3. Learning Curve</h3><p><strong>learning curve </strong>同样是以 error 为纵轴，只不过这次的横轴换成了训练集的样本数目，但同样是 Jcv(θ) 与 Jtest(θ) 的曲线。一般形状是这样：<br><img src="https://i.imgur.com/jBnE12f.png" alt=""></p><p>在样本较少的时候，训练集很容易被拟合，因此误差较小，但是因为训练样本少所以很难保证对于交叉验证集也是有效的，因此误差较大。</p><p>在样本较多的时候，训练集较难被拟合，因此误差升高，但是因为训练样本更多所以有更大可能保证对于交叉验证集也是有效的，因此 Jcv(θ) 会下降。</p><p>那么对于 high bias 与 high variance 而言，如果我们使用增加样本个数（最为耗时的方法）m，可否起到什么作用？</p><p><img src="https://i.imgur.com/1GHLkWf.png" alt=""></p><p>由图可以看出，在 high bias 情况下，一般都是由于本身选择的 hypothesis 的模型错误，而与训练集样本个数无关。错的模型训练再多次还是错的。</p><p><img src="https://i.imgur.com/iWW3jJS.png" alt=""></p><p>由图可以看出，在 high variance 情况下，很可能是过拟合问题，而增加训练集样本个数会使得模型的泛化能力进一步增强，从而消除过拟合问题，使得误差降低。</p><h3 id="4-Deciding-What-to-Do-Next-Revisited"><a href="#4-Deciding-What-to-Do-Next-Revisited" class="headerlink" title="4. Deciding What to Do Next Revisited"></a>4. Deciding What to Do Next Revisited</h3><p>回想一下最开始提出的若干种优化方案，各自适用情况有：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">获取更多的训练样本 ⇒ high variance</span><br><span class="line">尝试更少的特征 ⇒ high bias</span><br><span class="line">尝试更多的特征 ⇒ high variance</span><br><span class="line">尝试引入多项式特征 ⇒ high bias</span><br><span class="line">增加正则化系数 λ ⇒ high variance</span><br><span class="line">减少正则化系数 λ ⇒ high bias</span><br></pre></td></tr></table></figure></p><p>将以上所有内容，如果引用到神经网络中，同样适用。 </p><p>小规模（层数少、各层神经元数目少）的神经网络容易出现欠拟合问题；大规模的神经网络容易出现过拟合问题。 </p><p>那么使用多少个隐藏层？可以使用依次递增层数，交叉验证集上误差来确定。 </p><p>λ 该如何确定？根据上文，利用交叉验证集，依次递增选择误差最小的确定即可。</p><h3 id="5-Building-a-Spam-Classifier"><a href="#5-Building-a-Spam-Classifier" class="headerlink" title="5. Building a Spam Classifier"></a>5. Building a Spam Classifier</h3><p>让我们以<strong>“垃圾邮件分类（Spam-Classifier）”</strong>问题作为例子：如何判断一封邮件是垃圾邮件，还是正常邮件？</p><p>很容易我们会想到，依据某些单词作为特征，这些单词的出现与否决定这封邮件的性质。</p><p>所以，我们会想到一个0，1组成的特征向量，1代表邮件具备这个特征（出现这个单词），0代表邮件不具备这个特征（没出现这个单词）。</p><p>有了基本大方向之后，就要开始一场头脑风暴了，以求提高机器学习的效果。这种行为，不存在标准答案，任何结果都是有可能的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">增加数据</span><br><span class="line">采用更为复杂的特征（邮件路径）</span><br><span class="line">基于正文寻找更为精确复杂的特征（discount, discounts 视为同一单词；deal, dealer 视为同一单词）</span><br><span class="line">算法改进（故意拼错隐瞒检查的单词？deeler = dealer）</span><br></pre></td></tr></table></figure></p><p>头脑风暴之后，谁说的才更有道理呢？自然需要各种方法对于误差的分析（error analysis）。这里需要注意：<strong>所有的 error analysis 都是在交叉验证集上完成的。</strong></p><p>一般来说，机器学习算法的设计会经历以下几个过程：<br>以最快的速度（一天之内？）用最简单的方法，尝试去解决眼下的问题，并在交叉验证集上验证；</p><p>画出 learning curve，去观察，发现现在的问题是 high bias，还是 high variance，需不需要更多的特征？更多的数据集？</p><p>error analysis：上面已经在cross-validation数据集上测试了系统性能，现在我们人工去看哪些数据造成了大error的产生？是否可以通过改变systematic trend减少error？</p><p>Spam-Classifier 举例，我们看一下进行Error Analysis的步骤：<br>所有 spam 分为四类：pharma，replica/fake，Steal password 和 其他</p><p>如下图，寻找一些可能有助于改善分类效果的features<br><img src="https://i.imgur.com/lZ2lLvZ.png" alt=""></p><p>然后，在是否引入特征的问题上，一定要做实验：例如，可以比较引入此特征前后，预测的准确率<img src="https://i.imgur.com/CJLBwew.png" alt="">是否提高？</p><p>error analysis 的核心在于：一定要找到一种<strong>数值化的</strong>评定方法，以求判断 error 的大小。</p><h3 id="6-Handling-Skewed-Data"><a href="#6-Handling-Skewed-Data" class="headerlink" title="6. Handling Skewed Data"></a>6. Handling Skewed Data</h3><p>首先，我们需要介绍，什么样的数据称为<strong>偏斜数据（skewed data）</strong>。这次，我们举的例子是预测癌症。</p><p>预测癌症例子：实际生活中，癌症的发病率极低，可能在人群中只有0.5%的发病率。假设我们手头有一种预测方法，预测出来有1%的人会发病。这样，按照之前的评价标准，有0.5%的误诊率。但是，如果我不做任何的检查，直接判断病人是没病的，是不是同样有着0.5%的误诊率？这是不应该的，我们化验检查一番力气之后的答案竟然和瞎猜的误诊率相同。</p><p>当分类问题中，某一类所占比例极小<strong>（一般将较小比例的类别置为1）</strong>，就会有偏斜数据的问题。</p><p>这时候，直接用 error 来描述在这种数据集上的问题不合适，我们需要寻求更新的数值化评价标准。</p><p>引入新的标准 precision 和 recall 之前，我们需要介绍几个新名字：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">True positive：预测结果为真，并且实际分类同样为真</span><br><span class="line">True negative：预测结果为假，并且实际分类同样为假</span><br><span class="line">False positive：预测结果为真，但是实际分类为假</span><br><span class="line">False negative：预测结果为假，但是实际分类为真</span><br></pre></td></tr></table></figure></p><p>说起来有点复杂，来看下面这张图一目了然：<br><img src="https://i.imgur.com/bVYkrZU.png" alt=""></p><p>接下来，我们在此基础上，介绍两个评价标准：</p><p><strong>查准率（precision）</strong>：在查出有癌症的患者中，实际患病的概率，公式见上图。<strong>precision 越高算法越好</strong>；</p><p><strong>召回率（recall）：</strong>所有身患癌症的病人中，有多少概率被查出，公式见上图。 <strong>recall越高算法越好</strong>；</p><p>再回到一开始的例子，如果我全部将病人视为健康，准确率是99.5%，但是我的查准率和召回率极低。可以判定不是一个好算法。</p><p>回到预测癌症例子</p><p>如果为了保证确诊率，可以将逻辑回归的分类值由0.5改为0.7，这样一来 precision 必然会升高，但是也会导致 recall 的下降</p><p>如果为了引起大家的更多重视，可以将逻辑回归的分类值由0.5改为0.3，这样一来 precision 必然会下降，但是也会导致 recall 的升高</p><p>貌似 precision（记为P） 和 recall（记为R） 总是背道而驰，偏偏两者还都是重要的偏斜类上的算法评价标准，如何将这两个标准合为一个？平均值显然是不行的，P=1且R=0时平均值为0.5，看不出端倪。</p><p><strong>F值（F-score）</strong>：F=2PR/(P+R)，完美地解决了这个问题。F值的中心思想就是：赋予P、R中更低的值更大的权值，因为P、R总是此消彼长，这样就制约两者必须同时保持在较大值才使得F值较高。可以用于<strong>所有有着此消彼长关系的标准的综合评价</strong>。</p><h3 id="7-Using-Large-Data-Sets"><a href="#7-Using-Large-Data-Sets" class="headerlink" title="7. Using Large Data Sets"></a>7. Using Large Data Sets</h3><p>一般来说，增大数据集，可以提高算法的 accuracy，但也不全是这样。比如房价预测，如果我仅仅给你房子的面积，而没有房子在市中心还是偏远地区？房龄多少？等信息，我们是无法进行良好预测的。所以，我们需要知道的前提条件是： </p><p><strong>如果当前的特征已经足够预测，增大数据集的确可以提高准确性 </strong><br>总结： </p><ol><li><p>想要保证bias小，就要保证有足够多的feature，即linear/logistics regression中有很多parameters，neuron networks中应该有很多hidden layer neurons</p></li><li><p>想要保证variance小，就要保证不产生overfit，那么就需要很多data set（只要样本数远远大于特征数，是无法过拟合每个点）。这里需要Jcv和Jtrain都很小，才能使Jtest相对小</p></li></ol><p>综上所述，对数据及进行理性分析的结果是两条： </p><p><strong>首先，x中有足够多的feature，以得到low bias;</strong><br><strong>其次，有足够大的training set，以得到low variance</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (5) Neural Network Part2</title>
    <link href="https://paradoxallen.github.io/53070/"/>
    <id>https://paradoxallen.github.io/53070/</id>
    <published>2017-08-29T16:00:00.000Z</published>
    <updated>2018-06-15T04:54:42.481Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><p>上一篇文章，介绍了<strong>前向传播(forward propagation)</strong>的过程，以及神经网络计算非线性问题的例子(XOR问题)</p><p>这一篇文章，开始介绍，如何来计算神经网络中各种参数的方法：<strong>后向传播(backward propagation)</strong></p><h3 id="1-Cost-Function"><a href="#1-Cost-Function" class="headerlink" title="1. Cost Function"></a>1. Cost Function</h3><p>为了拟合神经网络的各个参数，我们首先需要规定一些变量：<br><img src="https://i.imgur.com/GxSynvL.png" alt=""></p><p>不同分类问题：</p><p>若分为2类，其实用一个神经元作为输出层就可以了，用y=0和y=1区分；</p><p>多类问题，利用以前说过的，分为K类则最终的y∈R^K，例如分为三类的问题输出可选择为<img src="https://i.imgur.com/ariPfuP.png" alt=""></p><p>因为同样是分类问题，我们回想一下逻辑回归的cost function，其实神经网络的相同，只不过是对于分为K类问题的版本而已：<br><img src="https://i.imgur.com/9XJbSNE.png" alt=""></p><p>是不是……太复杂了，如果用 cost function 计算梯度之后梯度直接梯度下降法来计算神经网络参数，明显不如接下来介绍的这种方法简单快捷。</p><h3 id="2-Backpropagation"><a href="#2-Backpropagation" class="headerlink" title="2. Backpropagation"></a>2. Backpropagation</h3><p><strong>后向传播</strong>，是神经网络中用于替代直接计算梯度下降法，来设定各层参数的方法。方法的精髓在于，<strong>训练网络时先根据初始化的参数（一般是随机设定），计算得到最后一层（输出层）的输出，计算与实际网络输出之间的差，再根据当前层的差，反推出上一层的差，逐渐反推到第一层。每一层根据自身层的差，来逼近真实参数</strong>。</p><p>首先我们设定某l层的某j个神经元，与真实的神经元的值，两者的差距为<img src="https://i.imgur.com/Nc3lrrH.png" alt="">，the error of node j in layer l.</p><p>对于输出层的每个神经元来说，可以直接计算：<img src="https://i.imgur.com/3Ds9vuD.png" alt="">，向量化表示为<img src="https://i.imgur.com/cgaxaK5.png" alt=""></p><p>对于其他层的每个神经元而言，就需要依靠上一层神经元的计算结果来反推，<strong>反推的过程可以视为原本 Forwardpropagation 过程中分散到各个下一层神经元的水流沿着同样的道路再次汇聚到上一层的神经元中</strong>：<img src="https://i.imgur.com/9XmWAoL.png" alt=""></p><p>对于g′(t)的计算，这里需要一点点小技巧：<br><img src="https://i.imgur.com/CqMnQoj.png" alt=""></p><p>推导过程见下：<br><img src="https://i.imgur.com/Or2ogvB.png" alt=""></p><p>不存在δ(1)，因为<strong>输入层没有误差</strong></p><p>我们首先不考虑正则化项（最小化各个系数<img src="https://i.imgur.com/AbSeYkS.png" alt="">，得到计算各层各个单元的误差项之和</p><p>因此，我们得到求出神经网络各层参数的方法：<br><img src="https://i.imgur.com/SKUhVtK.png" alt=""></p><h3 id="3-Gradient-Checking"><a href="#3-Gradient-Checking" class="headerlink" title="3. Gradient Checking"></a>3. Gradient Checking</h3><p>当我们需要验证求出的神经网络工作的对不对呢？可以使用 gradient checking，通过check梯度判断我们的code有没有问题</p><p>对于下面这个[θ−J(θ)]图，取θ点左右各一点(θ−ϵ)与(θ+ϵ)，则点θ的梯度近似等于<img src="https://i.imgur.com/xliVzIX.png" alt=""></p><p><img src="https://i.imgur.com/XvkBIeh.png" alt=""></p><p>对于神经网络中的情况，则有：<br><img src="https://i.imgur.com/SPV9RtR.png" alt=""></p><p>由于在backpropagation算法中我们一直能得到J(θ)的导数D（derivative），那么就可以将这个近似值与D进行比较</p><p><strong>Summary: </strong><br>(1) 在backpropagation中计算出J(θ)对θ的导数D并组成vector（Dvec） </p><p>(2) 用numerical gradient checking方法计算大概的梯度<img src="https://i.imgur.com/xa4LZR9.png" alt=""></p><p>(3) 看是否得到相同（or相近）的结果 </p><p>(4) <strong>（非常重要）</strong>停止checking，只用 backpropagation 进行神经网络学习，否则会非常非常慢</p><h3 id="4-Backpropagation-in-Practice"><a href="#4-Backpropagation-in-Practice" class="headerlink" title="4. Backpropagation in Practice"></a>4. Backpropagation in Practice</h3><p>这一节我们来看看实际 octave/MATLAB 编程中的一些技巧，例如对于神经网络如下：<br><img src="https://i.imgur.com/e2uWpzX.png" alt=""><br>当s1=10,s2=10时，则有θ(1)∈R10×11,θ(2)∈R10×11，一般来说，为了方便变形与传递参数，我们是将所有θ展开成一个完整的变量：<br>    thetaVec=[Theta1(:);Theta2(:);Theta3(:)] </p><p>再次展开的时候，例如重组为θ(1)时，取出其中前110个重组就好：<br>    Theta1=reshape(thetaVec(1:110),10,11)</p><p>如何初始化各层的参数呢？这同样是一个需要注意的地方：<strong>不能将各层的各个神经元的参数赋值为相同的数</strong>。</p><p>有兴趣的同学可以计算一下，这样神经网络的某一层内的所有神经元计算都变得相同，这样神经网络的非线性程度就降低了。<strong>一般来说，都是在(+ϵ,−ϵ)之间随机赋值</strong></p><p>我的神经网络需要有多少层呢？同样是个有趣的问题，一般来说，<strong>三层结构（仅仅一个隐藏层）</strong>已经足够来处理大部分非线性情况。如果分类效果不好，可以尝试使用更多的隐藏层，但需要保证每个隐藏层的神经元个数相同，层数越多就越慢，当然一般来说分类效果就更好</p><p>每层神经网络需要多少个神经元？能确定的只有输入层与输出层：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">No. of input units: Dimension of features</span><br><span class="line">No. output units: Number of classes</span><br><span class="line">Other units: who knows… usually the more the better</span><br></pre></td></tr></table></figure></p><h3 id="5-Put-it-together"><a href="#5-Put-it-together" class="headerlink" title="5. Put it together"></a>5. Put it together</h3><p>最终我们回顾一下神经网络的主要步骤： </p><p><strong>randomly initialize weights</strong></p><p><strong>(for 1 to m) forward-propagation</strong></p><p><strong>(for 1 to m) cost function</strong></p><p><strong>(for 1 to m) backward-propogation</strong></p><p><strong>gradient checking, then stop</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (4) Neural Network Part1</title>
    <link href="https://paradoxallen.github.io/21187/"/>
    <id>https://paradoxallen.github.io/21187/</id>
    <published>2017-08-22T16:00:00.000Z</published>
    <updated>2018-06-14T00:26:45.751Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-由XOR-Problem想到"><a href="#1-由XOR-Problem想到" class="headerlink" title="1. 由XOR Problem想到"></a>1. 由XOR Problem想到</h3><p>有一种经典的非线性问题：<strong>XOR，也就是异或</strong>。真值表如下： </p><p>0 0 | 0<br>1 0 | 1<br>1 1 | 0<br>0 1 | 1，| 左侧是输入，| 右侧是输出 </p><p>如果在二维坐标系上绘图，可以看出来仅利用一条直线是无法对这个问题分类的，也就是<strong>线性不可分</strong>问题。</p><p>如果利用逻辑回归的方法，可以枚举各种特征的出现可能，即<br>继续想下去，如果基础特征量更多的话？就会出现过拟合的问题，同时带来极大的计算量。</p><p>例如，计算机视觉中处理一张照片，每个像素都需要一个数值表示。对于100*100像素值的图片，仅仅考虑二次项等级，就有特征数量为5000个（与相同，故除以2）。</p><p>于是，这章介绍的非线性分类器，<strong>神经网络（Neural Network，NN）</strong>就发挥了作用。</p><h3 id="2-人工神经网络"><a href="#2-人工神经网络" class="headerlink" title="2. 人工神经网络"></a>2. 人工神经网络</h3><p>神经网络最初提出的初衷，是因为要模拟人类大脑的结构（很初级的模拟，因为人类对于自己大脑究竟是怎样都没有弄清楚）。<strong>通过多个感知机之间的输入输出，从而完成整体的智能行为</strong>。</p><p>在人工神经网络中，“感知机”就是一个有着输入与输出功能的小单元，接收上一层的输入，将输出传给下一层。</p><p>人工神经网络是层级结构，某一层上的单元之间互相不会有输入输出关系，只和上一层或者下一层的单元产生数据传输关系。至少会有两层：<strong>输入层（input layer）与输出层（output layer）</strong>，但是两层的神经网络可以解决的问题很少，一般都是三层或者三层以上，中间的这些层就称为<strong>“隐藏层（hidden layer）”</strong>，我们来看一个最简单的例子：<br><img src="https://i.imgur.com/U02TDKa.jpg" alt=""></p><p>由<strong>输入层，到隐藏层，最终到输出层</strong>。这是一次 <strong>forward propogation</strong> 过程。类似于逻辑回归，但是神经网络的输入是某个样本的所有基础特征，不需要考虑 这一类新加入的特征。</p><h3 id="3-回到XOR-Problem"><a href="#3-回到XOR-Problem" class="headerlink" title="3. 回到XOR Problem"></a>3. 回到XOR Problem</h3><p>先讲几个基础的利用神经网络进行二进制运算分类的问题：</p><ol><li><p>二进制 AND<br><img src="https://i.imgur.com/5pgAvdw.jpg" alt=""><br>即为，只有1,1时返回值才为1，符合 AND 的操作结果。</p></li><li><p>二进制 OR<br><img src="https://i.imgur.com/vNpszeX.jpg" alt=""><br>即为，只有0,0时返回值才为0，符合 OR 的操作结果。</p></li><li><p>二进制 NOT<br><img src="https://i.imgur.com/8ODYZRw.jpg" alt=""></p></li></ol><p>XOR 问题复杂一些，但是如果我们做了如下转换：</p><p><code>XNOR = NOT XOR = AND OR NOT AND NOT</code></p><p>变换的正确性，很容易通过真值表来验证。大家可以分别计算各个括号中的内容，然后通过 OR 连接起来。<br>我们将 AND 的内容视为 ，NOT AND NOT 的内容视为<br><img src="https://i.imgur.com/tcDGEAE.jpg" alt=""></p><h3 id="4-神经网络多类分类"><a href="#4-神经网络多类分类" class="headerlink" title="4. 神经网络多类分类"></a>4. 神经网络多类分类</h3><p>神经网络处理多类的分类问题是很方便的。举个例子，区分手写数字时，有10个类别：0，1，2，……，9。<br>对于某一个训练样本来说，有着特征组合,这个神经网络的输出层有10个单元。当输出层的10个单元全部取 0，意味着输入不是任何一种数字。</p><p>对于手写数字的识别，一直是业界的研究重点之一。视频中举了一篇经典的利用神经网络处理该问题的Paper，有兴趣的同学可以访问作者的个人主页查看Demo与Paper：<a href="http://yann.lecun.com/exdb/lenet/" target="_blank" rel="noopener">Yann LeCun</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (3) Logistic Regression &amp; Regularization</title>
    <link href="https://paradoxallen.github.io/26112/"/>
    <id>https://paradoxallen.github.io/26112/</id>
    <published>2017-08-15T16:00:00.000Z</published>
    <updated>2018-06-13T04:32:05.451Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Logistic-Regression"><a href="#1-Logistic-Regression" class="headerlink" title="1. Logistic Regression"></a>1. Logistic Regression</h3><p>对于分类问题而言，很容易想到利用<strong>线性回归</strong>方法，拟合之后的<strong>hθ(x)&gt;0.5则为True，其余为False</strong>.</p><p>但是线性回归有一个问题，拟合出的值都是离散的，范围不确定。</p><p>为了方便分析，我们希望将拟合出的值限制在0~1之间。因此，出现了<strong>逻辑回归</strong>。</p><p>逻辑回归的模型是一个<strong>非线性模型</strong>：<strong>sigmoid函数，又称逻辑回归函数</strong>。但它本质上又是一个线性回归模型，因为除去sigmoid映射函数关系，其他的步骤，算法都是线性回归的。</p><p>sigmoid函数（或，逻辑回归函数）：<br><img src="https://i.imgur.com/QLwtN5k.png" alt=""><br>其函数图像为：<br><img src="https://i.imgur.com/wC6xW9Z.png" alt=""></p><p>这个函数的特征非常明显<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">函数值一直在0~1范围内；</span><br><span class="line">经过(0,0.5)点。这个很容易作为区分0，1类的分界线。</span><br></pre></td></tr></table></figure></p><p>逻辑回归中，对于原本线性回归中拟合而成的hypothesis函数，需要经过sigmoid函数的修饰：<br><img src="https://i.imgur.com/ElJujkC.png" alt=""></p><p>此时，hθ(x)的含义发生了变化，<br><img src="https://i.imgur.com/6DX46yY.png" alt=""><br>。成为<br>        <strong>‘’the probability that y=1, given x, parameterized by θ’’</strong></p><p>因此有<br><img src="https://i.imgur.com/SbZ7wQN.png" alt=""></p><p><strong>Decision Boundary</strong>。表示的是 hypothesis 函数确定之后，划分数据分类的界限，并不一定可以百分百区分数据集，只是函数的属性之一。下图蓝色曲线即为某个 Desicision Boundary。<br><img src="https://i.imgur.com/nM4tJjr.jpg" alt=""></p><h3 id="2-Cost-Function"><a href="#2-Cost-Function" class="headerlink" title="2. Cost Function"></a>2. Cost Function</h3><p>回忆线性回归的 cost function，我们在其中插入 cost 函数的概念：<br><img src="https://i.imgur.com/2fhUuge.png" alt=""></p><p>完全照搬线性回归的 cost function 到逻辑回归中，因为sigmoid函数的非线性，会造成J(θ)取值的不断震荡，导致其是一个非凸形函数（non-convex）。表示在“J(θ)—θ”二维图中如下：<br><img src="https://i.imgur.com/YxpdAK6.png" alt=""></p><p>我们需要构造一种新的 cost 函数。出发点为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">当y=1时，若hypothesis函数拟合结果为0，即为“重大失误”，cost 趋于无穷大；</span><br><span class="line">当y=0时，若hypothesis函数拟合结果为1，即为“重大失误”，cost 趋于无穷大；</span><br></pre></td></tr></table></figure></p><p>构造的新 cost 函数：<br><img src="https://i.imgur.com/1AZPras.png" alt=""></p><p>如果进一步合并，可以得到最终逻辑回归的cost函数。并且值得指出的是，代入这个cost函数通过梯度下降法得到的 θ 更新函数依然成立：<br><img src="https://i.imgur.com/F16aq5V.png" alt=""></p><h3 id="3-梯度下降法的优化"><a href="#3-梯度下降法的优化" class="headerlink" title="3. 梯度下降法的优化"></a>3. 梯度下降法的优化</h3><p>对于梯度下降法的优化有很多，但是都需要J(θ)与∂J(θ)/∂θj的代码。</p><p>以此为基础的对于梯度下降法的优化（视频中都没有具体介绍，有兴趣的同学可以点击链接）有：<br><a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method" target="_blank" rel="noopener">共轭梯度法</a><br><a href="https://en.wikipedia.org/w/index.php?title=Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm&amp;redirect=no" target="_blank" rel="noopener">BFGS</a><br><a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" target="_blank" rel="noopener">L-BFGS</a></p><p>这些优化方法的特点也很一致：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">不需要人为选择 α，自适应性</span><br><span class="line">更复杂，更慢</span><br></pre></td></tr></table></figure></p><p>这里提到了两个MATLAB的非线性优化函数：<br><strong>optimset：创建或编辑一个最优化参数选项。</strong>具体调用在MATLAB中 <code>help optimset</code> 命令查看；<br><strong>fminunc：最小值优化。</strong>具体调用在MATLAB中 <code>help fminunc</code>命令查看；</p><p><strong>建议</strong>：Ng在优化这一部分讲的过于简略，基本等于什么都没说……还是要根据这几个方法名称在使用时搜索更多。</p><h3 id="4-one-vs-all-one-vs-rest"><a href="#4-one-vs-all-one-vs-rest" class="headerlink" title="4.one vs. all (one vs. rest)"></a>4.one vs. all (one vs. rest)</h3><p>如果需要进行多类的分类，需要一种精妙的修改，使得两类的分类问题得以适用于多类的分类。 </p><ol><li><p>现已知有n类样本需要区分开（1，2，3，……）；</p></li><li><p>以原1类为新1类，剩余的原2，3，……作为新2类。原本的多类问题变成了二类问题:<br><img src="https://i.imgur.com/TKq60Ef.png" alt=""></p></li><li>以原2类为新1类，剩余的原1，3，……作为新2类。再分类:<br><img src="https://i.imgur.com/EioDwff.png" alt=""></li><li><p><img src="https://i.imgur.com/wK9pOb4.png" alt=""></p></li><li><p>对于任意一个 x 而言，如何分辨是哪一类呢？于是，求出所有的<img src="https://i.imgur.com/7U91HVE.png" alt="">值最大对应的i（<strong>表示y=i的概率最大</strong>）即为x的所属分类</p></li></ol><h3 id="5-Regularization（正则化）"><a href="#5-Regularization（正则化）" class="headerlink" title="5. Regularization（正则化）"></a>5. Regularization（正则化）</h3><p>拟合会产生三种情况： </p><ol><li><p><strong>underfitting（欠拟合）=high bias</strong>，大部分训练样本无法拟合</p></li><li><p><strong>overfitting（过拟合）=high variance</strong>，为了拟合几乎每一个训练样本。导致拟合函数极为复杂，易产生波动，泛化（generalize）能力差，虽然训练样本几乎百分百拟合，但是测试样本很可能因为极大波动而极少拟合成功</p></li><li><p><strong>just right</strong>，对于训练样本，拟合得不多不少刚刚好，并且泛化到测试样本拟合效果同样较好</p></li></ol><p>欠拟合，比较好解决，创造并引入更多的特征即可。例如：对于x,y而言，可以引入x2,y2,xy等等新的特征</p><p>过拟合，则比较复杂。可用的方法有两个： </p><ol><li><p><strong>Reduce number of features, 减少特征量</strong></p></li><li><p><strong>Regularization，正则化</strong>。保持所有的特征数量不变，而去改变特征前的度量单位 θj（若 θj 趋于0，则此特征可视为无影响）</p></li></ol><p>解决过拟合的正则化方法，因此需要引入全新的优化目标到 cost function 中。原先的 cost function 只是希望适合拟合更为接近，现在还需要使得特征前的度量单位 θj 的最小。因此有：<br><img src="https://i.imgur.com/Msp5tw4.png" alt=""></p><p>正则化方法处理之后，∂J(θ)/∂θj发生对应变化，因此我们有：<br><img src="https://i.imgur.com/dmhGxHY.png" alt=""></p><p>若λ非常大（例如10^10），则正则化方法会导致结果 underfitting。这也很好理解，因为优化目标中有使得 <img src="https://i.imgur.com/ctA4muM.png" alt=""> 尽可能小，这样会导致 θ 全部趋于 0。</p><p>一般来说，<strong>α,λ,m&gt;0，所以(1−αλ/m)&lt;1，常见使其取值0.99 左右</strong></p><h3 id="6-Regularization-for-Normal-Equation"><a href="#6-Regularization-for-Normal-Equation" class="headerlink" title="6. Regularization for Normal Equation"></a>6. Regularization for Normal Equation</h3><p>课程视频中缺少证明，因此我们仅需掌握结论使用即可<br>对于 Week 2 中的<strong>Normal Equation</strong>方法，原本需要求解的方程<br><img src="https://i.imgur.com/ZiPg749.png" alt=""><br>做一个小小的改动：<br><img src="https://i.imgur.com/cZwlXun.png" alt=""></p><p>若样本拥有n个特征，则<img src="https://i.imgur.com/Gu1x2Z9.png" alt="">表示的是(n+1) * (n+1)维的对角矩阵，除了(0, 0)取值为 0，其余对角位置取 1。</p><p><strong>non-invertibility</strong>：非不可逆性……好拗口，意思就是对于原本的(xTx)矩阵可能会出现不可逆的情况。但是，对于正则化之后的矩阵<img src="https://i.imgur.com/KB79QV2.png" alt="">一定是可逆的（未提供证明）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (2) Linear Regression</title>
    <link href="https://paradoxallen.github.io/58080/"/>
    <id>https://paradoxallen.github.io/58080/</id>
    <published>2017-08-09T16:00:00.000Z</published>
    <updated>2018-06-13T04:32:05.448Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Multivariate-Linear-Regression"><a href="#1-Multivariate-Linear-Regression" class="headerlink" title="1. Multivariate Linear Regression"></a>1. Multivariate Linear Regression</h3><p>Week 1 讨论仅一个特征，即仅有一个未知量x影响了目标y的取值。如果现在有很多特征？现在我们有x1,x2…xn影响了目标y的取值。</p><p>此时需要区分的是变量标记规则：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xi表示的是第i个特征</span><br><span class="line">x(i)表示的是第i个样本，一个样本是由多个特征组成的列向量</span><br></pre></td></tr></table></figure></p><p>例如：<br><img src="https://i.imgur.com/1VQTtI7.png" alt=""><br>综上，我们有<br><img src="https://i.imgur.com/IN8QuHt.png" alt=""><br>可以视为，每个样本都多出一个特征：x0=1，这样表示有利于之后的矩阵表示</p><h3 id="2-多变量梯度下降法："><a href="#2-多变量梯度下降法：" class="headerlink" title="2. 多变量梯度下降法："></a>2. 多变量梯度下降法：</h3><p><img src="https://i.imgur.com/jqtDLIq.png" alt=""></p><h3 id="3-Feature-Scaling（特征缩放）"><a href="#3-Feature-Scaling（特征缩放）" class="headerlink" title="3. Feature Scaling（特征缩放）"></a>3. Feature Scaling（特征缩放）</h3><p>很简单，就是将每种特征的数据范围限定在同一个数量级。例如<br><img src="https://i.imgur.com/cauqMeB.png" alt=""><br>这样会导致迭代次数过多。这时候，如果我们找到一种mapping方式，使得两者属于同一个数量级的范围内，可以有效减少迭代次数</p><p><strong>注意</strong>：无法降低单次的迭代时间，但是却能有效地降低迭代次数</p><p>其实方法很多，这有一种：<br><img src="https://i.imgur.com/egjFhiQ.png" alt=""><br>其中，mean(x)表示向量每个元素的平均值，max(x)表示向量中最大元素，min(x)表示向量中最小元素</p><h3 id="4-Learning-Rate"><a href="#4-Learning-Rate" class="headerlink" title="4. Learning Rate"></a>4. Learning Rate</h3><p>learning rate 是机器学习中的一个不稳定因素，如何判断选取的 learning rate 是合适的？我们可以看看以下这幅图：<br><img src="https://i.imgur.com/81mq8t6.jpg" alt=""></p><p>如果以迭代次数为横坐标，cost function 结果为纵坐标，绘制的图像是递减的，说明 learning rate 选择的是恰当的。如果碰到下图所显示的三种情况，那就只有一条路：<strong>减小 learning rate </strong><br><img src="https://i.imgur.com/DmExZo6.jpg" alt=""></p><p>但是 learning rate 太小同样会导致一个问题：<strong>学习过慢</strong>。所以，只能靠试：0.001，0.003，0.01，0.03，0.1，0.3……</p><h3 id="5-Polynomial-Regression（多项式回归，不同于多变量线性回归）"><a href="#5-Polynomial-Regression（多项式回归，不同于多变量线性回归）" class="headerlink" title="5. Polynomial Regression（多项式回归，不同于多变量线性回归）"></a>5. Polynomial Regression（多项式回归，不同于多变量线性回归）</h3><p>有时候，我们需要自己创造一些“特征”，来拟合一些非线性分布情况<br>例如：<br><img src="https://i.imgur.com/5F2PbWA.png" alt=""><br>看上去只有一个特征x，但我们完全可以理解为x^2和√x都是单独的新特征</p><p>以后的课程会具体讲述如何选择这些特征</p><h3 id="6-Normal-Equation"><a href="#6-Normal-Equation" class="headerlink" title="6. Normal Equation"></a>6. Normal Equation</h3><p>梯度下降法可以用于寻找函数（cost function）的最小值，想一想，初高中的时候我们使用的是什么方法？最小值点的导数为零，然后解方程</p><p>将导数置为零这种方法即<strong> Normal Equation</strong>。<br><img src="https://i.imgur.com/aqLezpl.png" alt=""></p><p>上文提过，增加一个全1分量x0后得到<br><img src="https://i.imgur.com/N6qtlas.png" alt=""></p><p>可以得到：<br><img src="https://i.imgur.com/LU4vw47.png" alt=""></p><p>matlab编程十分简单：<br><img src="https://i.imgur.com/OVUbKD9.png" alt=""></p><p>Normal Equation 有以下<strong>优缺点</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">不需要 learning rate，也就不需要选择；</span><br><span class="line">不需要迭代，不需要考虑收敛的问题；</span><br><span class="line">当特征非常多的时候，因为涉及求逆操作，会非常慢（注：方阵才有逆矩阵）</span><br></pre></td></tr></table></figure></p><h3 id="7-Octave-Tutorial"><a href="#7-Octave-Tutorial" class="headerlink" title="7. Octave Tutorial"></a>7. Octave Tutorial</h3><p>这一部分十分简单，其实就是MATLAB的使用方法。建议不论是否初学者都去看看，会有收获。 </p><p>谈到一个问题：<strong>如果现有的样本数，小于每个样本所有的特征数怎么办？去除多余的特征（PCA？）</strong>。特征过多，也可能会导致矩阵不可逆的情况。 </p><p>下面记录一些觉得挺有趣的命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">~=：不等于号</span><br><span class="line">xor(0, 1)：异或操作</span><br><span class="line">rand(m, n)：0~1之间的大小为m*n的随机数矩阵；randn：产生均值为0，方差为1的符合正态分布的随机数（有负数）</span><br><span class="line">length(A)：返回A中行、列中更大值</span><br><span class="line">A(:)：将矩阵A变为列向量形式，不论A是向量还是矩阵</span><br><span class="line">sum(A,1)：每列求和得到一个行向量；sum(A,2)：每行求和得到一个列向量</span><br><span class="line">pinv：伪求逆；inv：求逆</span><br><span class="line">imagesc(A)：帅爆！根据矩阵中每个值绘制各种颜色的方块</span><br><span class="line">A.^2 ~= A^2，后者是两个矩阵相乘</span><br></pre></td></tr></table></figure></p><h3 id="8-Submitting-Programming-Assignments"><a href="#8-Submitting-Programming-Assignments" class="headerlink" title="8. Submitting Programming Assignments"></a>8. Submitting Programming Assignments</h3><p>其实看看视频就行了，主要要注意，submit() 时输入的Token，不是Coursera 的密码，而是作业的密码：<br><img src="https://i.imgur.com/a8OsCRt.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (1) Introduction</title>
    <link href="https://paradoxallen.github.io/36971/"/>
    <id>https://paradoxallen.github.io/36971/</id>
    <published>2017-08-04T16:00:00.000Z</published>
    <updated>2018-06-13T04:32:05.450Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Environment-Setup-Instructions"><a href="#1-Environment-Setup-Instructions" class="headerlink" title="1. Environment Setup Instructions"></a>1. Environment Setup Instructions</h3><p>这一章介绍课程一般使用的工具。octave或者matlab即可，这两者本质上没有什么区别，都有着丰富的数学库计算库。</p><h3 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h3><ol><li><strong>机器学习定义：</strong>简单来说，让计算机执行一些行为，但是without explicit programmed。</li><li><p><strong>监督学习（supervised learning）：</strong>通过<strong>已知</strong>的正确信息，得到未知的信息。一般用于解决的问题有两类： </p><ol><li><p><strong>回归问题（regression）</strong><br> 例如：房价预测。已知一百平房子售价，两百平房子售价，三百平房子售价。那么，一百五十平房子的售价呢？</p><p> 很容易想到，<strong>拟合</strong>。那么接下来，是直线拟合？二次曲线拟合？</p><p> 这就是回归问题要解决的，通过已知离散信息，<strong>预测</strong>未知的连续信息。</p></li><li><p><strong>分类问题（classification）</strong><br> 例如：肿瘤分类。已知肿瘤size与肿瘤良性（label: 0）还是恶性（label: 1）分类的关系如下：<br><img src="https://i.imgur.com/MlGVT51.jpg" alt=""></p><p> 更进一步，完全可以用一维坐标图来表示：<br><img src="https://i.imgur.com/xI6gTxQ.jpg" alt=""></p><p> 现在需要对于根据size的对某个肿瘤判断它是良性还是恶性，就是分类问题。</p><p> 再进一步，现在我仅仅把size作为feature之一，如果我们加入考虑肿瘤积水（二维坐标图）？肿瘤硬度（三维坐标图）？原则上可以考虑无限种feature，如何处理？这里卖了个关子，可以使用支持向量机（SVM）。</p></li><li><p><strong>无监督学习（unsupervised learning）/ 聚类算法（clustering）：</strong><br> 对于有监督学习而言，我们已知的信息很多：每个data的label是什么，label可以分为多少类，等等。<strong>但是无监督学习中，这些都是未知的。</strong><br> 最常见的应用：浏览新闻时经常会有的相关新闻，或者搜索时的相关搜索结果，又或是weibo中的推荐分组，等等。</p><p> 可以理解为计算机自己理解后将信息分为了若干类。</p><p> 经典案例：<a href="http://soma.mcmaster.ca/papers/Paper_7.pdf" target="_blank" rel="noopener"><strong>cocktail party problem</strong></a>。简而言之，就是在聚会上摆俩麦克风分别记录声音，需要通过这些信息将不同声源的声音分隔开来。</p><p> 如果考虑声音本身的特征与背景知识，解法会比较复杂，需要大量的先验知识。但站在机器学习的角度来思考，就是一类无监督学习的应用场景。matlab实现仅需几行代码而已。</p></li></ol></li></ol><h3 id="3-Bonus-Course-Wiki-Lecture-Notes"><a href="#3-Bonus-Course-Wiki-Lecture-Notes" class="headerlink" title="3. Bonus: Course Wiki Lecture Notes"></a>3. Bonus: Course Wiki Lecture Notes</h3><p><strong>单一变量线性回归</strong>：即通过线性方程来拟合已知data的feature，与label之间的关系。表达为<br><img src="https://i.imgur.com/zMe7YGr.png" alt=""></p><p>不同的线性方程之间，需要挑出来一个拟合更适合的，这就需要一个判断标准。常用的基于<strong>Mean squared error (MSE)</strong>：<br><img src="https://i.imgur.com/adK8AA2.png" alt=""></p><p>如果让计算机来实现这个过程，怎么办呢？<strong>梯度下降法（Gradient Descent）</strong>是一种常见手段。<br>该方法的核心是：<br><img src="https://i.imgur.com/i2kZYPl.png" alt=""></p><p>其中α是下降步长，人为确定。可以看出来，梯度下降法的目的是使J(θi,θj)越来越小，对于其是否线性，参数的数量，都没有限制。</p><p>最后利用梯度下降法代入导数（不论是链式法则，还是单个变量代入）得到最优的拟合函数参数：<br><img src="https://i.imgur.com/VBHlmsF.jpg" alt=""></p><h3 id="4-Model-and-Cost-Function"><a href="#4-Model-and-Cost-Function" class="headerlink" title="4. Model and Cost Function"></a>4. Model and Cost Function</h3><p>再次回顾一下监督学习，两类：<strong>regression和classification</strong>。<br>简单来说，<strong>regression是predict real-valued output，classification是predict discrete-valued output。</strong><br>注明了以下符号，方便以后沟通：<br><img src="https://i.imgur.com/0Lr7GzM.png" alt=""></p><p>我们设计拟合，目的就是为了使得表示拟合数据与已知真实数据直接差距的cost function最小，这个cost function是啥？就是我们之前提到的判断标准，常见的为：<br><img src="https://i.imgur.com/GiYdN6C.png" alt=""><br>当有两个未知量时，cost function就需要表示为三维图：<br><img src="https://i.imgur.com/DJLSXXG.jpg" alt=""></p><p>为了方便表示，也可以把上图表示为类似地理上的“等高线”。术语称为 <strong>contour plot（轮廓图）</strong>：<br><img src="https://i.imgur.com/ZbnTLxm.jpg" alt=""></p><h3 id="5-Parameter-Learning"><a href="#5-Parameter-Learning" class="headerlink" title="5. Parameter Learning"></a>5. Parameter Learning</h3><p>现在，我们开始具体来看看，第一个机器学习算法：<strong>梯度下降法（gradient descent）</strong></p><p><img src="https://i.imgur.com/SAMHpSX.png" alt=""><br><strong>特别注意</strong>，这里的temp是为了保证更新是同一时间发生的，这才是最正宗的梯度下降法。如果先更新了θ0，再用更新之后的θ0去更新θ1，就违背了梯度下降法的初衷。</p><p>可能你已经发现，<strong>这里的最优值，仅仅是局部的（local）而不是全局的（global）</strong>。因此选取的初值不同，可能会导致算法停留在不同的最优值。因此，这是梯度下降法的一个缺陷。但是对于线性回归而言，cost function 是一个convex function（形状为弓形），仅有一个最优值，局部最优值就是全局最优值。</p><p>到此为止，我们说的其实都是：<strong>batch</strong> gradient descent。也就是我的cost function是所有样本的MSE之和。如果不是计算所有样本，仅仅是计算某个重要子集的MSE之和，这个方法在以后的课程中会提到。</p><p>最后，提到线性代数中的寻找极值的方法：<strong>normal equations</strong>。但是在大规模的数据计算中，还是梯度下降法更为适用。</p><h3 id="6-Linear-Algebra-Review"><a href="#6-Linear-Algebra-Review" class="headerlink" title="6. Linear Algebra Review"></a>6. Linear Algebra Review</h3><p>矩阵：<a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Matrix_(mathematics)</a></p><p>向量：特殊的矩阵，注意是 列数 = 1</p><p>1-index：下标从1开始。0-index：下标从0开始。</p><p>矩阵乘法没有交换律，但是有结合律。</p><p>只有方阵（#col=#row）才有逆矩阵。但仅仅满足方阵这一个条件并不足够，同时需要满足行列式不等于0。没有逆矩阵的矩阵，称之为“奇异（singular）矩阵”或“退化（degenerate）矩阵”。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(5) 常见算法优缺点</title>
    <link href="https://paradoxallen.github.io/65434/"/>
    <id>https://paradoxallen.github.io/65434/</id>
    <published>2017-06-09T16:00:00.000Z</published>
    <updated>2018-06-13T05:09:21.235Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p><a href="https://paradoxallen.github.io/62602/">机器学习(三) 模型结果应用</a></p><p><a href="https://paradoxallen.github.io/21484/">机器学习(四) 常见算法优缺点</a></p><p>文章结构：</p><ul><li><p><strong>什么是感知器分类算法</strong></p></li><li><p><strong>在Python中实现感知器学习算法</strong></p></li></ul><p><em>在iris（鸢尾花）数据集上训练一个感知器模型</em></p><ul><li><strong>自适应线性神经元和融合学习</strong></li></ul><p><em>使用梯度下降方法来最小化损失函数</em></p><p><em>在Python中实现一个自适应的线性神经元</em></p><a id="more"></a><hr><h3 id="什么是感知器分类算法"><a href="#什么是感知器分类算法" class="headerlink" title="什么是感知器分类算法"></a><strong>什么是感知器分类算法</strong></h3><p>设想我们改变逻辑回归算法，“迫使”它只能输出-1或1抑或其他定值。在这种情况下，之前的逻辑函数‍‍g就会变成阈值函数sign：</p><p><img src="https://i.imgur.com/TwrMWwh.png" alt=""></p><p><img src="https://i.imgur.com/pDzWDxS.png" alt=""></p><p>如果我们令假设为hθ(x)=g(θTx)hθ(x)=g(θTx)，将其带入之前的迭代法中：</p><p><img src="https://i.imgur.com/r4P3819.png" alt=""></p><p>至此我们就得出了感知器学习算法。简单地来说，感知器学习算法是神经网络中的一个概念，单层感知器是最简单的神经网络，输入层和输出层直接相连。</p><p><img src="https://i.imgur.com/Nb3JtYy.png" alt=""></p><p>每一个输入端和其上的权值相乘，然后将这些乘积相加得到乘积和，这个结果与阈值相比较（一般为0），若大于阈值输出端就取1，反之，输出端取-1。</p><p>初始权重向量W=[0,0,0]，更新公式W(i)=W(i)+ΔW(i)；ΔW(i)=η<em>(y-y’)</em>X(i)； </p><p>η：学习率，介于[0,1]之间 </p><p>y：输入样本的正确分类 </p><p>y’：感知器计算出来的分类 </p><p>通过上面公式不断更新权值，直到达到分类要求。</p><p><img src="https://i.imgur.com/RlHERhT.jpg" alt=""></p><p>初始化权重向量W，与输入向量做点乘，将结果与阈值作比较，得到分类结果1或-1。</p><hr><h3 id="在Python中实现感知器学习算法"><a href="#在Python中实现感知器学习算法" class="headerlink" title="在Python中实现感知器学习算法"></a><strong>在Python中实现感知器学习算法</strong></h3><p>下面直接贴上实现代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Perceptron(object):</span><br><span class="line">    &quot;&quot;&quot;Perceptron classifier.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ------------</span><br><span class="line">    eta : float</span><br><span class="line">        Learning rate (between 0.0 and 1.0)</span><br><span class="line">    n_iter : int</span><br><span class="line">        Passes over the training dataset.</span><br><span class="line"></span><br><span class="line">    Attributes</span><br><span class="line">    -----------</span><br><span class="line">    w_ : 1d-array</span><br><span class="line">        Weights after fitting.</span><br><span class="line">    errors_ : list</span><br><span class="line">        Number of misclassifications (updates) in each epoch.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, eta=0.01, n_iter=10):</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y):</span><br><span class="line">        &quot;&quot;&quot;Fit training data.</span><br><span class="line"></span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span><br><span class="line">            Training vectors, where n_samples is the number of samples and</span><br><span class="line">            n_features is the number of features.</span><br><span class="line">        y : array-like, shape = [n_samples]</span><br><span class="line">            Target values.</span><br><span class="line"></span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        self : object</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.w_ = np.zeros(1 + X.shape[1])</span><br><span class="line">        self.errors_ = []</span><br><span class="line"></span><br><span class="line">        for _ in range(self.n_iter):</span><br><span class="line">            errors = 0</span><br><span class="line">            for xi, target in zip(X, y):</span><br><span class="line">                update = self.eta * (target - self.predict(xi))</span><br><span class="line">                self.w_[1:] += update * xi</span><br><span class="line">                self.w_[0] += update</span><br><span class="line">                errors += int(update != 0.0)</span><br><span class="line">            self.errors_.append(errors)</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def net_input(self, X):</span><br><span class="line">        &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span><br><span class="line">        return np.dot(X, self.w_[1:]) + self.w_[0]</span><br><span class="line"></span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span><br><span class="line">        return np.where(self.net_input(X) &gt;= 0.0, 1, -1)</span><br></pre></td></tr></table></figure><p><strong>特别说明：</strong></p><p>学习速率η(eta)只有在权重（一般取值0或者很小的数）为非零值的时候，才会对分类结果产生作用。如果所有的权重都初始化为0，学习速率参数eta只影响权重向量的大小，而不影响其方向，为了使学习速率影响分类结果，权重需要初始化为非零值。需要更改的代码中的相应行在下面突出显示:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, eta=0.01, n_iter=50, random_seed=1): # add random_seed=1</span><br><span class="line">    ...</span><br><span class="line">    self.random_seed = random_seed # add this line</span><br><span class="line">def fit(self, X, y):</span><br><span class="line">    ...</span><br><span class="line">    # self.w_ = np.zeros(1 + X.shape[1]) ## remove this line</span><br><span class="line">    rgen = np.random.RandomState(self.random_seed) # add this line</span><br><span class="line">    self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1]) # add this line</span><br></pre></td></tr></table></figure></p><p><strong>在iris（鸢尾）数据集上训练一个感知器模型</strong></p><p><strong>读取iris数据集</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import collections</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(&apos;https://archive.ics.uci.edu/ml/&apos;</span><br><span class="line">        &apos;machine-learning-databases/iris/iris.data&apos;, header=None)</span><br><span class="line">print (df.head())</span><br><span class="line">print (&quot;\n&quot;)</span><br><span class="line">print (df.describe())</span><br><span class="line">print (&quot;\n&quot;)</span><br><span class="line">print (collections.Counter(df[4]))</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/tRDUNXi.jpg" alt=""></p><p><strong>可视化iris数据</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 为了显示中文(这里是Mac的解决方法，其他的大家可以去百度一下)</span><br><span class="line">from matplotlib.font_manager import FontProperties</span><br><span class="line">font = FontProperties(fname=&apos;/System/Library/Fonts/STHeiti Light.ttc&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 选择 setosa and versicolor类型的花</span><br><span class="line">y = df.iloc[0:100, 4].values</span><br><span class="line">y = np.where(y == &apos;Iris-setosa&apos;, -1, 1)</span><br><span class="line"></span><br><span class="line"># 提取它们的特征 （sepal length and petal length）</span><br><span class="line">X = df.iloc[0:100, [0, 2]].values</span><br><span class="line"></span><br><span class="line"># 可视化数据，因为数据有经过处理，总共150行数据，1-50行是setosa花，51-100是versicolor花，101-150是virginica花</span><br><span class="line">plt.scatter(X[:50, 0], X[:50, 1],</span><br><span class="line">            color=&apos;red&apos;, marker=&apos;o&apos;, label=&apos;setosa&apos;)</span><br><span class="line">plt.scatter(X[50:100, 0], X[50:100, 1],</span><br><span class="line">            color=&apos;blue&apos;, marker=&apos;x&apos;, label=&apos;versicolor&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlabel(&apos;sepal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;petal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/wR17A5s.png" alt=""></p><p><strong>训练感知器模型</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Perceptron是我们前面定义的感知器算法函数，这里就直接调用就好</span><br><span class="line">ppn = Perceptron(eta=0.1, n_iter=10)</span><br><span class="line"></span><br><span class="line">ppn.fit(X, y)</span><br><span class="line"></span><br><span class="line">plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=&apos;o&apos;)</span><br><span class="line">plt.xlabel(&apos;迭代次数&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;权重更新次数（错误次数）&apos;,FontProperties=font,fontsize=14)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/e6o2LBT.png" alt=""></p><p><strong>绘制函数决策区域</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib.colors import ListedColormap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot_decision_regions(X, y, classifier, resolution=0.02):</span><br><span class="line"></span><br><span class="line">    # setup marker generator and color map</span><br><span class="line">    markers = (&apos;s&apos;, &apos;x&apos;, &apos;o&apos;, &apos;^&apos;, &apos;v&apos;)</span><br><span class="line">    colors = (&apos;red&apos;, &apos;blue&apos;, &apos;lightgreen&apos;, &apos;gray&apos;, &apos;cyan&apos;)</span><br><span class="line">    cmap = ListedColormap(colors[:len(np.unique(y))])</span><br><span class="line"></span><br><span class="line">    # plot the decision surface</span><br><span class="line">    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),</span><br><span class="line">                           np.arange(x2_min, x2_max, resolution))</span><br><span class="line">    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br><span class="line">    Z = Z.reshape(xx1.shape)</span><br><span class="line">    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)</span><br><span class="line">    plt.xlim(xx1.min(), xx1.max())</span><br><span class="line">    plt.ylim(xx2.min(), xx2.max())</span><br><span class="line"></span><br><span class="line">    # plot class samples</span><br><span class="line">    for idx, cl in enumerate(np.unique(y)):</span><br><span class="line">        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],</span><br><span class="line">                    alpha=0.8, c=cmap(idx),</span><br><span class="line">                    edgecolor=&apos;black&apos;,</span><br><span class="line">                    marker=markers[idx], </span><br><span class="line">                    label=cl)</span><br><span class="line">plot_decision_regions(X, y, classifier=ppn)</span><br><span class="line">plt.xlabel(&apos;sepal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;petal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/AFltDVw.png" alt=""></p><hr><h3 id="自适应线性神经元和融合学习"><a href="#自适应线性神经元和融合学习" class="headerlink" title="自适应线性神经元和融合学习"></a><strong>自适应线性神经元和融合学习</strong></h3><p><strong>使用梯度下降方法来最小化损失函数</strong></p><p>梯度下降的方法十分常见，具体的了解可以参考附录的文章[2]，如今，梯度下降主要用于在神经网络模型中进行权重更新，即在一个方向上更新和调整模型的参数，来最小化损失函数。</p><p><img src="https://i.imgur.com/pYoV9cF.jpg" alt=""><br>图：梯度下降原理过程演示</p><p><strong>在Python中实现一个自适应的线性神经元</strong></p><p>先贴上定义的python函数，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"># 定义神经元函数</span><br><span class="line">class AdalineGD(object):</span><br><span class="line">    &quot;&quot;&quot;ADAptive LInear NEuron classifier.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ------------</span><br><span class="line">    eta : float</span><br><span class="line">        Learning rate (between 0.0 and 1.0)</span><br><span class="line">    n_iter : int</span><br><span class="line">        Passes over the training dataset.</span><br><span class="line"></span><br><span class="line">    Attributes</span><br><span class="line">    -----------</span><br><span class="line">    w_ : 1d-array</span><br><span class="line">        Weights after fitting.</span><br><span class="line">    cost_ : list</span><br><span class="line">        Sum-of-squares cost function value in each epoch.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, eta=0.01, n_iter=50):</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y):</span><br><span class="line">        &quot;&quot;&quot; Fit training data.</span><br><span class="line"></span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span><br><span class="line">            Training vectors, where n_samples is the number of samples and</span><br><span class="line">            n_features is the number of features.</span><br><span class="line">        y : array-like, shape = [n_samples]</span><br><span class="line">            Target values.</span><br><span class="line"></span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        self : object</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.w_ = np.zeros(1 + X.shape[1])</span><br><span class="line">        self.cost_ = []</span><br><span class="line"></span><br><span class="line">        for i in range(self.n_iter):</span><br><span class="line">            net_input = self.net_input(X)</span><br><span class="line">            # Please note that the &quot;activation&quot; method has no effect</span><br><span class="line">            # in the code since it is simply an identity function. We</span><br><span class="line">            # could write `output = self.net_input(X)` directly instead.</span><br><span class="line">            # The purpose of the activation is more conceptual, i.e.,  </span><br><span class="line">            # in the case of logistic regression, we could change it to</span><br><span class="line">            # a sigmoid function to implement a logistic regression classifier.</span><br><span class="line">            output = self.activation(X)</span><br><span class="line">            errors = (y - output)</span><br><span class="line">            self.w_[1:] += self.eta * X.T.dot(errors)</span><br><span class="line">            self.w_[0] += self.eta * errors.sum()</span><br><span class="line">            cost = (errors**2).sum() / 2.0</span><br><span class="line">            self.cost_.append(cost)</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def net_input(self, X):</span><br><span class="line">        &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span><br><span class="line">        return np.dot(X, self.w_[1:]) + self.w_[0]</span><br><span class="line"></span><br><span class="line">    def activation(self, X):</span><br><span class="line">        &quot;&quot;&quot;Compute linear activation&quot;&quot;&quot;</span><br><span class="line">        return self.net_input(X)</span><br><span class="line"></span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span><br><span class="line">        return np.where(self.activation(X) &gt;= 0.0, 1, -1)</span><br></pre></td></tr></table></figure></p><p><strong>查看不同学习率下的错误率随迭代次数的变化情况：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))</span><br><span class="line"></span><br><span class="line"># 可视化W调整的过程中，错误率随迭代次数的变化</span><br><span class="line">ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)</span><br><span class="line">ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker=&apos;o&apos;)</span><br><span class="line">ax[0].set_xlabel(&apos;Epochs&apos;)</span><br><span class="line">ax[0].set_ylabel(&apos;log(Sum-squared-error)&apos;)</span><br><span class="line">ax[0].set_title(&apos;Adaline - Learning rate 0.01&apos;)</span><br><span class="line"></span><br><span class="line">ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)</span><br><span class="line">ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker=&apos;o&apos;)</span><br><span class="line">ax[1].set_xlabel(&apos;Epochs&apos;)</span><br><span class="line">ax[1].set_ylabel(&apos;Sum-squared-error&apos;)</span><br><span class="line">ax[1].set_title(&apos;Adaline - Learning rate 0.0001&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/g6mKKU3.png" alt=""></p><p><strong>iris数据的应用情况：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 标准化特征</span><br><span class="line">X_std = np.copy(X)</span><br><span class="line">X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()</span><br><span class="line">X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()</span><br><span class="line"># 调用函数开始训练</span><br><span class="line">ada = AdalineGD(n_iter=15, eta=0.01)</span><br><span class="line">ada.fit(X_std, y)</span><br><span class="line"># 绘制效果</span><br><span class="line">plot_decision_regions(X_std, y, classifier=ada)</span><br><span class="line">plt.title(&apos;Adaline - Gradient Descent&apos;)</span><br><span class="line">plt.xlabel(&apos;sepal length [standardized]&apos;)</span><br><span class="line">plt.ylabel(&apos;petal length [standardized]&apos;)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"># 可视化W调整的过程中，错误率随迭代次数的变化</span><br><span class="line">plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=&apos;o&apos;)</span><br><span class="line">plt.xlabel(&apos;Epochs&apos;)</span><br><span class="line">plt.ylabel(&apos;Sum-squared-error&apos;)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/kRorVXJ.png" alt=""></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>1）<a href="https://blog.csdn.net/u013719780/article/details/51755409" target="_blank" rel="noopener">机器学习系列：感知器</a><br>2）<a href="https://blog.csdn.net/zyq522376829/article/details/66632699" target="_blank" rel="noopener">机器学习入门系列04，Gradient Descent（梯度下降法）</a><br>3）<a href="https://zhuanlan.zhihu.com/p/27449596?utm_source=weibo&amp;utm_medium=social" target="_blank" rel="noopener">一文看懂各种神经网络优化算法：从梯度下降到Adam方法</a><br>4）<a href="https://blog.csdn.net/huakai16/article/details/77701020" target="_blank" rel="noopener">机器学习与神经网络（三）：自适应线性神经元的介绍和Python代码实现</a><br>5）<a href="http://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb" target="_blank" rel="noopener">《Training Machine Learning Algorithms for Classification》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/62602/&quot;&gt;机器学习(三) 模型结果应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/21484/&quot;&gt;机器学习(四) 常见算法优缺点&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文章结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;什么是感知器分类算法&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;在Python中实现感知器学习算法&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;在iris（鸢尾花）数据集上训练一个感知器模型&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;自适应线性神经元和融合学习&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;使用梯度下降方法来最小化损失函数&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;在Python中实现一个自适应的线性神经元&lt;/em&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="算法" scheme="https://paradoxallen.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(4) 常见算法优缺点</title>
    <link href="https://paradoxallen.github.io/21484/"/>
    <id>https://paradoxallen.github.io/21484/</id>
    <published>2017-06-04T16:00:00.000Z</published>
    <updated>2018-06-13T05:09:08.547Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p><a href="https://paradoxallen.github.io/62602/">机器学习(三) 模型结果应用</a></p><p>机器学习算法我们了解了很多，但是放在一起来比较优缺点是缺少的，本篇文章就一些常见的算法来进行一次优缺点梳理。</p><a id="more"></a><hr><h3 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a><strong>决策树算法</strong></h3><h4 id="一、决策树优点"><a href="#一、决策树优点" class="headerlink" title="一、决策树优点"></a><strong>一、决策树优点</strong></h4><p>1、决策树易于理解和解释，可以可视化分析，容易提取出规则。</p><p>2、可以同时处理标称型和数值型数据。</p><p>3、测试数据集时，运行速度比较快。</p><p>4、决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。</p><h4 id="二、决策树缺点"><a href="#二、决策树缺点" class="headerlink" title="二、决策树缺点"></a><strong>二、决策树缺点</strong></h4><p>1、对缺失数据处理比较困难。</p><p>2、容易出现过拟合问题。</p><p>3、忽略数据集中属性的相互关联。</p><p>4、ID3算法计算信息增益时结果偏向数值比较多的特征。</p><h4 id="三、改进措施"><a href="#三、改进措施" class="headerlink" title="三、改进措施"></a><strong>三、改进措施</strong></h4><p>1、对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。</p><p>2、使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题</p><h4 id="四、常见算法"><a href="#四、常见算法" class="headerlink" title="四、常见算法"></a><strong>四、常见算法</strong></h4><h5 id="一）C4-5算法"><a href="#一）C4-5算法" class="headerlink" title="一）C4.5算法"></a><strong>一）C4.5算法</strong></h5><p>ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定的测试属性。</p><p>C4.5算法核心思想是ID3算法，是ID3算法的改进，改进方面有：</p><ul><li><p>用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；</p></li><li><p>在树构造过程中进行剪枝；</p></li><li><p>能处理非离散的数据；</p></li><li><p>能处理不完整的数据。</p></li></ul><p><strong>优点</strong>：产生的分类规则易于理解，准确率较高。</p><p><strong>缺点</strong>：</p><p>1）在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效；</p><p>2）C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</p><h5 id="二）CART分类与回归树"><a href="#二）CART分类与回归树" class="headerlink" title="二）CART分类与回归树"></a><strong>二）CART分类与回归树</strong></h5><p>是一种决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数<br>据集生成的决策树的拓展形。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。</p><p><strong>优点</strong></p><p>1）非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。</p><p>2）在面对诸如存在缺失值、变量数多等问题时CART 显得非常稳健。</p><hr><h3 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a><strong>分类算法</strong></h3><h4 id="一、KNN算法"><a href="#一、KNN算法" class="headerlink" title="一、KNN算法"></a><strong>一、KNN算法</strong></h4><p><strong>KNN算法的优点</strong> </p><p>1、KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练</p><p>2、KNN理论简单，容易实现</p><p><strong>KNN算法的缺点</strong></p><p>1、对于样本容量大的数据集计算量比较大。</p><p>2、样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。</p><p>3、KNN每一次分类都会重新进行一次全局运算。</p><p>4、k值大小的选择。</p><p><strong>KNN算法应用领域</strong></p><p>文本分类、模式识别、聚类分析，多分类领域</p><h4 id="二、支持向量机（SVM）"><a href="#二、支持向量机（SVM）" class="headerlink" title="二、支持向量机（SVM）"></a><strong>二、支持向量机（SVM）</strong></h4><p>支持向量机是一种基于分类边界的方法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。基于分类边界的分类算法的目标是，通过训练，找到这些分类之间的边界（直线的――称为线性划分，曲线的――称为非线性划分）。对于多维数据（如N维），可以将它们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面（超面比N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。</p><p>支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。</p><p><strong>SVM优点</strong></p><p>1、解决小样本下机器学习问题。<br>2、解决非线性问题。<br>3、无局部极小值问题。（相对于神经网络等算法）<br>4、可以很好的处理高维数据集。<br>5、泛化能力比较强。</p><p><strong>SVM缺点</strong></p><p>1、对于核函数的高维映射解释力不强，尤其是径向基函数。<br>2、对缺失数据敏感。</p><p><strong>SVM应用领域</strong></p><p>文本分类、图像识别、主要二分类领域</p><h4 id="三、朴素贝叶斯算法"><a href="#三、朴素贝叶斯算法" class="headerlink" title="三、朴素贝叶斯算法"></a><strong>三、朴素贝叶斯算法</strong></h4><p><strong>朴素贝叶斯算法优点</strong></p><p>1、对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。<br>2、支持增量式运算。即可以实时的对新增的样本进行训练。<br>3、朴素贝叶斯对结果解释容易理解。</p><p><strong>朴素贝叶斯缺点</strong></p><p>1、由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。</p><p><strong>朴素贝叶斯应用领域</strong></p><p>文本分类、欺诈检测中使用较多</p><h4 id="四、Logistic回归算法"><a href="#四、Logistic回归算法" class="headerlink" title="四、Logistic回归算法"></a><strong>四、Logistic回归算法</strong></h4><p><strong>logistic回归优点</strong></p><p>1、计算代价不高，易于理解和实现</p><p><strong>logistic回归缺点</strong></p><p>1、容易产生欠拟合。</p><p>2、分类精度不高。</p><p><strong>logistic回归应用领域</strong></p><p>用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。</p><p>Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。</p><hr><h3 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a><strong>聚类算法</strong></h3><h4 id="一、K-means-算法"><a href="#一、K-means-算法" class="headerlink" title="一、K means 算法"></a><strong>一、K means 算法</strong></h4><p>是一个简单的聚类算法，把n的对象根据他们的属性分为k个分割，k&lt; n。 算法的核心就是要优化失真函数J,使其收敛到局部最小值但不是全局最小值。<br>其中N为样本数，K是簇数，rnk b表示n属于第k个簇，uk 是第k个中心点的值。然后求出最优的uk</p><p><strong>优点</strong>：算法速度很快</p><p><strong>缺点</strong>：分组的数目k是一个输入参数，不合适的k可能返回较差的结果。</p><h4 id="二、EM最大期望算法"><a href="#二、EM最大期望算法" class="headerlink" title="二、EM最大期望算法"></a><strong>二、EM最大期望算法</strong></h4><p>EM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。</p><p>EM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。</p><hr><h3 id="集成算法（AdaBoost算法）"><a href="#集成算法（AdaBoost算法）" class="headerlink" title="集成算法（AdaBoost算法）"></a><strong>集成算法（AdaBoost算法）</strong></h3><h4 id="一、-AdaBoost算法优点"><a href="#一、-AdaBoost算法优点" class="headerlink" title="一、  AdaBoost算法优点"></a><strong>一、  AdaBoost算法优点</strong></h4><p>1、很好的利用了弱分类器进行级联。</p><p>2、可以将不同的分类算法作为弱分类器。</p><p>3、AdaBoost具有很高的精度。</p><p>4、相对于bagging算法和Random Forest算法，AdaBoost充分考虑的每个分类器的权重。</p><h4 id="二、Adaboost算法缺点"><a href="#二、Adaboost算法缺点" class="headerlink" title="二、Adaboost算法缺点"></a><strong>二、Adaboost算法缺点</strong></h4><p>1、AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。</p><p>2、数据不平衡导致分类精度下降。</p><p>3、训练比较耗时，每次重新选择当前分类器最好切分点。</p><h4 id="三、AdaBoost应用领域"><a href="#三、AdaBoost应用领域" class="headerlink" title="三、AdaBoost应用领域"></a><strong>三、AdaBoost应用领域</strong></h4><p>模式识别、计算机视觉领域，用于二分类和多分类场景</p><hr><h3 id="人工神经网络算法"><a href="#人工神经网络算法" class="headerlink" title="人工神经网络算法"></a><strong>人工神经网络算法</strong></h3><h4 id="一、神经网络优点"><a href="#一、神经网络优点" class="headerlink" title="一、神经网络优点"></a><strong>一、神经网络优点</strong></h4><p>1、分类准确度高，学习能力极强。</p><p>2、对噪声数据鲁棒性和容错性较强。</p><p>3、有联想能力，能逼近任意非线性关系。</p><h4 id="二、神经网络缺点"><a href="#二、神经网络缺点" class="headerlink" title="二、神经网络缺点"></a><strong>二、神经网络缺点</strong></h4><p>1、神经网络参数较多，权值和阈值。</p><p>2、黑盒过程，不能观察中间结果。</p><p>3、学习过程比较长，有可能陷入局部极小值。</p><h4 id="三、人工神经网络应用领域"><a href="#三、人工神经网络应用领域" class="headerlink" title="三、人工神经网络应用领域"></a><strong>三、人工神经网络应用领域</strong></h4><p>目前深度神经网络已经应用与计算机视觉，自然语言处理，语音识别等领域并取得很好的效果。</p><hr><h3 id="排序算法（PageRank）"><a href="#排序算法（PageRank）" class="headerlink" title="排序算法（PageRank）"></a><strong>排序算法（PageRank）</strong></h3><p>PageRank是google的页面排序算法，是基于从许多优质的网页链接过来的网页，必定还是优质网页的回归关系，来判定所有网页的重要性。（也就是说，一个人有着越多牛X朋友的人，他是牛X的概率就越大。）</p><h4 id="一、PageRank优点"><a href="#一、PageRank优点" class="headerlink" title="一、PageRank优点"></a><strong>一、PageRank优点</strong></h4><p>完全独立于查询，只依赖于网页链接结构，可以离线计算。</p><h4 id="二、PageRank缺点"><a href="#二、PageRank缺点" class="headerlink" title="二、PageRank缺点"></a><strong>二、PageRank缺点</strong></h4><p>1）PageRank算法忽略了网页搜索的时效性。</p><p>2）旧网页排序很高，存在时间长，积累了大量的in-links，拥有最新资讯的新网页排名却很低，因为它们几乎没有in-links。</p><hr><h3 id="关联规则算法（Apriori算法）"><a href="#关联规则算法（Apriori算法）" class="headerlink" title="关联规则算法（Apriori算法）"></a><strong>关联规则算法（Apriori算法）</strong></h3><p>Apriori算法是一种挖掘关联规则的算法，用于挖掘其内含的、未知的却又实际存在的数据关系，其核心是基于两阶段频集思想的递推算法 。</p><p><strong>Apriori算法分为两个阶段：</strong></p><p>1）寻找频繁项集</p><p>2）由频繁项集找关联规则</p><p><strong>算法缺点：</strong></p><p>1）在每一步产生侯选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；</p><p>2） 每次计算项集的支持度时，都对数据库中    的全部记录进行了一遍扫描比较，需要很大的I/O负载。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1）Jason Brownlee  《How To Use Machine Learning Results》</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/62602/&quot;&gt;机器学习(三) 模型结果应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;机器学习算法我们了解了很多，但是放在一起来比较优缺点是缺少的，本篇文章就一些常见的算法来进行一次优缺点梳理。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="算法" scheme="https://paradoxallen.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(3) 模型结果应用</title>
    <link href="https://paradoxallen.github.io/62602/"/>
    <id>https://paradoxallen.github.io/62602/</id>
    <published>2017-05-31T16:00:00.000Z</published>
    <updated>2018-06-13T05:08:51.908Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p>当你有了一个相当不错的模型结果了，这个时间就需要上线应用了，但实际上这个过程也是需要注意很多东西的呢，比如汇报你的项目结果、上线计划沟通、上线后的监控等等，这都是相当重要的。</p><a id="more"></a><hr><p>永远要记得，建立模型只是为了解决业务问题，<strong>模型只是一个工具而已</strong>，所以，脱离具体业务场景的模型都是假的。所以，在一开始，就要对自己的目标进行明确，在做完了模型后再度审视自己目标，看下自己做出来的模型是否仍是解决这个目标。</p><p>根据你试图解决的问题类型，我们可以大致分为两种呈现方式：</p><ul><li><p><strong>报告汇报式</strong></p></li><li><p><strong>部署上线式</strong></p></li></ul><p>当然了，实际上更多的是两种方式的融合，即两个都需要做，那么下面我们就分别来说一下这两种方式在实际操作上都需要做些什么呗。</p><h2 id="报告汇报式"><a href="#报告汇报式" class="headerlink" title="报告汇报式"></a>报告汇报式</h2><p>一旦你发现了一个很不错的模型，并且训练的结果也很不错，你此时就需要总结这一切内容，并很好的展示给你的观众（可以是老板、客户或者是同事等），而此时如何完美地展示显得格外重要。</p><p>展示的最好方式我个人觉得是ppt，但有些地方更偏好于单页报告，不过也不影响，下面罗列的内容，都是可以在这两种方式内容上展示的，不管你现在做的模型是什么，比赛的、教程的，还是工作的，都可以试着去总结这些关键点，完成一次报告的撰写。</p><ul><li><p><strong><em>Context</em></strong> (Why): 定义问题存在的大背景，并且说明研究的动机或目的。</p></li><li><p><strong><em>Problem</em></strong> (Question): 简单扼要地把问题描述一个具体需要解决的问题并回答它。</p></li><li><p><strong><em>Solution</em></strong> (Answer): 简单扼要地描述关于上一个环节提出的问题的解决方案，而且要详细具体。</p></li><li><p><strong><em>Findings</em></strong>: 罗列一下你在建模过程中发现的一些有价值的点，比如在数据上的发现，又或者是原先方式的缺点及现有方式的优点，也可以是模型在性能方面的优势等等。</p></li><li><p><strong><em>Limitations</em></strong>: 考虑模型能力所不能覆盖的点，或者是方案不能解决的点。不要回避这些问题，你不说别人也会问，而且，只有你重新认识模型的短处，才能知道模型的优点。</p></li><li><p><strong><em>Conclusions</em></strong> (Why+Question+Answer): 回顾目的、研究的问题及解决方案，并将它们尽可能压缩在几句话，让人能够记住。</p></li></ul><p>如果是自己平时做练习的项目，我觉得可以多按照上面的点来描述自己的项目结果，并且将报告上传到社区网络，让更多的人来评价，你从中也可以得到更多的反馈，这对你下一次的报告有很大的帮助。</p><h2 id="部署上线式"><a href="#部署上线式" class="headerlink" title="部署上线式"></a>部署上线式</h2><p>同样的，你有一个训练得很不错的模型，这时候需要将它部署到生产系统中，你需要确定很多东西，比如调用的环节、入参出参以及各种接口开发，下面有3个方面的内容需要在做这些事情之前进行考虑，分别是：<strong>算法实现、模型自动化测试、模型效果追踪</strong>。</p><p><strong>1）算法实现</strong></p><p>其实python里有很多算法都是可以直接通过库来调用的，但这对于一般情况下是很好用的，但是如果涉及到要具体部署应用，这要考虑的东西就多了。</p><p>在你考虑部署一个新模型在现有的生产系统上，你需要非常仔细地研究这可能需要产生的依赖项和“技术负债”（这里可以理解为一些所需的技术，包括硬软件）。所以，在建模前，需要考虑去查找能匹配你的方法的公司生产级别的库，要不然，等到要上线的时候，你就需要重复模型调优的过程了哦。</p><p><strong>2）模型自动化测试</strong></p><p>编写自动化测试代码，对模型的应用进行验证，监控在实际的使用过程中，并且能够重复实现模型效果的最低水平，尽可能是可以对不同的数据都可以随机性地测试。</p><p><strong>3）模型效果追踪</strong></p><p>增添一些基础设施来监控模型的性能，并且可以在精度低于最低水平的时候发出警报，追踪模型实时或者离线的数据样本的效果，包括入参。当你发现不仅仅是模型效果发生了很大的变化，就连入参也有很大的变化，那这个时候就需要考虑模型的更新或者重构了。</p><p>当然，有一些模型是可以实现在线自我学习并且更新自己的，但并不是所有的生产系统可以支持这种操作，毕竟这种还只是一个比较先进的办法，仍存在很多不太完善的地方。比较传统的方式还是对现有的模型进行人工管理，人工更新与切换，这样子显得更加明智而且稳健。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1）Jason Brownlee  《How To Use Machine Learning Results》</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当你有了一个相当不错的模型结果了，这个时间就需要上线应用了，但实际上这个过程也是需要注意很多东西的呢，比如汇报你的项目结果、上线计划沟通、上线后的监控等等，这都是相当重要的。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="模型" scheme="https://paradoxallen.github.io/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(2) 模型调优</title>
    <link href="https://paradoxallen.github.io/4840/"/>
    <id>https://paradoxallen.github.io/4840/</id>
    <published>2017-05-24T16:00:00.000Z</published>
    <updated>2018-06-13T05:08:37.892Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一)算法介绍</a></p><p>前面讲了一些机器学习的算法的介绍，如果有一些数据这时候也可以建立出自己的模型了，但是，如果模型的效果不尽人意，那么应该如何调整呢？</p><p>以下是一份关于模型调优的方法，每当出现效果不好的时候或者是在建模前，都可以按照这个来进行检查，话不多说，一起来看～</p><p><img src="https://i.imgur.com/MHeHLhM.jpg" alt=""></p><a id="more"></a><hr><p>为了文章阅读的清晰，先在前面简单说明一下文章的目录框架。</p><p>本文存在的意义在于辅助大家提升机器学习模型的效果，方法有很多，如果你在其中的一个办法中找到了突破，仍可以回头再找其他，直到满足你的模型效果要求，主要从4个角度来进行方法的阐述，分别是：</p><ul><li>Improve Performance With Data.（数据）</li><li>Improve Performance With Algorithms.（算法选择）</li><li>Improve Performance With Algorithm Tuning.（算法调优）</li><li>Improve Performance With Ensembles.（效果集成）</li></ul><p>好的，下面就分别从这4个角度来说一下。</p><h3 id="1-Improve-Performance-With-Data（数据）"><a href="#1-Improve-Performance-With-Data（数据）" class="headerlink" title="1. Improve Performance With Data（数据）"></a>1. Improve Performance With Data（数据）</h3><p>事实上，你直接改变训练数据或者改变目标的定义，好效果会来得更加“不费吹灰之力”，有的时候还可能是最好的操作，所以有一句话说得很有道理：<strong>选择比努力重要哈哈哈</strong>。</p><p>话不多说，说下Strategy: 创建不同的目标样本并且尽量使用最底层的特征来训练模型算法。</p><p><strong>具体策略：</strong></p><p>获得更多的数据：一个好的深度学习模型需要更多的数据来训练，其他非线性的机器学习模型也是如此。</p><ul><li>开发更多变量：如果你实在不能获得更多的数据亦或是更好质量的数据，也许可以通过概率模型、统计模型来生成新的变量。</li><li>清洗数据：通过数据清洗，你可以对缺失值、异常值进行合理的填补与修复，从而提升数据整体的质量。</li><li>重新采样数据：其实可以通过对数据的重新采样来改变数据的分布和大小。对某一特定类型的样本进行采样，说不定可以更好滴表现出效果。又或者是使用更小的数据，从而加快速度。</li><li>问题解决思路的重新思考：有的时候，你可以把你目前正在“焦头烂耳”想要解决的“预测性”问题，换成回归、分类、时间序列、异常检测、排序、推荐等等的问题。</li><li>调整变量再入模：这里指的是对数据进行离散化、标准化等的操作。</li><li>改变现有的变量：这里相信大家也很常见，就是对变量进行对数转换或指数转换，让其特性能更好地表现。</li><li>对变量进行降维：有的时候降维后的变量有更好的表现哦。</li><li>特征选择：这个就是特征工程了，简单来说，就是你对特征（变量）进行重要性的排序，选择相对预测力强的特征进入模型。</li></ul><h3 id="2-Improve-Performance-With-Algorithms（算法选择）"><a href="#2-Improve-Performance-With-Algorithms（算法选择）" class="headerlink" title="2. Improve Performance With Algorithms（算法选择）"></a>2. Improve Performance With Algorithms（算法选择）</h3><p>机器学习其实都是关于算法的学习。</p><p>Strategy: 识别出优于平均值的算法，但要对其实验过程以及结果抱着怀疑态度，并反复思考。</p><p><strong>具体策略：</strong></p><ul><li>重采样方法：使用什么方法来估计效果？有个原则就是要充分利用可用的数据来验证，这里，k-fold交叉验证方法可以是最好的哦。</li><li>评价指标：不同的目标需要使用不同的评价指标，这个相信大家在学习混淆矩阵的时候应该有所了解，什么pv+，命中率等等，都是对于特定类型的目标有着非常有效的识别。如果是一个排序性的问题，而你却用了准确度的指标来衡量模型的好坏似乎也说不过去把？</li><li>关注线性算法：线性算法通常会不那么好用，但是却更好地被人类理解且可以快速测试，如果你发现某个线性算法表现地还行，请继续优化它。</li><li>关注非线性算法：非线性算法往往会需要更多的数据，通过更加复杂的计算来获得一个不错的效果。</li><li>从文献中找ideas：这个方法还经常做，从文献中可以了解到更多的经典算法在特定需要下的应用，通过对文献的阅读来扩充你的“解题”思路把。</li></ul><h3 id="3-Improve-Performance-With-Algorithm-Tuning（算法调优）"><a href="#3-Improve-Performance-With-Algorithm-Tuning（算法调优）" class="headerlink" title="3. Improve Performance With Algorithm Tuning（算法调优）"></a>3. Improve Performance With Algorithm Tuning（算法调优）</h3><p>模型调参也是一个非常费时间的环节，有的时候“好运”可以马上抽查出表现还不错的结果，并持续调参，就可以得到一个不错的结果。但如果要对其他所有的算法进行优化，那么需要的时间就可能是几天、几个星期或者几个月了。</p><p>Strategy: 充分利用性能良好的机器学习算法。</p><p><strong>具体策略：</strong></p><ul><li>诊断方法：不同的算法需要提供不同的可视化和诊断的方法。</li><li>调参的直觉：这个就很“玄学”了，但其实都是一些经验，当你调的参足够多，也可以大致可以对这些不同算法的参数有了自己的理解，自然就有了这些所谓的“直觉”。</li><li>随机搜索：在N维参数空间按某种分布（如正态分布）随机取值，因为参数空间的各个维度的重要性是不等的，随机搜索方法可以在不重要的维度上取巧。</li><li>网格搜索：先固定一个超参，然后对其他各个超参依次进行穷举搜索。</li><li>从文献中找ideas：从文献中了解这个算法用到了哪些算法，并且这些算法主要的取值值域，有益于自身工作的开展哦。</li><li>从知名网站中找ideas：国内我个人觉得知乎还是蛮可以的，关于这节的参数调参，也是有好多好文章，其外还有csdn也不错。</li></ul><h3 id="4-Improve-Performance-With-Ensembles（效果集成）"><a href="#4-Improve-Performance-With-Ensembles（效果集成）" class="headerlink" title="4. Improve Performance With Ensembles（效果集成）"></a>4. Improve Performance With Ensembles（效果集成）<code></code></h3><p>这个算法集成的方法也是非常常用的，你可以结合多个模型的结果，综合输出一个更加稳定且效果不错的结果。</p><p>Strategy: 结合各种模型的预测结果并输出。</p><p><strong>具体策略：</strong></p><ul><li>混合模型的预测值：你可以把多个模型的预测结果结合起来，你可以将多个训练效果还不错的模型的预测结合综合起来，输出一个“平均”结果。</li><li>混合不同数据的预测值：你也可以把不同的数据集训练出来模型的结果进行结合，作为一个输出。（这个与上面的区别在于数据集的特征不同）</li><li>混合数据样本：很拗口，其实意思就是将数据集拆分成不同的子数据集，用于训练同一个算法，最后输出综合的预测结果。这个也被称之为bootstrap aggregation 或 bagging。</li><li>使用模型的方法集成：你也可以使用一个新的模型来学习如何结合多个性能不错的模型结果，输出一个最优的结合。这被称之为堆叠泛化或叠加，通常在子模型有技巧时很有效，但在不同的方式下，聚合器模型是预测的一个简单的线性加权。这个过程可以重复多层深度。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1）<a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" target="_blank" rel="noopener">A Tour of Machine Learning Algorithms</a></p><p>2）<a href="https://www.zhihu.com/question/34470160?sort=created" target="_blank" rel="noopener">机器学习各种算法怎么调参?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一)算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前面讲了一些机器学习的算法的介绍，如果有一些数据这时候也可以建立出自己的模型了，但是，如果模型的效果不尽人意，那么应该如何调整呢？&lt;/p&gt;
&lt;p&gt;以下是一份关于模型调优的方法，每当出现效果不好的时候或者是在建模前，都可以按照这个来进行检查，话不多说，一起来看～&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/MHeHLhM.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="模型" scheme="https://paradoxallen.github.io/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>A Tour of Machine Learning Algorithms(1) 算法介绍</title>
    <link href="https://paradoxallen.github.io/40392/"/>
    <id>https://paradoxallen.github.io/40392/</id>
    <published>2017-05-21T16:00:00.000Z</published>
    <updated>2018-06-13T05:03:57.846Z</updated>
    
    <content type="html"><![CDATA[<p>接下来的文章基于来自Jason Brownlee 的文章《A Tour of Machine Learning Algorithms》</p><a id="more"></a><hr><p>算法很多，如何做好分组有助于我们更好记住它们，主要有2条算法分组的方式：</p><ul><li><p>The first is a grouping of algorithms by the learning style.（通过算法的学习方式）</p></li><li><p>The second is a grouping of algorithms by similarity in form or function (like grouping similar animals together).（通过算法的功能）</p></li></ul><p>下面就会从这2个角度来阐述一下机器学习的算法。</p><p><img src="https://i.imgur.com/yQR3iN4.png" alt=""></p><h2 id="Algorithms-Grouped-by-Learning-Style（通过算法的学习方式）"><a href="#Algorithms-Grouped-by-Learning-Style（通过算法的学习方式）" class="headerlink" title="Algorithms Grouped by Learning Style（通过算法的学习方式）"></a>Algorithms Grouped by Learning Style（通过算法的学习方式）</h2><p>关于机器学习算法，有三种不同的学习方式：</p><h3 id="1-Supervised-Learning（监督学习）"><a href="#1-Supervised-Learning（监督学习）" class="headerlink" title="1. Supervised Learning（监督学习）"></a>1. Supervised Learning（监督学习）</h3><p>当输入的数据集（我们称之为训练集）的数据有标签，如好坏标签，分类标签等，那么通过这些数据来建立的预测或者分类模型，属于监督学习模型。</p><ul><li><p>经典问题：classification and regression.（分类与回归）</p></li><li><p>经典算法：Logistic Regression and the Back Propagation Neural Network.（逻辑回归算法与BP神经网络算法）</p></li></ul><p><img src="https://i.imgur.com/pBeHrAV.png" alt=""></p><h3 id="2-Unsupervised-Learning（无监督学习）"><a href="#2-Unsupervised-Learning（无监督学习）" class="headerlink" title="2. Unsupervised Learning（无监督学习）"></a>2. Unsupervised Learning（无监督学习）</h3><p>与监督学习相反，训练集中的数据并没有标签，这意味着你需要从这堆没有标签的数据中去提炼它们的特点规则等等，可能是通过数学推理过程来系统地减少冗余，又或者是通过数据相似度来组织数据。</p><ul><li><p>经典问题：clustering, dimensionality reduction and association rule learning.（聚类、降维、规则学习）</p></li><li><p>经典算法：the Apriori algorithm and k-Means.（这个专用名词就不翻译了）</p></li></ul><p><img src="https://i.imgur.com/uXgOV5r.png" alt=""></p><h3 id="3-Semi-Supervised-Learning（半监督学习）"><a href="#3-Semi-Supervised-Learning（半监督学习）" class="headerlink" title="3. Semi-Supervised Learning（半监督学习）"></a>3. Semi-Supervised Learning（半监督学习）</h3><p>顾名思义，半监督学习意味着训练数据有一部分有标签，而一些没有，一般而言，当训练数据量过少时，监督学习得到的模型效果不能满足需求，因此用半监督学习来增强效果。</p><ul><li><p>经典问题：classification and regression.</p></li><li><p>经典算法：半监督SVM，高斯模型，KNN模型</p></li></ul><p><img src="https://i.imgur.com/FGSCTCb.png" alt=""></p><h2 id="Algorithms-Grouped-By-Similarity（通过算法的功能）"><a href="#Algorithms-Grouped-By-Similarity（通过算法的功能）" class="headerlink" title="Algorithms Grouped By Similarity（通过算法的功能）"></a>Algorithms Grouped By Similarity（通过算法的功能）</h2><p>根据算法的功能相似性来区分算法也是一种很好的办法，如基于树结构的算法或者基于神经网络的算法。所以我觉得从这个角度来了解这些算法会更加好。<br>即便这是一个很好的方式，但也绝非完美，仍会有一些算法不能简单地被归类，比如Learning Vector Quantization（LVQ，学习矢量量化算法），它既是神经网络，也是基于距离的算法，所以下面的归类也只是适用于大多数算法，但是常用的算法。</p><h3 id="1-Regression-Algorithms（回归算法）"><a href="#1-Regression-Algorithms（回归算法）" class="headerlink" title="1. Regression Algorithms（回归算法）"></a>1. Regression Algorithms（回归算法）</h3><p>回归更多地关注自变量与因变量之间的关系，并通过对误差的测算来建模，回归算法是对于数学统计的一个很好应用，也被纳入统计机器学习中。</p><p>常见的回归算法包括：</p><ul><li><p>Ordinary Least Squares Regression (OLSR，普通最小二乘回归)</p></li><li><p>Linear Regression（线性回归）</p></li><li><p>Logistic Regression（逻辑回归）</p></li><li><p>Stepwise Regression（逐步回归）</p></li><li><p>Adaptive Regression Splines (MARS，多元自适应回归)</p></li><li><p>Locally Estimated Scatterplot Smoothing (LOESS，本地散点平滑估计)</p></li></ul><p><img src="https://i.imgur.com/68VmdwF.png" alt=""></p><h3 id="2-Instance-based-Algorithms（基于距离的算法）"><a href="#2-Instance-based-Algorithms（基于距离的算法）" class="headerlink" title="2. Instance-based Algorithms（基于距离的算法）"></a>2. Instance-based Algorithms（基于距离的算法）</h3><p>基于距离学习的模型非常常见，这类的模型是对训练集数据进行建模并比较新数据与之的距离，而距离的衡量有很多，常见的是欧氏距离、曼哈顿距离等。</p><p>常见的算法包括：</p><ul><li><p>k-Nearest Neighbor (kNN)</p></li><li><p>Learning Vector Quantization (LVQ，学习矢量量化)</p></li><li><p>Self-Organizing Map (SOM，自组织映射)</p></li><li><p>Locally Weighted Learning (LWL，局部加权学习)</p></li></ul><p><img src="https://i.imgur.com/0L3V9S0.png" alt=""></p><h3 id="3-Regularization-Algorithms（正则化算法）"><a href="#3-Regularization-Algorithms（正则化算法）" class="headerlink" title="3. Regularization Algorithms（正则化算法）"></a>3. Regularization Algorithms（正则化算法）</h3><p>正则化是对另一种方法(通常是回归方法)的扩展，使基于其复杂性的模型受到惩罚，支持更简单的模型，这些模型在泛化能力方面也比较好。</p><p>常见的正则化算法包括：<br>Ridge Regression（岭回归算法）<br>Least Absolute Shrinkage and Selection Operator (LASSO算法，稀疏约束)<br>Elastic Net（弹性网络）<br>Least-Angle Regression (LARS，最小角回归算法)</p><p><img src="https://i.imgur.com/zAMpcRi.png" alt=""></p><h3 id="4-Decision-Tree-Algorithms（决策树算法）"><a href="#4-Decision-Tree-Algorithms（决策树算法）" class="headerlink" title="4. Decision Tree Algorithms（决策树算法）"></a>4. Decision Tree Algorithms（决策树算法）</h3><p>决策树方法构建基于数据中属性的实际值来建模的，决策树经常被训练用于分类和回归问题，决策树通常是快速和准确的，并且是机器学习中最受欢迎的。</p><p>常见的决策树算法包括：</p><ul><li><p>Classification and Regression Tree (CART，分类回归树算法)</p></li><li><p>Iterative Dichotomiser 3 (ID3)</p></li><li><p>C4.5 and C5.0 (不同版本的区别)</p></li><li><p>Chi-squared Automatic Interaction Detection (CHAID)</p></li><li><p>Decision Stump（决策树桩）</p></li><li><p>MD5（Message-Digest Algorithm，讯息摘要算法）</p></li><li><p>Decision Trees（条件决策树）</p></li></ul><p><img src="https://i.imgur.com/gYqnyoA.png" alt=""></p><h3 id="5-Bayesian-Algorithms（贝叶斯算法）"><a href="#5-Bayesian-Algorithms（贝叶斯算法）" class="headerlink" title="5. Bayesian Algorithms（贝叶斯算法）"></a>5. Bayesian Algorithms（贝叶斯算法）</h3><p>基于贝叶斯定理的方式来构建的算法，常用语分类与回归问题。</p><p>常见的贝叶斯算法包括：</p><ul><li><p>Naive Bayes（朴素贝叶斯）</p></li><li><p>Gaussian Naive Bayes（高斯朴素贝叶斯）</p></li><li><p>Multinomial Naive Bayes（多项式朴素贝叶斯）</p></li><li><p>Averaged One-Dependence Estimators (AODE)</p></li><li><p>Belief Network (BBN，贝叶斯定理网络)</p></li><li><p>Bayesian Network (BN，贝叶斯网络)</p></li></ul><p><img src="https://i.imgur.com/qCapCEh.png" alt=""></p><h3 id="6-Clustering-Algorithms（聚类算法）"><a href="#6-Clustering-Algorithms（聚类算法）" class="headerlink" title="6. Clustering Algorithms（聚类算法）"></a>6. Clustering Algorithms（聚类算法）</h3><p>聚类分析又称群分析，它是研究（样品或指标）分类问题的一种统计分析方法，同时也是数据挖掘的一个重要算法。<br>聚类（Cluster）分析是由若干模式（Pattern）组成的，通常，模式是一个度量（Measurement）的向量，或者是多维空间中的一个点。<br>聚类分析以相似性为基础，在一个聚类中的模式之间比不在同一聚类中的模式之间具有更多的相似性。</p><p>常见的聚类算法包括：<br>k-Means<br>k-Medians<br>Expectation Maximisation (EM，Expectation Maximization Algorithm，是一种迭代算法)<br>Hierarchical Clustering（层次聚类）</p><p><img src="https://i.imgur.com/M9fOxmf.png" alt=""></p><h3 id="7-Association-Rule-Learning-Algorithms（关联规则学习算法）"><a href="#7-Association-Rule-Learning-Algorithms（关联规则学习算法）" class="headerlink" title="7. Association Rule Learning Algorithms（关联规则学习算法）"></a>7. Association Rule Learning Algorithms（关联规则学习算法）</h3><p>关联规则学习方法提取的规则最能解释数据中变量之间的关系，这些规则可以在大型多维数据集中发现重要和商业有用的关联，而被组织利用。</p><p>最常见的算法包括：</p><ul><li><p>Apriori algorithm</p></li><li><p>Eclat algorithm</p></li></ul><p><img src="https://i.imgur.com/xPgGsZt.png" alt=""></p><h3 id="8-Artificial-Neural-Network-Algorithms（人工神经网络算法）"><a href="#8-Artificial-Neural-Network-Algorithms（人工神经网络算法）" class="headerlink" title="8. Artificial Neural Network Algorithms（人工神经网络算法）"></a>8. Artificial Neural Network Algorithms（人工神经网络算法）</h3><p>人工神经网络是受生物神经网络结构和/或功能启发的模型，它们是一类模式匹配，通常用于回归和分类问题，但实际上是一个巨大的子字段，包含数百种算法和各种类型的问题类型。</p><p>最常见的算法包括：</p><ul><li><p>Perceptron（感知器）</p></li><li><p>Back-Propagation（反向传播法）</p></li><li><p>Hopfield Network（霍普菲尔网络）</p></li><li><p>Radial Basis Function Network (RBFN，径向基函数网络)</p></li></ul><p><img src="https://i.imgur.com/EPMHKEF.png" alt=""></p><h3 id="9-Deep-Learning-Algorithms（深度学习算法）"><a href="#9-Deep-Learning-Algorithms（深度学习算法）" class="headerlink" title="9. Deep Learning Algorithms（深度学习算法）"></a>9. Deep Learning Algorithms（深度学习算法）</h3><p>深度学习方法是利用大量廉价计算的人工神经网络的更新，它关心的是构建更大更复杂的神经网络，正如上面所提到的，许多方法都与半监督学习问题有关，在这些问题中，大型数据集包含的标签数据非常少。</p><p>最常见的算法包括：</p><ul><li><p>Deep Boltzmann Machine (DBM)</p></li><li><p>Deep Belief Networks (DBN)</p></li><li><p>Convolutional Neural Network (CNN)</p></li><li><p>Stacked Auto-Encoders</p></li></ul><p><img src="https://i.imgur.com/cPZsY1E.png" alt=""></p><h3 id="10-Dimensionality-Reduction-Algorithms（降维算法）"><a href="#10-Dimensionality-Reduction-Algorithms（降维算法）" class="headerlink" title="10. Dimensionality Reduction Algorithms（降维算法）"></a>10. Dimensionality Reduction Algorithms（降维算法）</h3><p>像聚类方法一样，维数的减少有利于寻找到数据的关联关系，但在这种情况下，是不受监督的方式，或者用较少的信息来概括或描述数据。<br>这些方法中的许多可以用于分类和回归。</p><p>常见的算法包括：</p><ul><li><p>Principal Component Analysis (PCA)</p></li><li><p>Principal Component Regression (PCR)</p></li><li><p>Partial Least Squares Regression (PLSR)</p></li><li><p>Sammon Mapping</p></li><li><p>Multidimensional Scaling (MDS)</p></li><li><p>Projection Pursuit</p></li><li><p>Linear Discriminant Analysis (LDA)</p></li><li><p>Mixture Discriminant Analysis (MDA)</p></li><li><p>Quadratic Discriminant Analysis (QDA)</p></li><li><p>Flexible Discriminant Analysis (FDA)</p></li></ul><p><img src="https://i.imgur.com/EKVzeTU.png" alt=""></p><h3 id="11-Ensemble-Algorithms（集成算法）"><a href="#11-Ensemble-Algorithms（集成算法）" class="headerlink" title="11. Ensemble Algorithms（集成算法）"></a>11. Ensemble Algorithms（集成算法）</h3><p>集成方法是由多个较弱的模型而组成的模型，这些模型是独立训练的，它们的预测在某种程度上是结合在一起来进行总体预测的。<br>这类算法是把更多精力放到了弱学习器身上，以及如何将它们结合起来。这是一门非常强大的技术，因此非常受欢迎。</p><p>常见的算法包括：</p><ul><li><p>Boosting</p></li><li><p>Bootstrapped Aggregation (Bagging)</p></li><li><p>AdaBoost</p></li><li><p>Stacked Generalization (blending)</p></li><li><p>Gradient Boosting Machines (GBM)</p></li><li><p>Gradient Boosted Regression Trees (GBRT)</p></li><li><p>Random Forest</p></li></ul><p><img src="https://i.imgur.com/FpJLEyH.png" alt=""></p><h3 id="12-Other-Algorithms（其他算法）"><a href="#12-Other-Algorithms（其他算法）" class="headerlink" title="12. Other Algorithms（其他算法）"></a>12. Other Algorithms（其他算法）</h3><p>还有很多算法没有被覆盖到，大概还有下面的算法：</p><p>Feature selection algorithms（特征选择算法）</p><ul><li><p>Algorithm accuracy evaluation（算法精度估计）</p></li><li><p>Performance measures（效果评估）</p></li><li><p>Computational intelligence (evolutionary algorithms, etc.)</p></li><li><p>Computer Vision (CV)</p></li><li><p>Natural Language Processing (NLP)</p></li><li><p>Recommender Systems</p></li><li><p>Reinforcement Learning</p></li><li><p>Graphical Models</p></li><li><p>And more…</p></li></ul><p>Further Reading<br>网络上对这些算法有更加详细的讲解，需要大家自己动手去查了，这样子才会更加了解这些算法内容，本文内容来自网络，还有一些我觉得很有用的资料也在下面，大家可以抽时间去细细研究哈。</p><p>##参考资料<br>1）<a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" target="_blank" rel="noopener">A Tour of Machine Learning Algorithms</a></p><p>2）<a href="https://www.zhihu.com/question/20691338/answer/53910077" target="_blank" rel="noopener">机器学习该如何入门——张松阳的回答</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接下来的文章基于来自Jason Brownlee 的文章《A Tour of Machine Learning Algorithms》&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="模型" scheme="https://paradoxallen.github.io/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>数据可视化(一) 图表简介</title>
    <link href="https://paradoxallen.github.io/41070/"/>
    <id>https://paradoxallen.github.io/41070/</id>
    <published>2017-02-11T16:00:00.000Z</published>
    <updated>2018-06-07T16:37:33.999Z</updated>
    
    <content type="html"><![CDATA[<h4 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h4><p>数据分析界有一句经典名言，字不如表，表不如图。数据可视化是数据分析的主要方向之一。除掉数据挖掘这类高级分析，不少数据分析就是监控数据观察数据。</p><p>数据分析的最终都是要兜售自己的观点和结论的。兜售的最好方式就是做出观点清晰数据详实的PPT给老板看。如果没人认同分析结果，那么分析也不会被改进和优化，不落地的数据分析价值又在哪里？</p><p>各类图表的详细介绍可以查看这篇文章。</p><p>温馨提示：如果您已经熟悉数据可视化，大可不必再看这篇文章，或只挑选部分。</p><a id="more"></a><hr><p>数据可视化是一个热门的概念，是分析师手中的优秀工具。好的可视化是会讲故事的，它向我们揭示了数据背后的规律。</p><p>大家对可视化的使用认知或许来源于下面这张图。虽然结构很清晰，但它更多针对Excel的图表，不够丰富。本文会结合数据分析师的使用场景展示更多的可视化案例。</p><p><img src="https://i.imgur.com/UwcqFXj.jpg" alt=""></p><blockquote><p>为方便演示，文中绝大多数图表为ECharts.js的范例。</p></blockquote><hr><p>了解可视化前，先知悉基础概念。</p><h3 id="维度"><a href="#维度" class="headerlink" title="维度"></a>维度</h3><p>数据分析中经常会提及维度。维度是观察数据的角度和对数据的描述。我们可以说地区是一种维度，这个维度包含上海北京这些城市。也可以认为销售额是一个维度，里面有各类销售数据。</p><p>维度可以用时间、数值表示，也可以用文本，文本常作为类别。数据分析的本质是各种维度的组合，我想了解和分析全国各地的销售额，就需要将地区维度和销售维度结合，如果想知道各个年份的变化，那么再加入时间维度。</p><p>说的再透彻点，Excel首行各字段就可以理解成维度。</p><p><img src="https://i.imgur.com/mOX36Y2.jpg" alt=""></p><p>互联网行业的PV、UV、活跃数也能算作维度。</p><p>图表的绘制依赖多个维度的组合。</p><h3 id="维度类型和转换"><a href="#维度类型和转换" class="headerlink" title="维度类型和转换"></a>维度类型和转换</h3><p>维度主要是三大类的数据结构：文本、时间、数值。地区的上海、北京就是文本维度（也可以称为类别维度），销售额度就是数值维度，时间更好理解了。不同图表有维度使用限制。</p><p>数值维度可以通过其他维度加工计算得出，例如按地区维度，count出有多少是上海的，有多少是北京的。</p><p>维度可以互相转换。比如年龄原本是数值型的维度，但是可以通过对年龄的划分，将其分类为小孩、青年、老年三个年龄段，此时就转换为文本维度。具体按照分析场景使用。</p><p>接下来介绍主要的可视化图表。</p><hr><h3 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h3><p>散点图在报表中不常用到，但是在数据分析中可以算出镜率最高的。散点图通过坐标轴，表示两个变量之间的关系。绘制它依赖大量数据点的分布。</p><p>散点图的优势是揭示数据间的关系，发觉变量与变量之间的关联。</p><p>散点图需要两个数值维度表示X轴、Y轴，下图范例就是身高和体重两个维度。</p><p><img src="https://i.imgur.com/vBTCNJl.jpg" alt=""></p><p>为了进行分析，该图又引入性别维度，通过颜色来区分。</p><p>当我们想知道两个指标互相之间有没有关系，散点图是最好的工具之一。因为它直观。尤其是大数据量，散点图会有更精准的结果。</p><p>后续的学习中，我们也会多次借用到散点图，比如统计中的回归分析，比如数据挖掘中的聚类。</p><h3 id="折线图"><a href="#折线图" class="headerlink" title="折线图"></a>折线图</h3><p>折线图是观察数据的趋势，它和时间是好基友，当我们想要了解某一维度在时间上的规律或者趋势时，就用折线图吧。</p><p><img src="https://i.imgur.com/AgemBW9.jpg" alt=""></p><p>折线图一般使用时间维度作为X轴，数值维度作为Y轴。</p><h3 id="柱形图"><a href="#柱形图" class="headerlink" title="柱形图"></a>柱形图</h3><p>柱形图是分析师最常用到的图表之一，常用于多个维度的比较和变化。</p><p>文本维度／时间维度通常作为X轴。数值型维度作为Y轴。柱形图至少需要一个数值型维度。</p><p>下图就是柱形图的对比分析，通过颜色区分类别。当需要对比的维度过多，柱形图是力不从心的。</p><p><img src="https://i.imgur.com/q4B7jy9.jpg" alt=""></p><p>柱形图和折线图在时间维度的分析中是可以互换的。但推荐使用折线图，因为它对趋势的变化表达更清晰。</p><p>柱形图还有许多丰富的应用。例如堆积柱形图，瀑布图，横向条形图，横轴正负图等。</p><p><img src="https://i.imgur.com/hHrVs1H.jpg" alt=""></p><p>直方图是柱形图的特殊形式。它的数值坐标轴是连续的，专用于统计，表达的是数据分布情况。在统计学的内容会专门讲解。</p><h3 id="地理图"><a href="#地理图" class="headerlink" title="地理图"></a>地理图</h3><p>一切和空间属性有关的分析都可以用到地理图。比如各地区销量，或者某商业区域店铺密集度等。</p><p>地理图一定需要用到坐标维度。可以是经纬度、也可以是地域名称（上海市、北京市）。坐标粒度即能细到具体某条街道，也能宽到世界各国范围。</p><p><img src="https://i.imgur.com/5HeGS3C.jpg" alt=""></p><p>除了经纬度，地理图的绘制离不开地图数据，POI是很重要的要素。POI是“Point of Information”的缩写，可以翻译成信息点，每个POI包含四方面信息，名称、类别、经度纬度、附近的酒店饭店商铺等信息。借助POI，才能按地理维度展现数据。</p><h3 id="饼图"><a href="#饼图" class="headerlink" title="饼图"></a>饼图</h3><p>饼图经常表示一组数据的占比。可以用扇面、圆环、或者多圆环嵌套。商务类的汇报中应用较多。</p><p>为了表示占比，拼图需要数值维度。</p><p><img src="https://i.imgur.com/MyuuzCb.jpg" alt=""></p><p>饼图是有缺陷的，它擅长表达某一占比较大的类别。但是不擅长对比。30%和35%在饼图上凭肉眼是难以分辨出区别的。当类别过多，也不适宜在饼图上表达。</p><p>对数据分析师来说，除了做报告，饼图没啥用。</p><h3 id="雷达图"><a href="#雷达图" class="headerlink" title="雷达图"></a>雷达图</h3><p>也叫蛛网图。可能男同胞们在游戏中看到它比较多。它在商务、财务领域应用较大，适合用在固定的框架内表达某种已知的结果。常见于经营状况，财务健康程度。</p><p>比如我对企业财务进行分析，划分出六大类：销售、市场、研发、客服、技术、管理。通过雷达图绘制出预算和实际开销的维度对比，会很清晰。如下图：</p><p><img src="https://i.imgur.com/cFy4uqW.jpg" alt=""></p><h3 id="箱线图"><a href="#箱线图" class="headerlink" title="箱线图"></a>箱线图</h3><p>箱线图一般人了解的不多，它能准确地反映数据维度的离散（最大数、最小数、中位数、四分数）情况。凡是离散的数据都适用箱线图。</p><p>下图就是箱线图的典型应用。线的上下两端表示某组数据的最大值和最小值。箱的上下两端表示这组数据中排在前25%位置和75%位置的数值。箱中间的横线表示中位数。</p><p><img src="https://i.imgur.com/RUj6v32.jpg" alt=""></p><p>假如你是一位互联网电商分析师，你想知道某商品每天的卖出情况：该商品被用户最多购买了几个，大部分用户购买了几个，用户最少购买了几个。箱线图就能很清晰的表示出上面的几个指标以及变化。</p><p>绘制箱线图，新人需要了解统计的基础概念：最大值，最小值，中位数，四分位数。这个会在后续讲解。</p><h3 id="热力图"><a href="#热力图" class="headerlink" title="热力图"></a>热力图</h3><p>以高亮形式展现数据。</p><p>最常见的例子就是用热力图表现道路交通状况。老司机一眼就知道怎么开车了。</p><p><img src="https://i.imgur.com/vEHwRch.jpg" alt=""></p><p>互联网产品中，热力图可以用于网站／APP的用户行为分析，将浏览、点击、访问页面的操作以高亮的可视化形式表现。下图就是用户在Google搜索结果的点击行为。</p><p><img src="https://i.imgur.com/Dsfnarm.jpg" alt=""></p><p>热力图需要位置信息，比如经纬度坐标，或者屏幕位置坐标。</p><h3 id="关系图"><a href="#关系图" class="headerlink" title="关系图"></a>关系图</h3><p>展现事物相关性和关联性的图表，比如社交关系链、品牌传播、或者某种信息的流动。</p><p><img src="https://i.imgur.com/HWfnNtE.jpg" alt=""></p><p>有一条微博，现在想研究它的传播链：它是经由哪几个大V分享扩散开来，大V前又有谁分享过等，以此为基础可以绘制出一幅发散的网状图，分析病毒营销的过程。</p><p><img src="https://i.imgur.com/pqCU9kR.jpg" alt=""></p><p>关系图依赖大量的数据，它本身没有维度的概念。</p><h3 id="矩形树图"><a href="#矩形树图" class="headerlink" title="矩形树图"></a>矩形树图</h3><p>上文说过，柱形图不适合表达过多类目（比如上百）的数据，那应该怎么办?矩形树图出现了。它直观地以面积表示数值，以颜色表示类目。</p><p>下图中各颜色系代表各个类目维度，类目维度下又有多个二级类目。如果用柱形图表达，简直是灾难。用矩形树图则轻轻松松。</p><p><img src="https://i.imgur.com/cZ1C97C.jpg" alt=""></p><p>电子商务、产品销售等涉及大量品类的分析，都可以用到矩形树图。</p><h3 id="桑基图-Sankey-Diagram"><a href="#桑基图-Sankey-Diagram" class="headerlink" title="桑基图 Sankey Diagram"></a>桑基图 Sankey Diagram</h3><p>比较冷门的图表，它常表示信息的变化和流动状态。</p><p><img src="https://i.imgur.com/7HPOoTv.jpg" alt=""></p><p>在我曾经写过的教你读懂活跃数据中，用桑基图绘制了用户活跃状态的变化，这是用户分层的可视化应用。<br>其实数据分析师经常接触到桑基图，只是不知道它的正式名字，它就是Google网站分析中的用户行为和流量分析。用户从哪里来，去了哪个页面，在哪个页面离开，最后停留在哪个页面等。下图就是桑基图非常直观的解释。</p><p><img src="https://i.imgur.com/MKn7i1j.jpg" alt=""></p><h3 id="漏斗图"><a href="#漏斗图" class="headerlink" title="漏斗图"></a>漏斗图</h3><p>大名鼎鼎的转化率可视化，它适用在固定流程的转化分析，你也可以认为它是桑基图的简化版。说实话，随着个性化推荐和精准运营越来越多，漏斗转化有它的局限性。</p><p>转化率也可以用几组数字表示，不一定做成漏斗图。</p><p><img src="https://i.imgur.com/0mMFCLZ.jpg" alt=""></p><p>除了上述可视化图表，还有其他很多经典，例如词云图、气泡图、K线图等。我们使用图表，不只是为了好看，虽然好看的报告面向老板和合作方很有优势。更多的是围绕业务进行分析，得到我们想要的结果。</p><p>没有最好的可视化图表，只有更好的分析方法。</p><p>有些数据可视化，Excel就能完成，有些则必须借助第三方工具或者编程。下一篇文章会挑选部分图表教大家如何Excel绘制。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>本文内容来源于网络，版权归原作者</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h4&gt;&lt;p&gt;数据分析界有一句经典名言，字不如表，表不如图。数据可视化是数据分析的主要方向之一。除掉数据挖掘这类高级分析，不少数据分析就是监控数据观察数据。&lt;/p&gt;
&lt;p&gt;数据分析的最终都是要兜售自己的观点和结论的。兜售的最好方式就是做出观点清晰数据详实的PPT给老板看。如果没人认同分析结果，那么分析也不会被改进和优化，不落地的数据分析价值又在哪里？&lt;/p&gt;
&lt;p&gt;各类图表的详细介绍可以查看这篇文章。&lt;/p&gt;
&lt;p&gt;温馨提示：如果您已经熟悉数据可视化，大可不必再看这篇文章，或只挑选部分。&lt;/p&gt;
    
    </summary>
    
      <category term="数据分析" scheme="https://paradoxallen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="可视化" scheme="https://paradoxallen.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Excel学习(三) 实战篇</title>
    <link href="https://paradoxallen.github.io/17615/"/>
    <id>https://paradoxallen.github.io/17615/</id>
    <published>2017-02-09T16:00:00.000Z</published>
    <updated>2018-06-07T16:21:33.514Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/19174/">Excel学习(一) 函数篇</a><br><a href="https://paradoxallen.github.io/12853/">Excel学习(二) 技巧篇</a></p><h4 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h4><p>此文针对数据分析EXCEL部分的入门。</p><p>重点是了解各种函数，包括但不限于sum，count，sumif，countif，find，if，left/right，时间转换等。Excel函数不需要学全，<strong>重要的是学会搜索</strong>。即如何将遇到的问题在搜索引擎上描述清楚。掌握vlookup和数据透视表足够，是最具性价比的两个技巧。</p><p>学会vlookup，SQL中的join，Python中的merge很容易理解。</p><p>学会数据透视表，SQL中的group，Python中的pivot_table也是同理。</p><p>这两个搞定，基本10万条以内的数据统计没啥难度。Excel是熟能生巧，多找练习题。还有需要养成好习惯，不要合并单元格，不要过于花哨。表格按照原始数据（sheet1）、加工数据（sheet2），图表（sheet3）的类型管理。</p><p>第三篇数据分析—技巧篇。主要将前两篇的内容以实战方式进行，简单地进行了一次数据分析。数据源采用了真实的爬虫数据，是5000行数据分析师岗位数据。</p><p>温馨提示：如果您已经熟悉Excel，大可不必再看这篇文章，或只挑选部分。</p><a id="more"></a><hr><p>这篇文章讲解实战，如何运用上两篇文章的知识进行分析。</p><p>演示过程分为五个步骤：<strong>明确目的，观察数据，清洗数据，分析过程，得出结论。</strong></p><p>这也是通常数据分析的简化流程。</p><hr><h3 id="明确目的"><a href="#明确目的" class="headerlink" title="明确目的"></a>明确目的</h3><p>数据分析的大忌是不知道分析方向和目的，拿着一堆数据不知所措。<strong>一切数据分析都是以业务为核心目的</strong>，而不是以数据为目的。</p><p>数据用来解决什么问题？</p><p>是进行汇总统计制作成报表？</p><p>是进行数据可视化，作为一张信息图？</p><p>是验证某一类业务假设？</p><p>是希望提高某一个指标的KPI？</p><p>永远不要妄图在一堆数据中找出自己的结论，太难。目标在前，数据在后。哪怕给自己设立一个很简单的目标，例如计算业务的平均值，也比没有方向好。因为有了平均值可以想数字比预期是高了还是低了，原因在哪里，数据靠谱吗？为了找出原因还需要哪些数据。</p><p>既然有五千多条数据分析师的岗位数据。不妨在看数据前想一下自己会怎么运用数据。</p><p>数据分析师是一个什么样的岗位？</p><p>它的工资和薪酬是多少？</p><p>它有什么特点，需要掌握哪些能力？</p><p>哪类公司更会招聘数据分析师？</p><p>等等。有了目标和方向后，后续则是将目标拆解为实际过程。</p><hr><h3 id="观察数据"><a href="#观察数据" class="headerlink" title="观察数据"></a>观察数据</h3><p><img src="https://i.imgur.com/OKfRHrB.jpg" alt=""></p><p>拿出数据别急切计算，先观察数据。</p><p>字段名称都是英文，我是通过Json获取的数据，所以整体数据都较为规整。往后绝大部分的数据源的字段名都是英文。因为比起拼音和汉字，它更适合编程环境下。</p><p>先看一下columns的含义。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">city：城市</span><br><span class="line"></span><br><span class="line">companyFullName：公司全名</span><br><span class="line"></span><br><span class="line">companyId：公司ID</span><br><span class="line"></span><br><span class="line">companyLabelList：公司介绍标签</span><br><span class="line"></span><br><span class="line">companyShortName：公司简称</span><br><span class="line"></span><br><span class="line">companySize：公司大小</span><br><span class="line"></span><br><span class="line">businessZones：公司所在商区</span><br><span class="line"></span><br><span class="line">firstType：职位所属一级类目</span><br><span class="line"></span><br><span class="line">secondType：职业所属二级类目</span><br><span class="line"></span><br><span class="line">education：教育要求</span><br><span class="line"></span><br><span class="line">industryField：公司所属领域</span><br><span class="line"></span><br><span class="line">positionId：职位ID</span><br><span class="line"></span><br><span class="line">positionAdvantage：职位福利</span><br><span class="line"></span><br><span class="line">positionName：职位名称</span><br><span class="line"></span><br><span class="line">positionLables：职位标签</span><br><span class="line"></span><br><span class="line">salary：薪水</span><br><span class="line"></span><br><span class="line">workYear：工作年限要求</span><br></pre></td></tr></table></figure><p>数据基本涵盖了职位分析的所需。职位中的职位描述没有抓下来，一来纯文本不适合这次初级分析，二来文本需要分词以及文本挖掘，后续有机会再讲。</p><p>首先看一下哪些字段数据可以去除。<code>companyId</code>和<code>positionId</code>是数据的唯一标示，类似该职位的身份证号，这次分析用不到关联vlookup，我们先隐藏。<code>companyFullName</code>和<code>companyShortName</code>则重复了，只需要留一个公司名称，<code>companyFullName</code>依旧隐藏。</p><p>尽量不删除数据，而是隐藏，保证原始数据的完整，谁知道以后会不会用到呢？</p><p><img src="https://i.imgur.com/B3voXgf.jpg" alt=""></p><p>接下来进行数据清洗和转换。因为只是Excel级别的数据分析，不会有哑变量离散化标准化的操作。我简单归纳一下。</p><p><strong>数据有无缺失值</strong></p><p>数据的缺失值很大程度上影响分析结果。引起缺失的原因很多，例如技术原因，爬虫没有完全抓去，例如本身的缺失，该岗位的HR没有填写。</p><p>如果某一字段缺失数据较多（超过50%），分析过程中要考虑是否删除该字段，因为缺失过多就没有业务意义了。</p><p>Excel中可以通过选取该列，在屏幕的右下角查看计数，以此判别有无缺失。</p><p><code>companyLabelList、businessZones、positionLables</code>都有缺失，但不多。不影响实际分析。</p><p><strong>数据是否一致化</strong></p><p>一致化指的是数据是否有统一的标准或命名。例如上海市数据分析有限公司和上海数据分析有限公司，差别就在一个市字，主观上肯定会认为是同一家公司，但是对机器和程序依旧会把它们认成两家。会影响计数、数据透视的结果。</p><p>我们看一下表格中的<code>positionName</code></p><p><img src="https://i.imgur.com/EVbGN3f.jpg" alt=""></p><p>各类职位千奇百怪啊，什么品牌保护分析师实习生、足球分析师、商业数据分析、大数据业务分析师、数据合同管理助理。并不是纯粹的数据分析岗位。</p><p>为什么呢？这是招聘网站的原因，有些职位明确为数据分析师，有些职位要求具备数据分析能力，但是又干其他活。招聘网站为了照顾这种需求，采用关联法，只要和数据分析相关职位，都会在数据分析师的搜索结果中出现。我的爬虫没有过滤其他数据，这就需要手动清洗。</p><p>这会不会影响我们的分析？当然会。像大数据工程师是数据的另外发展方向，但不能归纳到数据分析岗位下，后续我们需要将数据分析强相关的职位挑选出来。</p><p><strong>数据是否有脏数据</strong></p><p>脏数据是分析过程中很讨厌的环节。例如乱码，错位，重复值，未匹配数据，加密数据等。能影响到分析的都算脏数据，没有一致化也可以算。</p><p>我们看表格中有没有重复数据。</p><p>这里有一个快速窍门，使用Excel的删除重复项功能，快速定位是否有重复数据，还记得<code>positionId</code>么？因为它是唯一标示，如果重复了，就说明有重复的职位数据。看来不删除它是正确的。</p><p>对<code>positionId</code>列进行重复项删除操作</p><p><img src="https://i.imgur.com/NKWu2cR.jpg" alt=""></p><p>有1845个重复值。数据重复了。这是我当时爬取完数据时，将北京地区多爬取一次人为制作出的脏数据。接下来全选所有数据，进行删除重复项，保留5032行（含表头字段）数据。</p><p><strong>数据标准结构</strong></p><p>数据标准结构，就是将特殊结构的数据进行转换和规整。</p><p>表格中，<code>companyLableList</code>就是以数组形式保存（JSON中的数组）</p><p><img src="https://i.imgur.com/uU7AVNo.jpg" alt=""></p><p>看来福利倒是不错，哈哈，不过这会影响我们的分析。<code>businessZones、positionAdvantage和positionLables</code>也是同样问题，我们后续得将这类格式拆分开来。</p><p><img src="https://i.imgur.com/pop4oIH.jpg" alt=""></p><p>薪水的话用了几K表示，但这是文本，并不能直接用于计算。而且是一个范围，后续得按照最高薪水和最低薪水拆成两列。</p><p>OK，数据大概都了解了，那么下一步就是将数据洗干净。</p><hr><h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><p>数据清洗可以新建Sheet，方便和原始数据区分开来。</p><p>先清洗薪水吧，大家肯定对钱感兴趣。将<code>salary</code>拆成最高薪水和最低薪水有三种办法。</p><p><strong>一是直接分列</strong>，以<code>&quot;-&quot;</code>为拆分符，得到两列数据，然后利用替换功能删除 k这个字符串。得到结果。</p><p><strong>二是自动填充功能</strong>，填写已填写的内容自动计算填充所有列。但我这个版本没有，就不演示了。</p><p><strong>三是利用文本查找的思想</strong>，重点讲一下这个。先用<code>=FIND(&quot;k&quot;,O2,1)</code>。查找第一个<code>K</code>（最低薪酬）出现的位置。</p><p><img src="https://i.imgur.com/zUVtDC4.jpg" alt=""></p><p>我们知道第一个k出现的位置，此时<code>=LEFT(O2,FIND(&quot;k&quot;,O2,1))</code>得到的结果就是<code>7K</code>，要去除掉<code>k</code>，<code>FIND(&quot;k&quot;,O2,1)</code>再减去1即可。</p><p><img src="https://i.imgur.com/443spNJ.jpg" alt=""></p><p>最高薪水也是同样的思路，但不能使用<code>k</code>，因为第二个薪水位置不固定。需要利用<code>find</code>查找<code>&quot;-&quot;</code>位置,然后截取 从<code>&quot;-&quot;</code> 到最后第二个位置的字符串。</p><pre><code>=MID(O2,FIND(&quot;-&quot;,O2,1)+1,LEN(O2)-FIND(&quot;-&quot;,O2,1)-1)</code></pre><p><img src="https://i.imgur.com/61k6qsj.jpg" alt=""></p><p>因为薪水是一个范围，我们不可能拿范围计算平均工资。那怎么办呢？我们只能取最高薪水和最低薪水的平均数作为该岗位薪资。这是数据来源的缺陷，因为我们并不能知道应聘者实际能拿多少。这是薪水计算的误差。</p><p><img src="https://i.imgur.com/eOw0B0w.jpg" alt=""></p><p>我们检查一下有没有错误，利用筛选功能快速定位。</p><p><img src="https://i.imgur.com/OuAa3zN.jpg" alt=""></p><p>居然有<code>#VALUE！</code>错误，看一下原因。</p><p><img src="https://i.imgur.com/BQvT1z6.jpg" alt=""></p><p>原来是大写<code>K</code>，因为<code>find</code>对大小写敏感，此时用<code>search</code>函数，或者将K替换成k都能解决。</p><p>另外还有一个错误是很多HR将工资写成<code>5K</code>以上，这样就无法计算<code>topSalar</code>。为了计算方便，将<code>topSalary</code>等于<code>bottomSalary</code>，虽然也有误差。</p><p>这就是我强调数据一致性的原因。</p><p><code>companyLabelList</code>是公司标签，诸如技能培训啊、五险一金啊等等。直接用分列即可。大家需要注意，分列会覆盖掉右列单元格，所以记得复制到最后一列再分。</p><p><img src="https://i.imgur.com/UpTMB87.jpg" alt=""></p><p>符号用搜索替换法删除即可。</p><p><code>positionLables、positionAdvantage、businessZones</code>同样也可以用分列法。如果观察过数据会知道，<code>companyLabelList</code>公司标签都是固定的内容，而其他三个不是。这些都是HR自己填写，所以就会有各种乱七八糟不统一的描述。</p><p><img src="https://i.imgur.com/AoHOT6x.jpg" alt=""></p><p>这些内容均是自定义，没有特别大的分析价值。如果要分析，必须花费很长的时间在清洗过程。主要思路是把这些内容统一成几十个固定标签。在这里我将不浪费时间讲解了，主要利用Python分词和词典进行快速清洗。</p><p>因为时间和性价比问题，<code>positionAdvantage</code>和<code>businessZones</code>我就不分列了。只清洗<code>positionLables</code>职位标签。某一个职位最多的标签有13个。</p><p><code>[&#39;实习生&#39;, &#39;主管&#39;, &#39;经理&#39;,&#39;顾问&#39;, &#39;销售&#39;, &#39;客户代表&#39;, &#39;分析师&#39;, &#39;职业培训&#39;, &#39;教育&#39;, &#39;培训&#39;, &#39;金融&#39;, &#39;证券&#39;, &#39;讲师&#39;]</code><br>这个职位叫金融证券分析师助理讲师助理，我真不知道为什么实习生、主管、经理这三个标签放在一起，我也是哔了狗了。反正大家数据分析做久了，会遇到很多<code>Magic Data</code>。</p><p>接下来是<code>positionName</code>，上文已经讲过有各种乱七八糟或非数据分析师职位，所以我们需要排除掉明显不是数据分析师的岗位。</p><p>单独针对<code>positionName</code>用数据透视表。统计各名称出现的次数。</p><p><img src="https://i.imgur.com/5U6EQdf.jpg" alt=""></p><p>出现次数为3次以下的职位，有约一千，都是各类特别称谓，HR你们为什么要这样写…要这样写…这样写。更改职位名称似乎不现实，那就用关键词查找的思路，找出包含有数据分析、分析师、数据运营等关键词的岗位。虽然依旧会有金融分析师这类非纯数据的岗位。</p><p>用<code>find</code>和数组函数结合 <code>=IF(COUNT(FIND({&quot;数据分析&quot;,&quot;数据运营&quot;,&quot;分析师&quot;},M33)),&quot;1&quot;,&quot;0&quot;)，shift+ctrl+enter</code>输入。就得到了多条件查找后的结果。</p><p>单纯的<code>find</code>只会查找数据分析这个词，必须嵌套<code>count</code>才会变成真数组。</p><p><img src="https://i.imgur.com/zJFvq0n.jpg" alt=""></p><p><code>1为包含，0不包含</code>。将1过滤出来，这就是需要分析的最终数据。</p><p>当然大家如果感兴趣，也可以看一下大数据工程师，数据产品经理这些岗位。</p><hr><h3 id="分析过程-amp-得出结论"><a href="#分析过程-amp-得出结论" class="headerlink" title="分析过程&amp;得出结论"></a>分析过程&amp;得出结论</h3><p>分析过程有很多玩法。因为主要数据均是文本格式，所以偏向汇总统计的计算。如果数值型的数据比较多，就会涉及到统计、比例等概念。如果有时间类数据，那么还会有趋势、变化的概念。</p><p>整体分析使用数据透视表完成，先利用数据透视表获得汇总型统计。</p><p><img src="https://i.imgur.com/W4w1AsB.jpg" alt=""></p><p>看来北京的数据分析岗位机会远较其他城市多。1-3年和3-5年两个时间段的缺口更大。应届毕业生似乎比1年一下经验的更吃香。爬取时间为11月，这时候校招陆续开始，大公司会有闲暇校招，实际岗位应该更多。小公司则倾向发布。这是招聘网站的限制。</p><p>看一下公司对数据分析师的缺口如何。</p><p><img src="https://i.imgur.com/8xBiZqa.jpg" alt=""></p><p>似乎是公司越大，需要的数据分析师越多。</p><p>但这样的分析并不准确。因为这只是一个汇总数据，而不是比例数据，我们需要计算的是不同类型企业人均招聘数。</p><p>如果北京的互联网公司特别多，那么即使有1000多个岗位发布也不算缺口大，如果南京的互联网公司少，即使只招聘30个，也是充满需求的。</p><p>还有一种情况是企业刚好招聘满数据分析师，就不发布岗位了，数据包含的只是正在招聘数据分析师的企业，这些都是限制分析的因素。我们要明确。</p><p>有兴趣大家可以深入研究。</p><p>看一下各城市招聘Top5公司。</p><p><img src="https://i.imgur.com/j4lsjT3.jpg" alt=""></p><p>北京的美团以78个数据分析职位招聘力压群雄，甚至一定程度上拉高了北京的数据。而个推则在上海和杭州都发布了多个数据分析师职位，不知道是HR的意外，还是要大规模补充业务线（在我写这篇文章的时候，约有一半职位已经下线）。</p><p>比较奇怪的是阿里巴巴并没有在杭州上榜，看来是该阶段招聘需求不大，或者数据分析师有其他招聘渠道。</p><p>没有上榜不代表不要数据分析师，但是上榜的肯定现阶段对数据分析师有需求。</p><p>我们看一下数据分析师的薪水，可能是大家最感兴趣的了。</p><p><img src="https://i.imgur.com/ifz87wL.jpg" alt=""></p><p>我们看到南京、西安在应届生中数据最高，是因为招聘职位不多，因为单独一两个企业的高薪影响了平均数，其余互联网二线城市同理。当工作年限达到3年以上，北上深杭的数据分析师薪资则明显高于其他城市。</p><p>数据会有误差性么？会的，因为存在薪资极值影响。而数据透视表没有中位数选项。我们也可以单独用分位数进行计算，降低误差。</p><p>薪资可以用更细的维度计算，比如学历、比如公司行业领域，是否博士生远高于本科生，是否金融业薪资高于O2O。</p><p>另外数据分析师的薪资，可能包括奖金、年终奖、季度奖等隐形福利。部分企业会在<code>positionAdvantage</code>的内容上说明，大家可以用筛选过滤出16薪这类关键词。作为横向对比。</p><p><img src="https://i.imgur.com/iy0Gy0U.jpg" alt=""></p><p>我们看一下数据分析的职位标签，数据透视后汇总。</p><p><img src="https://i.imgur.com/B63AFJS.jpg" alt=""></p><p><code>分析师、数据、数据分析</code>是最多的标签。除此以外，<code>需求分析，BI，数据挖掘</code>也出现在前列。看来不少数据分析师的要求掌握数据挖掘，将标签和薪水关联，是另外一种分析思路。职位标签并不是最优的解法，了解一个职位最好的必然是职位描述。</p><p>分析过程不多做篇幅了，主要使用数据透视表进行多维度分析，没有其他复杂的技巧。下图很直观的展现了多维度的应用。</p><p><img src="https://i.imgur.com/vRSysg5.jpg" alt=""></p><p>我们的分析也属于多维度，<code>城市、工作年限、企业大小、企业领域</code>等，利用不同维度形成一个直观的二位表格，而维度则是通过早期的数据清洗统一化标准化。这是一种很常见的分析技巧。</p><p>后续的数据报告，涉及到可视化制作，因为字不如表、表不如图，就放在后面讲解了。</p><p>最后多说几下：</p><p><strong>1.最好的分析，是拿数据分析师们的在职数据，而不是企业招聘数据。</strong></p><p><strong>2.承认招聘数据的非客观性，招聘要求与对数据分析师的实际要求是有差异的。</strong></p><hr><p>####写在最后<br>除了Excel的这三部分内容，还有一些也需要进一步了解的：</p><p>了解单元格格式，后期的数据类型包括各类timestamp，date，string，int，bigint，char，factor，float等。</p><p>了解数组，以及怎么用（excel的数组挺难用），Python和R也会涉及到 list。</p><p>了解函数和参数，当进阶为编程型的数据分析师时，会让你更快的掌握。</p><p>了解中文编码，UTF8和ASCII，包括CSV的delimiter等。</p><p>养成一个好习惯，不要合并单元格，不要过于花哨。表格按照原始数据、加工数据，图表的类型管理。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>本文内容来源于网络，版权归原作者</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/19174/&quot;&gt;Excel学习(一) 函数篇&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://paradoxallen.github.io/12853/&quot;&gt;Excel学习(二) 技巧篇&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h4&gt;&lt;p&gt;此文针对数据分析EXCEL部分的入门。&lt;/p&gt;
&lt;p&gt;重点是了解各种函数，包括但不限于sum，count，sumif，countif，find，if，left/right，时间转换等。Excel函数不需要学全，&lt;strong&gt;重要的是学会搜索&lt;/strong&gt;。即如何将遇到的问题在搜索引擎上描述清楚。掌握vlookup和数据透视表足够，是最具性价比的两个技巧。&lt;/p&gt;
&lt;p&gt;学会vlookup，SQL中的join，Python中的merge很容易理解。&lt;/p&gt;
&lt;p&gt;学会数据透视表，SQL中的group，Python中的pivot_table也是同理。&lt;/p&gt;
&lt;p&gt;这两个搞定，基本10万条以内的数据统计没啥难度。Excel是熟能生巧，多找练习题。还有需要养成好习惯，不要合并单元格，不要过于花哨。表格按照原始数据（sheet1）、加工数据（sheet2），图表（sheet3）的类型管理。&lt;/p&gt;
&lt;p&gt;第三篇数据分析—技巧篇。主要将前两篇的内容以实战方式进行，简单地进行了一次数据分析。数据源采用了真实的爬虫数据，是5000行数据分析师岗位数据。&lt;/p&gt;
&lt;p&gt;温馨提示：如果您已经熟悉Excel，大可不必再看这篇文章，或只挑选部分。&lt;/p&gt;
    
    </summary>
    
      <category term="数据分析" scheme="https://paradoxallen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Excel" scheme="https://paradoxallen.github.io/tags/Excel/"/>
    
  </entry>
  
</feed>
