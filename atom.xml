<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>paradox&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://paradoxallen.github.io/"/>
  <updated>2018-06-03T09:16:25.590Z</updated>
  <id>https://paradoxallen.github.io/</id>
  
  <author>
    <name>paradox</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习(五) 常见算法优缺点</title>
    <link href="https://paradoxallen.github.io/65434/"/>
    <id>https://paradoxallen.github.io/65434/</id>
    <published>2018-06-03T16:00:00.000Z</published>
    <updated>2018-06-03T09:16:25.590Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p><a href="https://paradoxallen.github.io/62602/">机器学习(三) 模型结果应用</a></p><p><a href="https://paradoxallen.github.io/21484/">机器学习(四) 常见算法优缺点</a></p><p>文章结构：</p><ul><li><p><strong>什么是感知器分类算法</strong></p></li><li><p><strong>在Python中实现感知器学习算法</strong></p></li></ul><p><em>在iris（鸢尾花）数据集上训练一个感知器模型</em></p><ul><li><strong>自适应线性神经元和融合学习</strong></li></ul><p><em>使用梯度下降方法来最小化损失函数</em></p><p><em>在Python中实现一个自适应的线性神经元</em></p><a id="more"></a><hr><h3 id="什么是感知器分类算法"><a href="#什么是感知器分类算法" class="headerlink" title="什么是感知器分类算法"></a><strong>什么是感知器分类算法</strong></h3><p>设想我们改变逻辑回归算法，“迫使”它只能输出-1或1抑或其他定值。在这种情况下，之前的逻辑函数‍‍g就会变成阈值函数sign：</p><p><img src="http://i4.bvimg.com/647637/5ceeec4c3896d4f6.png" alt=""></p><p><img src="http://i4.bvimg.com/647637/3a51f1d2122e938a.png" alt=""></p><p>如果我们令假设为hθ(x)=g(θTx)hθ(x)=g(θTx)，将其带入之前的迭代法中：</p><p><img src="http://i4.bvimg.com/647637/1ea8a43feab7ac88.png" alt=""></p><p>至此我们就得出了感知器学习算法。简单地来说，感知器学习算法是神经网络中的一个概念，单层感知器是最简单的神经网络，输入层和输出层直接相连。</p><p><img src="http://i4.bvimg.com/647637/8a3bcfa1b5cd38cd.png" alt=""></p><p>每一个输入端和其上的权值相乘，然后将这些乘积相加得到乘积和，这个结果与阈值相比较（一般为0），若大于阈值输出端就取1，反之，输出端取-1。</p><p>初始权重向量W=[0,0,0]，更新公式W(i)=W(i)+ΔW(i)；ΔW(i)=η<em>(y-y’)</em>X(i)； </p><p>η：学习率，介于[0,1]之间 </p><p>y：输入样本的正确分类 </p><p>y’：感知器计算出来的分类 </p><p>通过上面公式不断更新权值，直到达到分类要求。</p><p><img src="http://i4.bvimg.com/647637/3e5ea78692abac79.jpg" alt=""></p><p>初始化权重向量W，与输入向量做点乘，将结果与阈值作比较，得到分类结果1或-1。</p><hr><h3 id="在Python中实现感知器学习算法"><a href="#在Python中实现感知器学习算法" class="headerlink" title="在Python中实现感知器学习算法"></a><strong>在Python中实现感知器学习算法</strong></h3><p>下面直接贴上实现代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Perceptron(object):</span><br><span class="line">    &quot;&quot;&quot;Perceptron classifier.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ------------</span><br><span class="line">    eta : float</span><br><span class="line">        Learning rate (between 0.0 and 1.0)</span><br><span class="line">    n_iter : int</span><br><span class="line">        Passes over the training dataset.</span><br><span class="line"></span><br><span class="line">    Attributes</span><br><span class="line">    -----------</span><br><span class="line">    w_ : 1d-array</span><br><span class="line">        Weights after fitting.</span><br><span class="line">    errors_ : list</span><br><span class="line">        Number of misclassifications (updates) in each epoch.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, eta=0.01, n_iter=10):</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y):</span><br><span class="line">        &quot;&quot;&quot;Fit training data.</span><br><span class="line"></span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span><br><span class="line">            Training vectors, where n_samples is the number of samples and</span><br><span class="line">            n_features is the number of features.</span><br><span class="line">        y : array-like, shape = [n_samples]</span><br><span class="line">            Target values.</span><br><span class="line"></span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        self : object</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.w_ = np.zeros(1 + X.shape[1])</span><br><span class="line">        self.errors_ = []</span><br><span class="line"></span><br><span class="line">        for _ in range(self.n_iter):</span><br><span class="line">            errors = 0</span><br><span class="line">            for xi, target in zip(X, y):</span><br><span class="line">                update = self.eta * (target - self.predict(xi))</span><br><span class="line">                self.w_[1:] += update * xi</span><br><span class="line">                self.w_[0] += update</span><br><span class="line">                errors += int(update != 0.0)</span><br><span class="line">            self.errors_.append(errors)</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def net_input(self, X):</span><br><span class="line">        &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span><br><span class="line">        return np.dot(X, self.w_[1:]) + self.w_[0]</span><br><span class="line"></span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span><br><span class="line">        return np.where(self.net_input(X) &gt;= 0.0, 1, -1)</span><br></pre></td></tr></table></figure><p><strong>特别说明：</strong></p><p>学习速率η(eta)只有在权重（一般取值0或者很小的数）为非零值的时候，才会对分类结果产生作用。如果所有的权重都初始化为0，学习速率参数eta只影响权重向量的大小，而不影响其方向，为了使学习速率影响分类结果，权重需要初始化为非零值。需要更改的代码中的相应行在下面突出显示:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, eta=0.01, n_iter=50, random_seed=1): # add random_seed=1</span><br><span class="line">    ...</span><br><span class="line">    self.random_seed = random_seed # add this line</span><br><span class="line">def fit(self, X, y):</span><br><span class="line">    ...</span><br><span class="line">    # self.w_ = np.zeros(1 + X.shape[1]) ## remove this line</span><br><span class="line">    rgen = np.random.RandomState(self.random_seed) # add this line</span><br><span class="line">    self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1]) # add this line</span><br></pre></td></tr></table></figure></p><p><strong>在iris（鸢尾）数据集上训练一个感知器模型</strong></p><p><strong>读取iris数据集</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import collections</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(&apos;https://archive.ics.uci.edu/ml/&apos;</span><br><span class="line">        &apos;machine-learning-databases/iris/iris.data&apos;, header=None)</span><br><span class="line">print (df.head())</span><br><span class="line">print (&quot;\n&quot;)</span><br><span class="line">print (df.describe())</span><br><span class="line">print (&quot;\n&quot;)</span><br><span class="line">print (collections.Counter(df[4]))</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="http://i4.bvimg.com/647637/a583bd20347fb68b.jpg" alt=""></p><p><strong>可视化iris数据</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 为了显示中文(这里是Mac的解决方法，其他的大家可以去百度一下)</span><br><span class="line">from matplotlib.font_manager import FontProperties</span><br><span class="line">font = FontProperties(fname=&apos;/System/Library/Fonts/STHeiti Light.ttc&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 选择 setosa and versicolor类型的花</span><br><span class="line">y = df.iloc[0:100, 4].values</span><br><span class="line">y = np.where(y == &apos;Iris-setosa&apos;, -1, 1)</span><br><span class="line"></span><br><span class="line"># 提取它们的特征 （sepal length and petal length）</span><br><span class="line">X = df.iloc[0:100, [0, 2]].values</span><br><span class="line"></span><br><span class="line"># 可视化数据，因为数据有经过处理，总共150行数据，1-50行是setosa花，51-100是versicolor花，101-150是virginica花</span><br><span class="line">plt.scatter(X[:50, 0], X[:50, 1],</span><br><span class="line">            color=&apos;red&apos;, marker=&apos;o&apos;, label=&apos;setosa&apos;)</span><br><span class="line">plt.scatter(X[50:100, 0], X[50:100, 1],</span><br><span class="line">            color=&apos;blue&apos;, marker=&apos;x&apos;, label=&apos;versicolor&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlabel(&apos;sepal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;petal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="http://i4.bvimg.com/647637/1420ac757d167952.png" alt=""></p><p><strong>训练感知器模型</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Perceptron是我们前面定义的感知器算法函数，这里就直接调用就好</span><br><span class="line">ppn = Perceptron(eta=0.1, n_iter=10)</span><br><span class="line"></span><br><span class="line">ppn.fit(X, y)</span><br><span class="line"></span><br><span class="line">plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=&apos;o&apos;)</span><br><span class="line">plt.xlabel(&apos;迭代次数&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;权重更新次数（错误次数）&apos;,FontProperties=font,fontsize=14)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="http://i4.bvimg.com/647637/0557d85f969d74b5.png" alt=""></p><p><strong>绘制函数决策区域</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib.colors import ListedColormap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot_decision_regions(X, y, classifier, resolution=0.02):</span><br><span class="line"></span><br><span class="line">    # setup marker generator and color map</span><br><span class="line">    markers = (&apos;s&apos;, &apos;x&apos;, &apos;o&apos;, &apos;^&apos;, &apos;v&apos;)</span><br><span class="line">    colors = (&apos;red&apos;, &apos;blue&apos;, &apos;lightgreen&apos;, &apos;gray&apos;, &apos;cyan&apos;)</span><br><span class="line">    cmap = ListedColormap(colors[:len(np.unique(y))])</span><br><span class="line"></span><br><span class="line">    # plot the decision surface</span><br><span class="line">    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),</span><br><span class="line">                           np.arange(x2_min, x2_max, resolution))</span><br><span class="line">    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br><span class="line">    Z = Z.reshape(xx1.shape)</span><br><span class="line">    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)</span><br><span class="line">    plt.xlim(xx1.min(), xx1.max())</span><br><span class="line">    plt.ylim(xx2.min(), xx2.max())</span><br><span class="line"></span><br><span class="line">    # plot class samples</span><br><span class="line">    for idx, cl in enumerate(np.unique(y)):</span><br><span class="line">        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],</span><br><span class="line">                    alpha=0.8, c=cmap(idx),</span><br><span class="line">                    edgecolor=&apos;black&apos;,</span><br><span class="line">                    marker=markers[idx], </span><br><span class="line">                    label=cl)</span><br><span class="line">plot_decision_regions(X, y, classifier=ppn)</span><br><span class="line">plt.xlabel(&apos;sepal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;petal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="http://i4.bvimg.com/647637/d0436f465dbb6d06.png" alt=""></p><hr><h3 id="自适应线性神经元和融合学习"><a href="#自适应线性神经元和融合学习" class="headerlink" title="自适应线性神经元和融合学习"></a><strong>自适应线性神经元和融合学习</strong></h3><p><strong>使用梯度下降方法来最小化损失函数</strong></p><p>梯度下降的方法十分常见，具体的了解可以参考附录的文章[2]，如今，梯度下降主要用于在神经网络模型中进行权重更新，即在一个方向上更新和调整模型的参数，来最小化损失函数。</p><p><img src="http://i4.bvimg.com/647637/09c54e58b0ce88f3.jpg" alt=""><br>图：梯度下降原理过程演示</p><p><strong>在Python中实现一个自适应的线性神经元</strong></p><p>先贴上定义的python函数，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"># 定义神经元函数</span><br><span class="line">class AdalineGD(object):</span><br><span class="line">    &quot;&quot;&quot;ADAptive LInear NEuron classifier.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ------------</span><br><span class="line">    eta : float</span><br><span class="line">        Learning rate (between 0.0 and 1.0)</span><br><span class="line">    n_iter : int</span><br><span class="line">        Passes over the training dataset.</span><br><span class="line"></span><br><span class="line">    Attributes</span><br><span class="line">    -----------</span><br><span class="line">    w_ : 1d-array</span><br><span class="line">        Weights after fitting.</span><br><span class="line">    cost_ : list</span><br><span class="line">        Sum-of-squares cost function value in each epoch.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, eta=0.01, n_iter=50):</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y):</span><br><span class="line">        &quot;&quot;&quot; Fit training data.</span><br><span class="line"></span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span><br><span class="line">            Training vectors, where n_samples is the number of samples and</span><br><span class="line">            n_features is the number of features.</span><br><span class="line">        y : array-like, shape = [n_samples]</span><br><span class="line">            Target values.</span><br><span class="line"></span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        self : object</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.w_ = np.zeros(1 + X.shape[1])</span><br><span class="line">        self.cost_ = []</span><br><span class="line"></span><br><span class="line">        for i in range(self.n_iter):</span><br><span class="line">            net_input = self.net_input(X)</span><br><span class="line">            # Please note that the &quot;activation&quot; method has no effect</span><br><span class="line">            # in the code since it is simply an identity function. We</span><br><span class="line">            # could write `output = self.net_input(X)` directly instead.</span><br><span class="line">            # The purpose of the activation is more conceptual, i.e.,  </span><br><span class="line">            # in the case of logistic regression, we could change it to</span><br><span class="line">            # a sigmoid function to implement a logistic regression classifier.</span><br><span class="line">            output = self.activation(X)</span><br><span class="line">            errors = (y - output)</span><br><span class="line">            self.w_[1:] += self.eta * X.T.dot(errors)</span><br><span class="line">            self.w_[0] += self.eta * errors.sum()</span><br><span class="line">            cost = (errors**2).sum() / 2.0</span><br><span class="line">            self.cost_.append(cost)</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def net_input(self, X):</span><br><span class="line">        &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span><br><span class="line">        return np.dot(X, self.w_[1:]) + self.w_[0]</span><br><span class="line"></span><br><span class="line">    def activation(self, X):</span><br><span class="line">        &quot;&quot;&quot;Compute linear activation&quot;&quot;&quot;</span><br><span class="line">        return self.net_input(X)</span><br><span class="line"></span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span><br><span class="line">        return np.where(self.activation(X) &gt;= 0.0, 1, -1)</span><br></pre></td></tr></table></figure></p><p><strong>查看不同学习率下的错误率随迭代次数的变化情况：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))</span><br><span class="line"></span><br><span class="line"># 可视化W调整的过程中，错误率随迭代次数的变化</span><br><span class="line">ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)</span><br><span class="line">ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker=&apos;o&apos;)</span><br><span class="line">ax[0].set_xlabel(&apos;Epochs&apos;)</span><br><span class="line">ax[0].set_ylabel(&apos;log(Sum-squared-error)&apos;)</span><br><span class="line">ax[0].set_title(&apos;Adaline - Learning rate 0.01&apos;)</span><br><span class="line"></span><br><span class="line">ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)</span><br><span class="line">ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker=&apos;o&apos;)</span><br><span class="line">ax[1].set_xlabel(&apos;Epochs&apos;)</span><br><span class="line">ax[1].set_ylabel(&apos;Sum-squared-error&apos;)</span><br><span class="line">ax[1].set_title(&apos;Adaline - Learning rate 0.0001&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="http://i4.bvimg.com/647637/71c5d16340efe1bc.png" alt=""></p><p><strong>iris数据的应用情况：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 标准化特征</span><br><span class="line">X_std = np.copy(X)</span><br><span class="line">X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()</span><br><span class="line">X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()</span><br><span class="line"># 调用函数开始训练</span><br><span class="line">ada = AdalineGD(n_iter=15, eta=0.01)</span><br><span class="line">ada.fit(X_std, y)</span><br><span class="line"># 绘制效果</span><br><span class="line">plot_decision_regions(X_std, y, classifier=ada)</span><br><span class="line">plt.title(&apos;Adaline - Gradient Descent&apos;)</span><br><span class="line">plt.xlabel(&apos;sepal length [standardized]&apos;)</span><br><span class="line">plt.ylabel(&apos;petal length [standardized]&apos;)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"># 可视化W调整的过程中，错误率随迭代次数的变化</span><br><span class="line">plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=&apos;o&apos;)</span><br><span class="line">plt.xlabel(&apos;Epochs&apos;)</span><br><span class="line">plt.ylabel(&apos;Sum-squared-error&apos;)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="http://i4.bvimg.com/647637/505a411eac1ca35a.png" alt=""></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>1）<a href="https://blog.csdn.net/u013719780/article/details/51755409" target="_blank" rel="noopener">机器学习系列：感知器</a><br>2）<a href="https://blog.csdn.net/zyq522376829/article/details/66632699" target="_blank" rel="noopener">机器学习入门系列04，Gradient Descent（梯度下降法）</a><br>3）<a href="https://zhuanlan.zhihu.com/p/27449596?utm_source=weibo&amp;utm_medium=social" target="_blank" rel="noopener">一文看懂各种神经网络优化算法：从梯度下降到Adam方法</a><br>4）<a href="https://blog.csdn.net/huakai16/article/details/77701020" target="_blank" rel="noopener">机器学习与神经网络（三）：自适应线性神经元的介绍和Python代码实现</a><br>5）<a href="http://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb" target="_blank" rel="noopener">《Training Machine Learning Algorithms for Classification》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/62602/&quot;&gt;机器学习(三) 模型结果应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/21484/&quot;&gt;机器学习(四) 常见算法优缺点&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文章结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;什么是感知器分类算法&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;在Python中实现感知器学习算法&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;在iris（鸢尾花）数据集上训练一个感知器模型&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;自适应线性神经元和融合学习&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;使用梯度下降方法来最小化损失函数&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;在Python中实现一个自适应的线性神经元&lt;/em&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据分析" scheme="https://paradoxallen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="https://paradoxallen.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(四) 常见算法优缺点</title>
    <link href="https://paradoxallen.github.io/21484/"/>
    <id>https://paradoxallen.github.io/21484/</id>
    <published>2018-06-02T16:00:00.000Z</published>
    <updated>2018-06-02T17:24:48.011Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p><a href="https://paradoxallen.github.io/62602/">机器学习(三) 模型结果应用</a></p><p>机器学习算法我们了解了很多，但是放在一起来比较优缺点是缺少的，本篇文章就一些常见的算法来进行一次优缺点梳理。</p><a id="more"></a><hr><h3 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a><strong>决策树算法</strong></h3><h4 id="一、决策树优点"><a href="#一、决策树优点" class="headerlink" title="一、决策树优点"></a><strong>一、决策树优点</strong></h4><p>1、决策树易于理解和解释，可以可视化分析，容易提取出规则。</p><p>2、可以同时处理标称型和数值型数据。</p><p>3、测试数据集时，运行速度比较快。</p><p>4、决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。</p><h4 id="二、决策树缺点"><a href="#二、决策树缺点" class="headerlink" title="二、决策树缺点"></a><strong>二、决策树缺点</strong></h4><p>1、对缺失数据处理比较困难。</p><p>2、容易出现过拟合问题。</p><p>3、忽略数据集中属性的相互关联。</p><p>4、ID3算法计算信息增益时结果偏向数值比较多的特征。</p><h4 id="三、改进措施"><a href="#三、改进措施" class="headerlink" title="三、改进措施"></a><strong>三、改进措施</strong></h4><p>1、对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。</p><p>2、使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题</p><h4 id="四、常见算法"><a href="#四、常见算法" class="headerlink" title="四、常见算法"></a><strong>四、常见算法</strong></h4><h5 id="一）C4-5算法"><a href="#一）C4-5算法" class="headerlink" title="一）C4.5算法"></a><strong>一）C4.5算法</strong></h5><p>ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定的测试属性。</p><p>C4.5算法核心思想是ID3算法，是ID3算法的改进，改进方面有：</p><ul><li><p>用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；</p></li><li><p>在树构造过程中进行剪枝；</p></li><li><p>能处理非离散的数据；</p></li><li><p>能处理不完整的数据。</p></li></ul><p><strong>优点</strong>：产生的分类规则易于理解，准确率较高。</p><p><strong>缺点</strong>：</p><p>1）在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效；</p><p>2）C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</p><h5 id="二）CART分类与回归树"><a href="#二）CART分类与回归树" class="headerlink" title="二）CART分类与回归树"></a><strong>二）CART分类与回归树</strong></h5><p>是一种决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数<br>据集生成的决策树的拓展形。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。</p><p><strong>优点</strong></p><p>1）非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。</p><p>2）在面对诸如存在缺失值、变量数多等问题时CART 显得非常稳健。</p><hr><h3 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a><strong>分类算法</strong></h3><h4 id="一、KNN算法"><a href="#一、KNN算法" class="headerlink" title="一、KNN算法"></a><strong>一、KNN算法</strong></h4><p><strong>KNN算法的优点</strong> </p><p>1、KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练</p><p>2、KNN理论简单，容易实现</p><p><strong>KNN算法的缺点</strong></p><p>1、对于样本容量大的数据集计算量比较大。</p><p>2、样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。</p><p>3、KNN每一次分类都会重新进行一次全局运算。</p><p>4、k值大小的选择。</p><p><strong>KNN算法应用领域</strong></p><p>文本分类、模式识别、聚类分析，多分类领域</p><h4 id="二、支持向量机（SVM）"><a href="#二、支持向量机（SVM）" class="headerlink" title="二、支持向量机（SVM）"></a><strong>二、支持向量机（SVM）</strong></h4><p>支持向量机是一种基于分类边界的方法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。基于分类边界的分类算法的目标是，通过训练，找到这些分类之间的边界（直线的――称为线性划分，曲线的――称为非线性划分）。对于多维数据（如N维），可以将它们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面（超面比N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。</p><p>支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。</p><p><strong>SVM优点</strong></p><p>1、解决小样本下机器学习问题。<br>2、解决非线性问题。<br>3、无局部极小值问题。（相对于神经网络等算法）<br>4、可以很好的处理高维数据集。<br>5、泛化能力比较强。</p><p><strong>SVM缺点</strong></p><p>1、对于核函数的高维映射解释力不强，尤其是径向基函数。<br>2、对缺失数据敏感。</p><p><strong>SVM应用领域</strong></p><p>文本分类、图像识别、主要二分类领域</p><h4 id="三、朴素贝叶斯算法"><a href="#三、朴素贝叶斯算法" class="headerlink" title="三、朴素贝叶斯算法"></a><strong>三、朴素贝叶斯算法</strong></h4><p><strong>朴素贝叶斯算法优点</strong></p><p>1、对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。<br>2、支持增量式运算。即可以实时的对新增的样本进行训练。<br>3、朴素贝叶斯对结果解释容易理解。</p><p><strong>朴素贝叶斯缺点</strong></p><p>1、由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。</p><p><strong>朴素贝叶斯应用领域</strong></p><p>文本分类、欺诈检测中使用较多</p><h4 id="四、Logistic回归算法"><a href="#四、Logistic回归算法" class="headerlink" title="四、Logistic回归算法"></a><strong>四、Logistic回归算法</strong></h4><p><strong>logistic回归优点</strong></p><p>1、计算代价不高，易于理解和实现</p><p><strong>logistic回归缺点</strong></p><p>1、容易产生欠拟合。</p><p>2、分类精度不高。</p><p><strong>logistic回归应用领域</strong></p><p>用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。</p><p>Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。</p><hr><h3 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a><strong>聚类算法</strong></h3><h4 id="一、K-means-算法"><a href="#一、K-means-算法" class="headerlink" title="一、K means 算法"></a><strong>一、K means 算法</strong></h4><p>是一个简单的聚类算法，把n的对象根据他们的属性分为k个分割，k&lt; n。 算法的核心就是要优化失真函数J,使其收敛到局部最小值但不是全局最小值。<br>其中N为样本数，K是簇数，rnk b表示n属于第k个簇，uk 是第k个中心点的值。然后求出最优的uk</p><p><strong>优点</strong>：算法速度很快</p><p><strong>缺点</strong>：分组的数目k是一个输入参数，不合适的k可能返回较差的结果。</p><h4 id="二、EM最大期望算法"><a href="#二、EM最大期望算法" class="headerlink" title="二、EM最大期望算法"></a><strong>二、EM最大期望算法</strong></h4><p>EM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。</p><p>EM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。</p><hr><h3 id="集成算法（AdaBoost算法）"><a href="#集成算法（AdaBoost算法）" class="headerlink" title="集成算法（AdaBoost算法）"></a><strong>集成算法（AdaBoost算法）</strong></h3><h4 id="一、-AdaBoost算法优点"><a href="#一、-AdaBoost算法优点" class="headerlink" title="一、  AdaBoost算法优点"></a><strong>一、  AdaBoost算法优点</strong></h4><p>1、很好的利用了弱分类器进行级联。</p><p>2、可以将不同的分类算法作为弱分类器。</p><p>3、AdaBoost具有很高的精度。</p><p>4、相对于bagging算法和Random Forest算法，AdaBoost充分考虑的每个分类器的权重。</p><h4 id="二、Adaboost算法缺点"><a href="#二、Adaboost算法缺点" class="headerlink" title="二、Adaboost算法缺点"></a><strong>二、Adaboost算法缺点</strong></h4><p>1、AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。</p><p>2、数据不平衡导致分类精度下降。</p><p>3、训练比较耗时，每次重新选择当前分类器最好切分点。</p><h4 id="三、AdaBoost应用领域"><a href="#三、AdaBoost应用领域" class="headerlink" title="三、AdaBoost应用领域"></a><strong>三、AdaBoost应用领域</strong></h4><p>模式识别、计算机视觉领域，用于二分类和多分类场景</p><hr><h3 id="人工神经网络算法"><a href="#人工神经网络算法" class="headerlink" title="人工神经网络算法"></a><strong>人工神经网络算法</strong></h3><h4 id="一、神经网络优点"><a href="#一、神经网络优点" class="headerlink" title="一、神经网络优点"></a><strong>一、神经网络优点</strong></h4><p>1、分类准确度高，学习能力极强。</p><p>2、对噪声数据鲁棒性和容错性较强。</p><p>3、有联想能力，能逼近任意非线性关系。</p><h4 id="二、神经网络缺点"><a href="#二、神经网络缺点" class="headerlink" title="二、神经网络缺点"></a><strong>二、神经网络缺点</strong></h4><p>1、神经网络参数较多，权值和阈值。</p><p>2、黑盒过程，不能观察中间结果。</p><p>3、学习过程比较长，有可能陷入局部极小值。</p><h4 id="三、人工神经网络应用领域"><a href="#三、人工神经网络应用领域" class="headerlink" title="三、人工神经网络应用领域"></a><strong>三、人工神经网络应用领域</strong></h4><p>目前深度神经网络已经应用与计算机视觉，自然语言处理，语音识别等领域并取得很好的效果。</p><hr><h3 id="排序算法（PageRank）"><a href="#排序算法（PageRank）" class="headerlink" title="排序算法（PageRank）"></a><strong>排序算法（PageRank）</strong></h3><p>PageRank是google的页面排序算法，是基于从许多优质的网页链接过来的网页，必定还是优质网页的回归关系，来判定所有网页的重要性。（也就是说，一个人有着越多牛X朋友的人，他是牛X的概率就越大。）</p><h4 id="一、PageRank优点"><a href="#一、PageRank优点" class="headerlink" title="一、PageRank优点"></a><strong>一、PageRank优点</strong></h4><p>完全独立于查询，只依赖于网页链接结构，可以离线计算。</p><h4 id="二、PageRank缺点"><a href="#二、PageRank缺点" class="headerlink" title="二、PageRank缺点"></a><strong>二、PageRank缺点</strong></h4><p>1）PageRank算法忽略了网页搜索的时效性。</p><p>2）旧网页排序很高，存在时间长，积累了大量的in-links，拥有最新资讯的新网页排名却很低，因为它们几乎没有in-links。</p><hr><h3 id="关联规则算法（Apriori算法）"><a href="#关联规则算法（Apriori算法）" class="headerlink" title="关联规则算法（Apriori算法）"></a><strong>关联规则算法（Apriori算法）</strong></h3><p>Apriori算法是一种挖掘关联规则的算法，用于挖掘其内含的、未知的却又实际存在的数据关系，其核心是基于两阶段频集思想的递推算法 。</p><p><strong>Apriori算法分为两个阶段：</strong></p><p>1）寻找频繁项集</p><p>2）由频繁项集找关联规则</p><p><strong>算法缺点：</strong></p><p>1）在每一步产生侯选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；</p><p>2） 每次计算项集的支持度时，都对数据库中    的全部记录进行了一遍扫描比较，需要很大的I/O负载。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1）Jason Brownlee  《How To Use Machine Learning Results》</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/62602/&quot;&gt;机器学习(三) 模型结果应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;机器学习算法我们了解了很多，但是放在一起来比较优缺点是缺少的，本篇文章就一些常见的算法来进行一次优缺点梳理。&lt;/p&gt;
    
    </summary>
    
      <category term="数据分析" scheme="https://paradoxallen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="https://paradoxallen.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(三) 模型结果应用</title>
    <link href="https://paradoxallen.github.io/62602/"/>
    <id>https://paradoxallen.github.io/62602/</id>
    <published>2018-06-01T16:00:00.000Z</published>
    <updated>2018-06-02T16:49:34.914Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p>当你有了一个相当不错的模型结果了，这个时间就需要上线应用了，但实际上这个过程也是需要注意很多东西的呢，比如汇报你的项目结果、上线计划沟通、上线后的监控等等，这都是相当重要的。</p><a id="more"></a><hr><p>永远要记得，建立模型只是为了解决业务问题，<strong>模型只是一个工具而已</strong>，所以，脱离具体业务场景的模型都是假的。所以，在一开始，就要对自己的目标进行明确，在做完了模型后再度审视自己目标，看下自己做出来的模型是否仍是解决这个目标。</p><p>根据你试图解决的问题类型，我们可以大致分为两种呈现方式：</p><ul><li><p><strong>报告汇报式</strong></p></li><li><p><strong>部署上线式</strong></p></li></ul><p>当然了，实际上更多的是两种方式的融合，即两个都需要做，那么下面我们就分别来说一下这两种方式在实际操作上都需要做些什么呗。</p><h2 id="报告汇报式"><a href="#报告汇报式" class="headerlink" title="报告汇报式"></a>报告汇报式</h2><p>一旦你发现了一个很不错的模型，并且训练的结果也很不错，你此时就需要总结这一切内容，并很好的展示给你的观众（可以是老板、客户或者是同事等），而此时如何完美地展示显得格外重要。</p><p>展示的最好方式我个人觉得是ppt，但有些地方更偏好于单页报告，不过也不影响，下面罗列的内容，都是可以在这两种方式内容上展示的，不管你现在做的模型是什么，比赛的、教程的，还是工作的，都可以试着去总结这些关键点，完成一次报告的撰写。</p><ul><li><p><strong><em>Context</em></strong> (Why): 定义问题存在的大背景，并且说明研究的动机或目的。</p></li><li><p><strong><em>Problem</em></strong> (Question): 简单扼要地把问题描述一个具体需要解决的问题并回答它。</p></li><li><p><strong><em>Solution</em></strong> (Answer): 简单扼要地描述关于上一个环节提出的问题的解决方案，而且要详细具体。</p></li><li><p><strong><em>Findings</em></strong>: 罗列一下你在建模过程中发现的一些有价值的点，比如在数据上的发现，又或者是原先方式的缺点及现有方式的优点，也可以是模型在性能方面的优势等等。</p></li><li><p><strong><em>Limitations</em></strong>: 考虑模型能力所不能覆盖的点，或者是方案不能解决的点。不要回避这些问题，你不说别人也会问，而且，只有你重新认识模型的短处，才能知道模型的优点。</p></li><li><p><strong><em>Conclusions</em></strong> (Why+Question+Answer): 回顾目的、研究的问题及解决方案，并将它们尽可能压缩在几句话，让人能够记住。</p></li></ul><p>如果是自己平时做练习的项目，我觉得可以多按照上面的点来描述自己的项目结果，并且将报告上传到社区网络，让更多的人来评价，你从中也可以得到更多的反馈，这对你下一次的报告有很大的帮助。</p><h2 id="部署上线式"><a href="#部署上线式" class="headerlink" title="部署上线式"></a>部署上线式</h2><p>同样的，你有一个训练得很不错的模型，这时候需要将它部署到生产系统中，你需要确定很多东西，比如调用的环节、入参出参以及各种接口开发，下面有3个方面的内容需要在做这些事情之前进行考虑，分别是：<strong>算法实现、模型自动化测试、模型效果追踪</strong>。</p><p><strong>1）算法实现</strong></p><p>其实python里有很多算法都是可以直接通过库来调用的，但这对于一般情况下是很好用的，但是如果涉及到要具体部署应用，这要考虑的东西就多了。</p><p>在你考虑部署一个新模型在现有的生产系统上，你需要非常仔细地研究这可能需要产生的依赖项和“技术负债”（这里可以理解为一些所需的技术，包括硬软件）。所以，在建模前，需要考虑去查找能匹配你的方法的公司生产级别的库，要不然，等到要上线的时候，你就需要重复模型调优的过程了哦。</p><p><strong>2）模型自动化测试</strong></p><p>编写自动化测试代码，对模型的应用进行验证，监控在实际的使用过程中，并且能够重复实现模型效果的最低水平，尽可能是可以对不同的数据都可以随机性地测试。</p><p><strong>3）模型效果追踪</strong></p><p>增添一些基础设施来监控模型的性能，并且可以在精度低于最低水平的时候发出警报，追踪模型实时或者离线的数据样本的效果，包括入参。当你发现不仅仅是模型效果发生了很大的变化，就连入参也有很大的变化，那这个时候就需要考虑模型的更新或者重构了。</p><p>当然，有一些模型是可以实现在线自我学习并且更新自己的，但并不是所有的生产系统可以支持这种操作，毕竟这种还只是一个比较先进的办法，仍存在很多不太完善的地方。比较传统的方式还是对现有的模型进行人工管理，人工更新与切换，这样子显得更加明智而且稳健。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1）Jason Brownlee  《How To Use Machine Learning Results》</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当你有了一个相当不错的模型结果了，这个时间就需要上线应用了，但实际上这个过程也是需要注意很多东西的呢，比如汇报你的项目结果、上线计划沟通、上线后的监控等等，这都是相当重要的。&lt;/p&gt;
    
    </summary>
    
      <category term="数据分析" scheme="https://paradoxallen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型" scheme="https://paradoxallen.github.io/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(二) 模型调优</title>
    <link href="https://paradoxallen.github.io/4840/"/>
    <id>https://paradoxallen.github.io/4840/</id>
    <published>2018-05-31T16:00:00.000Z</published>
    <updated>2018-06-02T16:52:31.603Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一)算法介绍</a></p><p>前面讲了一些机器学习的算法的介绍，如果有一些数据这时候也可以建立出自己的模型了，但是，如果模型的效果不尽人意，那么应该如何调整呢？</p><p>以下是一份关于模型调优的方法，每当出现效果不好的时候或者是在建模前，都可以按照这个来进行检查，话不多说，一起来看～</p><p><img src="http://i2.bvimg.com/647637/9208883f4c7aeb45.jpg" alt=""></p><a id="more"></a><hr><p>为了文章阅读的清晰，先在前面简单说明一下文章的目录框架。</p><p>本文存在的意义在于辅助大家提升机器学习模型的效果，方法有很多，如果你在其中的一个办法中找到了突破，仍可以回头再找其他，直到满足你的模型效果要求，主要从4个角度来进行方法的阐述，分别是：</p><ul><li>Improve Performance With Data.（数据）</li><li>Improve Performance With Algorithms.（算法选择）</li><li>Improve Performance With Algorithm Tuning.（算法调优）</li><li>Improve Performance With Ensembles.（效果集成）</li></ul><p>好的，下面就分别从这4个角度来说一下。</p><h3 id="1-Improve-Performance-With-Data（数据）"><a href="#1-Improve-Performance-With-Data（数据）" class="headerlink" title="1. Improve Performance With Data（数据）"></a>1. Improve Performance With Data（数据）</h3><p>事实上，你直接改变训练数据或者改变目标的定义，好效果会来得更加“不费吹灰之力”，有的时候还可能是最好的操作，所以有一句话说得很有道理：<strong>选择比努力重要哈哈哈</strong>。</p><p>话不多说，说下Strategy: 创建不同的目标样本并且尽量使用最底层的特征来训练模型算法。</p><p><strong>具体策略：</strong></p><p>获得更多的数据：一个好的深度学习模型需要更多的数据来训练，其他非线性的机器学习模型也是如此。</p><ul><li>开发更多变量：如果你实在不能获得更多的数据亦或是更好质量的数据，也许可以通过概率模型、统计模型来生成新的变量。</li><li>清洗数据：通过数据清洗，你可以对缺失值、异常值进行合理的填补与修复，从而提升数据整体的质量。</li><li>重新采样数据：其实可以通过对数据的重新采样来改变数据的分布和大小。对某一特定类型的样本进行采样，说不定可以更好滴表现出效果。又或者是使用更小的数据，从而加快速度。</li><li>问题解决思路的重新思考：有的时候，你可以把你目前正在“焦头烂耳”想要解决的“预测性”问题，换成回归、分类、时间序列、异常检测、排序、推荐等等的问题。</li><li>调整变量再入模：这里指的是对数据进行离散化、标准化等的操作。</li><li>改变现有的变量：这里相信大家也很常见，就是对变量进行对数转换或指数转换，让其特性能更好地表现。</li><li>对变量进行降维：有的时候降维后的变量有更好的表现哦。</li><li>特征选择：这个就是特征工程了，简单来说，就是你对特征（变量）进行重要性的排序，选择相对预测力强的特征进入模型。</li></ul><h3 id="2-Improve-Performance-With-Algorithms（算法选择）"><a href="#2-Improve-Performance-With-Algorithms（算法选择）" class="headerlink" title="2. Improve Performance With Algorithms（算法选择）"></a>2. Improve Performance With Algorithms（算法选择）</h3><p>机器学习其实都是关于算法的学习。</p><p>Strategy: 识别出优于平均值的算法，但要对其实验过程以及结果抱着怀疑态度，并反复思考。</p><p><strong>具体策略：</strong></p><ul><li>重采样方法：使用什么方法来估计效果？有个原则就是要充分利用可用的数据来验证，这里，k-fold交叉验证方法可以是最好的哦。</li><li>评价指标：不同的目标需要使用不同的评价指标，这个相信大家在学习混淆矩阵的时候应该有所了解，什么pv+，命中率等等，都是对于特定类型的目标有着非常有效的识别。如果是一个排序性的问题，而你却用了准确度的指标来衡量模型的好坏似乎也说不过去把？</li><li>关注线性算法：线性算法通常会不那么好用，但是却更好地被人类理解且可以快速测试，如果你发现某个线性算法表现地还行，请继续优化它。</li><li>关注非线性算法：非线性算法往往会需要更多的数据，通过更加复杂的计算来获得一个不错的效果。</li><li>从文献中找ideas：这个方法还经常做，从文献中可以了解到更多的经典算法在特定需要下的应用，通过对文献的阅读来扩充你的“解题”思路把。</li></ul><h3 id="3-Improve-Performance-With-Algorithm-Tuning（算法调优）"><a href="#3-Improve-Performance-With-Algorithm-Tuning（算法调优）" class="headerlink" title="3. Improve Performance With Algorithm Tuning（算法调优）"></a>3. Improve Performance With Algorithm Tuning（算法调优）</h3><p>模型调参也是一个非常费时间的环节，有的时候“好运”可以马上抽查出表现还不错的结果，并持续调参，就可以得到一个不错的结果。但如果要对其他所有的算法进行优化，那么需要的时间就可能是几天、几个星期或者几个月了。</p><p>Strategy: 充分利用性能良好的机器学习算法。</p><p><strong>具体策略：</strong></p><ul><li>诊断方法：不同的算法需要提供不同的可视化和诊断的方法。</li><li>调参的直觉：这个就很“玄学”了，但其实都是一些经验，当你调的参足够多，也可以大致可以对这些不同算法的参数有了自己的理解，自然就有了这些所谓的“直觉”。</li><li>随机搜索：在N维参数空间按某种分布（如正态分布）随机取值，因为参数空间的各个维度的重要性是不等的，随机搜索方法可以在不重要的维度上取巧。</li><li>网格搜索：先固定一个超参，然后对其他各个超参依次进行穷举搜索。</li><li>从文献中找ideas：从文献中了解这个算法用到了哪些算法，并且这些算法主要的取值值域，有益于自身工作的开展哦。</li><li>从知名网站中找ideas：国内我个人觉得知乎还是蛮可以的，关于这节的参数调参，也是有好多好文章，其外还有csdn也不错。</li></ul><h3 id="4-Improve-Performance-With-Ensembles（效果集成）"><a href="#4-Improve-Performance-With-Ensembles（效果集成）" class="headerlink" title="4. Improve Performance With Ensembles（效果集成）"></a>4. Improve Performance With Ensembles（效果集成）<code></code></h3><p>这个算法集成的方法也是非常常用的，你可以结合多个模型的结果，综合输出一个更加稳定且效果不错的结果。</p><p>Strategy: 结合各种模型的预测结果并输出。</p><p><strong>具体策略：</strong></p><ul><li>混合模型的预测值：你可以把多个模型的预测结果结合起来，你可以将多个训练效果还不错的模型的预测结合综合起来，输出一个“平均”结果。</li><li>混合不同数据的预测值：你也可以把不同的数据集训练出来模型的结果进行结合，作为一个输出。（这个与上面的区别在于数据集的特征不同）</li><li>混合数据样本：很拗口，其实意思就是将数据集拆分成不同的子数据集，用于训练同一个算法，最后输出综合的预测结果。这个也被称之为bootstrap aggregation 或 bagging。</li><li>使用模型的方法集成：你也可以使用一个新的模型来学习如何结合多个性能不错的模型结果，输出一个最优的结合。这被称之为堆叠泛化或叠加，通常在子模型有技巧时很有效，但在不同的方式下，聚合器模型是预测的一个简单的线性加权。这个过程可以重复多层深度。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1）<a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" target="_blank" rel="noopener">A Tour of Machine Learning Algorithms</a></p><p>2）<a href="https://www.zhihu.com/question/34470160?sort=created" target="_blank" rel="noopener">机器学习各种算法怎么调参?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一)算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前面讲了一些机器学习的算法的介绍，如果有一些数据这时候也可以建立出自己的模型了，但是，如果模型的效果不尽人意，那么应该如何调整呢？&lt;/p&gt;
&lt;p&gt;以下是一份关于模型调优的方法，每当出现效果不好的时候或者是在建模前，都可以按照这个来进行检查，话不多说，一起来看～&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://i2.bvimg.com/647637/9208883f4c7aeb45.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="数据分析" scheme="https://paradoxallen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模型" scheme="https://paradoxallen.github.io/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>机器学习(一) 算法介绍</title>
    <link href="https://paradoxallen.github.io/9731/"/>
    <id>https://paradoxallen.github.io/9731/</id>
    <published>2018-05-30T16:00:00.000Z</published>
    <updated>2018-06-01T12:00:14.366Z</updated>
    
    <content type="html"><![CDATA[<p>前阵子阅读了院里一位博士研究生师兄的一篇有关基于神经网络算法对北京近五年的常规探空数据进行自组织分类，并揭示出大气污染物在不同边界层结构下的演变规律和相关机制的文章<a href="https://www.atmos-chem-phys.net/18/6771/2018/" target="_blank" rel="noopener">《Self-organized classification of boundary layer meteorology and associated characteristics of air quality in Beijing》</a>，看完顿时心生膜拜之情；然后恰巧也是那个时候吕教授在院群上也转发了一篇关于机器学习预测火势甚至天气的公众号文章<a href="https://mp.weixin.qq.com/s?__biz=MjM5ODE1NDYyMA==&amp;mid=2653384819&amp;idx=2&amp;sn=523f27cb9442ab4af27137edd1280248&amp;chksm=bd1cc8608a6b4176d7cae939f794e082cf86b5bf0ac0deb53790f507f563f588b2148687957c&amp;mpshare=1&amp;scene=1&amp;srcid=0517RYcQWk5dWJCcqoI7jLCe#rd" target="_blank" rel="noopener">《机器学习成功解决“蝴蝶效应”！以后你终于可以相信天气预报了》</a>。</p><p>如此的机缘巧合，我自己也想在这一方向进行深入了解，从基本的概述理论方法开始。</p><a id="more"></a><hr><p>算法很多，如何做好分组有助于我们更好记住它们，主要有2条算法分组的方式：</p><ul><li><p>The first is a grouping of algorithms by the learning style.（通过算法的学习方式）</p></li><li><p>The second is a grouping of algorithms by similarity in form or function (like grouping similar animals together).（通过算法的功能）</p></li></ul><p>下面就会从这2个角度来阐述一下机器学习的算法。</p><p><img src="http://i4.bvimg.com/647637/c8333c245bbeada6.png" alt=""></p><h2 id="Algorithms-Grouped-by-Learning-Style（通过算法的学习方式）"><a href="#Algorithms-Grouped-by-Learning-Style（通过算法的学习方式）" class="headerlink" title="Algorithms Grouped by Learning Style（通过算法的学习方式）"></a>Algorithms Grouped by Learning Style（通过算法的学习方式）</h2><p>关于机器学习算法，有三种不同的学习方式：</p><h3 id="1-Supervised-Learning（监督学习）"><a href="#1-Supervised-Learning（监督学习）" class="headerlink" title="1. Supervised Learning（监督学习）"></a>1. Supervised Learning（监督学习）</h3><p>当输入的数据集（我们称之为训练集）的数据有标签，如好坏标签，分类标签等，那么通过这些数据来建立的预测或者分类模型，属于监督学习模型。</p><ul><li><p>经典问题：classification and regression.（分类与回归）</p></li><li><p>经典算法：Logistic Regression and the Back Propagation Neural Network.（逻辑回归算法与BP神经网络算法）</p></li></ul><p><img src="http://i4.bvimg.com/647637/98402ff904a2e723.png" alt=""></p><h3 id="2-Unsupervised-Learning（无监督学习）"><a href="#2-Unsupervised-Learning（无监督学习）" class="headerlink" title="2. Unsupervised Learning（无监督学习）"></a>2. Unsupervised Learning（无监督学习）</h3><p>与监督学习相反，训练集中的数据并没有标签，这意味着你需要从这堆没有标签的数据中去提炼它们的特点规则等等，可能是通过数学推理过程来系统地减少冗余，又或者是通过数据相似度来组织数据。</p><ul><li><p>经典问题：clustering, dimensionality reduction and association rule learning.（聚类、降维、规则学习）</p></li><li><p>经典算法：the Apriori algorithm and k-Means.（这个专用名词就不翻译了）</p></li></ul><p><img src="http://i4.bvimg.com/647637/1559c4d4bbbd73c2.png" alt=""></p><h3 id="3-Semi-Supervised-Learning（半监督学习）"><a href="#3-Semi-Supervised-Learning（半监督学习）" class="headerlink" title="3. Semi-Supervised Learning（半监督学习）"></a>3. Semi-Supervised Learning（半监督学习）</h3><p>顾名思义，半监督学习意味着训练数据有一部分有标签，而一些没有，一般而言，当训练数据量过少时，监督学习得到的模型效果不能满足需求，因此用半监督学习来增强效果。</p><ul><li><p>经典问题：classification and regression.</p></li><li><p>经典算法：半监督SVM，高斯模型，KNN模型</p></li></ul><p><img src="http://i4.bvimg.com/647637/fc0034f6bf008c55.png" alt=""></p><h2 id="Algorithms-Grouped-By-Similarity（通过算法的功能）"><a href="#Algorithms-Grouped-By-Similarity（通过算法的功能）" class="headerlink" title="Algorithms Grouped By Similarity（通过算法的功能）"></a>Algorithms Grouped By Similarity（通过算法的功能）</h2><p>根据算法的功能相似性来区分算法也是一种很好的办法，如基于树结构的算法或者基于神经网络的算法。所以我觉得从这个角度来了解这些算法会更加好。<br>即便这是一个很好的方式，但也绝非完美，仍会有一些算法不能简单地被归类，比如Learning Vector Quantization（LVQ，学习矢量量化算法），它既是神经网络，也是基于距离的算法，所以下面的归类也只是适用于大多数算法，但是常用的算法。</p><h3 id="1-Regression-Algorithms（回归算法）"><a href="#1-Regression-Algorithms（回归算法）" class="headerlink" title="1. Regression Algorithms（回归算法）"></a>1. Regression Algorithms（回归算法）</h3><p>回归更多地关注自变量与因变量之间的关系，并通过对误差的测算来建模，回归算法是对于数学统计的一个很好应用，也被纳入统计机器学习中。</p><p>常见的回归算法包括：</p><ul><li><p>Ordinary Least Squares Regression (OLSR，普通最小二乘回归)</p></li><li><p>Linear Regression（线性回归）</p></li><li><p>Logistic Regression（逻辑回归）</p></li><li><p>Stepwise Regression（逐步回归）</p></li><li><p>Adaptive Regression Splines (MARS，多元自适应回归)</p></li><li><p>Locally Estimated Scatterplot Smoothing (LOESS，本地散点平滑估计)</p></li></ul><p><img src="http://i4.bvimg.com/647637/7d5edfbfdacf0ce5.png" alt=""></p><h3 id="2-Instance-based-Algorithms（基于距离的算法）"><a href="#2-Instance-based-Algorithms（基于距离的算法）" class="headerlink" title="2. Instance-based Algorithms（基于距离的算法）"></a>2. Instance-based Algorithms（基于距离的算法）</h3><p>基于距离学习的模型非常常见，这类的模型是对训练集数据进行建模并比较新数据与之的距离，而距离的衡量有很多，常见的是欧氏距离、曼哈顿距离等。</p><p>常见的算法包括：</p><ul><li><p>k-Nearest Neighbor (kNN)</p></li><li><p>Learning Vector Quantization (LVQ，学习矢量量化)</p></li><li><p>Self-Organizing Map (SOM，自组织映射)</p></li><li><p>Locally Weighted Learning (LWL，局部加权学习)</p></li></ul><p><img src="http://i4.bvimg.com/647637/7bde9d592d1937db.png" alt=""></p><h3 id="3-Regularization-Algorithms（正则化算法）"><a href="#3-Regularization-Algorithms（正则化算法）" class="headerlink" title="3. Regularization Algorithms（正则化算法）"></a>3. Regularization Algorithms（正则化算法）</h3><p>正则化是对另一种方法(通常是回归方法)的扩展，使基于其复杂性的模型受到惩罚，支持更简单的模型，这些模型在泛化能力方面也比较好。</p><p>常见的正则化算法包括：<br>Ridge Regression（岭回归算法）<br>Least Absolute Shrinkage and Selection Operator (LASSO算法，稀疏约束)<br>Elastic Net（弹性网络）<br>Least-Angle Regression (LARS，最小角回归算法)</p><p><img src="http://i4.bvimg.com/647637/c6951c24ab35ec34.png" alt=""></p><h3 id="4-Decision-Tree-Algorithms（决策树算法）"><a href="#4-Decision-Tree-Algorithms（决策树算法）" class="headerlink" title="4. Decision Tree Algorithms（决策树算法）"></a>4. Decision Tree Algorithms（决策树算法）</h3><p>决策树方法构建基于数据中属性的实际值来建模的，决策树经常被训练用于分类和回归问题，决策树通常是快速和准确的，并且是机器学习中最受欢迎的。</p><p>常见的决策树算法包括：</p><ul><li><p>Classification and Regression Tree (CART，分类回归树算法)</p></li><li><p>Iterative Dichotomiser 3 (ID3)</p></li><li><p>C4.5 and C5.0 (不同版本的区别)</p></li><li><p>Chi-squared Automatic Interaction Detection (CHAID)</p></li><li><p>Decision Stump（决策树桩）</p></li><li><p>MD5（Message-Digest Algorithm，讯息摘要算法）</p></li><li><p>Decision Trees（条件决策树）</p></li></ul><p><img src="http://i4.bvimg.com/647637/822d6578c331ab38.png" alt=""></p><h3 id="5-Bayesian-Algorithms（贝叶斯算法）"><a href="#5-Bayesian-Algorithms（贝叶斯算法）" class="headerlink" title="5. Bayesian Algorithms（贝叶斯算法）"></a>5. Bayesian Algorithms（贝叶斯算法）</h3><p>基于贝叶斯定理的方式来构建的算法，常用语分类与回归问题。</p><p>常见的贝叶斯算法包括：</p><ul><li><p>Naive Bayes（朴素贝叶斯）</p></li><li><p>Gaussian Naive Bayes（高斯朴素贝叶斯）</p></li><li><p>Multinomial Naive Bayes（多项式朴素贝叶斯）</p></li><li><p>Averaged One-Dependence Estimators (AODE)</p></li><li><p>Belief Network (BBN，贝叶斯定理网络)</p></li><li><p>Bayesian Network (BN，贝叶斯网络)</p></li></ul><p><img src="http://i4.bvimg.com/647637/1c88f6233fffff9c.png" alt=""></p><h3 id="6-Clustering-Algorithms（聚类算法）"><a href="#6-Clustering-Algorithms（聚类算法）" class="headerlink" title="6. Clustering Algorithms（聚类算法）"></a>6. Clustering Algorithms（聚类算法）</h3><p>聚类分析又称群分析，它是研究（样品或指标）分类问题的一种统计分析方法，同时也是数据挖掘的一个重要算法。<br>聚类（Cluster）分析是由若干模式（Pattern）组成的，通常，模式是一个度量（Measurement）的向量，或者是多维空间中的一个点。<br>聚类分析以相似性为基础，在一个聚类中的模式之间比不在同一聚类中的模式之间具有更多的相似性。</p><p>常见的聚类算法包括：<br>k-Means<br>k-Medians<br>Expectation Maximisation (EM，Expectation Maximization Algorithm，是一种迭代算法)<br>Hierarchical Clustering（层次聚类）</p><p><img src="http://i4.bvimg.com/647637/ffcd3a96458da550.png" alt=""></p><h3 id="7-Association-Rule-Learning-Algorithms（关联规则学习算法）"><a href="#7-Association-Rule-Learning-Algorithms（关联规则学习算法）" class="headerlink" title="7. Association Rule Learning Algorithms（关联规则学习算法）"></a>7. Association Rule Learning Algorithms（关联规则学习算法）</h3><p>关联规则学习方法提取的规则最能解释数据中变量之间的关系，这些规则可以在大型多维数据集中发现重要和商业有用的关联，而被组织利用。</p><p>最常见的算法包括：</p><ul><li><p>Apriori algorithm</p></li><li><p>Eclat algorithm</p></li></ul><p><img src="http://i4.bvimg.com/647637/862e1ef2bd8cb2b6.png" alt=""></p><h3 id="8-Artificial-Neural-Network-Algorithms（人工神经网络算法）"><a href="#8-Artificial-Neural-Network-Algorithms（人工神经网络算法）" class="headerlink" title="8. Artificial Neural Network Algorithms（人工神经网络算法）"></a>8. Artificial Neural Network Algorithms（人工神经网络算法）</h3><p>人工神经网络是受生物神经网络结构和/或功能启发的模型，它们是一类模式匹配，通常用于回归和分类问题，但实际上是一个巨大的子字段，包含数百种算法和各种类型的问题类型。</p><p>最常见的算法包括：</p><ul><li><p>Perceptron（感知器）</p></li><li><p>Back-Propagation（反向传播法）</p></li><li><p>Hopfield Network（霍普菲尔网络）</p></li><li><p>Radial Basis Function Network (RBFN，径向基函数网络)</p></li></ul><p><img src="http://i4.bvimg.com/647637/f1c3d36096b5b753.png" alt=""></p><h3 id="9-Deep-Learning-Algorithms（深度学习算法）"><a href="#9-Deep-Learning-Algorithms（深度学习算法）" class="headerlink" title="9. Deep Learning Algorithms（深度学习算法）"></a>9. Deep Learning Algorithms（深度学习算法）</h3><p>深度学习方法是利用大量廉价计算的人工神经网络的更新，它关心的是构建更大更复杂的神经网络，正如上面所提到的，许多方法都与半监督学习问题有关，在这些问题中，大型数据集包含的标签数据非常少。</p><p>最常见的算法包括：</p><ul><li><p>Deep Boltzmann Machine (DBM)</p></li><li><p>Deep Belief Networks (DBN)</p></li><li><p>Convolutional Neural Network (CNN)</p></li><li><p>Stacked Auto-Encoders</p></li></ul><p><img src="http://i4.bvimg.com/647637/42c64e10881d9023.png" alt=""></p><h3 id="10-Dimensionality-Reduction-Algorithms（降维算法）"><a href="#10-Dimensionality-Reduction-Algorithms（降维算法）" class="headerlink" title="10. Dimensionality Reduction Algorithms（降维算法）"></a>10. Dimensionality Reduction Algorithms（降维算法）</h3><p>像聚类方法一样，维数的减少有利于寻找到数据的关联关系，但在这种情况下，是不受监督的方式，或者用较少的信息来概括或描述数据。<br>这些方法中的许多可以用于分类和回归。</p><p>常见的算法包括：</p><ul><li><p>Principal Component Analysis (PCA)</p></li><li><p>Principal Component Regression (PCR)</p></li><li><p>Partial Least Squares Regression (PLSR)</p></li><li><p>Sammon Mapping</p></li><li><p>Multidimensional Scaling (MDS)</p></li><li><p>Projection Pursuit</p></li><li><p>Linear Discriminant Analysis (LDA)</p></li><li><p>Mixture Discriminant Analysis (MDA)</p></li><li><p>Quadratic Discriminant Analysis (QDA)</p></li><li><p>Flexible Discriminant Analysis (FDA)</p></li></ul><p><img src="http://i4.bvimg.com/647637/aee45bf976e3f058.png" alt=""></p><h3 id="11-Ensemble-Algorithms（集成算法）"><a href="#11-Ensemble-Algorithms（集成算法）" class="headerlink" title="11. Ensemble Algorithms（集成算法）"></a>11. Ensemble Algorithms（集成算法）</h3><p>集成方法是由多个较弱的模型而组成的模型，这些模型是独立训练的，它们的预测在某种程度上是结合在一起来进行总体预测的。<br>这类算法是把更多精力放到了弱学习器身上，以及如何将它们结合起来。这是一门非常强大的技术，因此非常受欢迎。</p><p>常见的算法包括：</p><ul><li><p>Boosting</p></li><li><p>Bootstrapped Aggregation (Bagging)</p></li><li><p>AdaBoost</p></li><li><p>Stacked Generalization (blending)</p></li><li><p>Gradient Boosting Machines (GBM)</p></li><li><p>Gradient Boosted Regression Trees (GBRT)</p></li><li><p>Random Forest</p></li></ul><p><img src="http://i4.bvimg.com/647637/762ed312f28aa7be.png" alt=""></p><h3 id="12-Other-Algorithms（其他算法）"><a href="#12-Other-Algorithms（其他算法）" class="headerlink" title="12. Other Algorithms（其他算法）"></a>12. Other Algorithms（其他算法）</h3><p>还有很多算法没有被覆盖到，大概还有下面的算法：</p><p>Feature selection algorithms（特征选择算法）</p><ul><li><p>Algorithm accuracy evaluation（算法精度估计）</p></li><li><p>Performance measures（效果评估）</p></li><li><p>Computational intelligence (evolutionary algorithms, etc.)</p></li><li><p>Computer Vision (CV)</p></li><li><p>Natural Language Processing (NLP)</p></li><li><p>Recommender Systems</p></li><li><p>Reinforcement Learning</p></li><li><p>Graphical Models</p></li><li><p>And more…</p></li></ul><p>Further Reading<br>网络上对这些算法有更加详细的讲解，需要大家自己动手去查了，这样子才会更加了解这些算法内容，本文内容来自网络，还有一些我觉得很有用的资料也在下面，大家可以抽时间去细细研究哈。</p><p>##参考资料<br>1）<a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" target="_blank" rel="noopener">A Tour of Machine Learning Algorithms</a></p><p>2）<a href="https://www.zhihu.com/question/20691338/answer/53910077" target="_blank" rel="noopener">机器学习该如何入门——张松阳的回答</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前阵子阅读了院里一位博士研究生师兄的一篇有关基于神经网络算法对北京近五年的常规探空数据进行自组织分类，并揭示出大气污染物在不同边界层结构下的演变规律和相关机制的文章&lt;a href=&quot;https://www.atmos-chem-phys.net/18/6771/2018/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Self-organized classification of boundary layer meteorology and associated characteristics of air quality in Beijing》&lt;/a&gt;，看完顿时心生膜拜之情；然后恰巧也是那个时候吕教授在院群上也转发了一篇关于机器学习预测火势甚至天气的公众号文章&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5ODE1NDYyMA==&amp;amp;mid=2653384819&amp;amp;idx=2&amp;amp;sn=523f27cb9442ab4af27137edd1280248&amp;amp;chksm=bd1cc8608a6b4176d7cae939f794e082cf86b5bf0ac0deb53790f507f563f588b2148687957c&amp;amp;mpshare=1&amp;amp;scene=1&amp;amp;srcid=0517RYcQWk5dWJCcqoI7jLCe#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《机器学习成功解决“蝴蝶效应”！以后你终于可以相信天气预报了》&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;如此的机缘巧合，我自己也想在这一方向进行深入了解，从基本的概述理论方法开始。&lt;/p&gt;
    
    </summary>
    
      <category term="数据分析" scheme="https://paradoxallen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="https://paradoxallen.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>统计推断(零) 章节简介</title>
    <link href="https://paradoxallen.github.io/5451/"/>
    <id>https://paradoxallen.github.io/5451/</id>
    <published>2018-03-31T16:00:00.000Z</published>
    <updated>2018-06-03T15:42:01.441Z</updated>
    
    <content type="html"><![CDATA[<p>《统计推断(翻译版·原书第2版)》从概率论的基础开始，通过例子与习题的旁征博引，引进了大量近代统计处理的新技术和一些国内同类教材中不常见而又广为使用的分布。</p><p>其内容既包括工科概率入门、经典统计和现代统计的基础，又加进了不少近代统计中数据处理的实用方法和思想，例如：Bootstrap再抽样法、刀切(Jackkrlife)估计、EM算法、Logistic回归、稳健(Robest)回归、Markov链、Monte Carlo方法等。</p><p>它的统计内容与国内流行的教材相比，理论较深，模型较多，案例的涉及面要广，理论的应用面要丰富，统计思想的阐述与算法更为具体。</p><p>《统计推断(翻译版·原书第2版)》可作为工科、管理类学科专业本科生、研究生的教材或参考书，也可供教师、工程技术人员自学之用。</p><a id="more"></a><hr><h3 id="章节简介"><a href="#章节简介" class="headerlink" title="章节简介"></a><strong>章节简介</strong></h3><p><strong>出版说明</strong><br><strong>第2版序</strong><br><strong>第1版序</strong><br><strong>译后序</strong><br><strong>第1章 概率论</strong><br>1.1 集合论<br>1.2 概率论基础<br>1.2.1 公理化基础<br>1.2.2 概率演算<br>1.2.3 计数<br>1.2.4 枚举结果<br>1.3 条件概率与独立性<br>1.4 随机变量<br>1.5 分布函数<br>1.6 概率密度函数和概率质量函数<br>1.7 习题<br>1.8 杂录<br><strong>第2章 变换和期望</strong><br>2.1 随机变量函数的分布<br>2.2 期望<br>2.3 矩和矩母函数<br>2.4 积分号下的求导<br>2.5 习题<br>2.6 杂录<br>2.6.1 矩列的唯一性<br>2.6.2 其他母函数<br>2.6.3 矩母函数能否唯一地确定分布？<br><strong>第3章 常见分布族</strong><br>3.1 引言<br>3.2 离散分布<br>3.3 连续分布<br>3.4 指数族<br>3.5 位置与尺度族<br>3.6 不等式与恒等式<br>3.6.1 概率不等式<br>3.6.2 恒等式<br>3.7 习题<br>3.8 杂录<br>3.8.1 Poisson假设<br>3.8.2 Chebychev不等式及其改进<br>3.8.3 再谈指数族<br><strong>第4章 多维随机变量</strong><br>4.1 联合分布与边缘分布<br>4.2 条件分布与独立性<br>4.3 二维变换<br>4.4 多层模型与混合分布<br>4.5 协方差与相关<br>4.6 多维分布<br>4.7 不等式<br>4.7.1 数值不等式<br>4.7.2 函数不等式<br>4.8 习题<br>4.9 杂录<br>4.9.1 交换悖论<br>4.9.2 算术－几何－调和平均值不等式<br>8.3.1 错误概率与功效函数<br>8.3.2 最大功效检验<br>8.3.3 并－检验与交－并检验的真实水平<br>8.3.4 P-值<br>8.3.5 损失函数最优性<br>8.4 习题<br>8.5 杂录<br>8.5.1 单调功效函数<br>8.5.2 似然比作为证据<br>8.5.3 P-值和后验概率<br>8.5.4 置信集P-值<br><strong>第9章 区间估计</strong><br>9.1 引言<br>9.2 区间估计量的求法<br>9.2.1 反转一个检验统计量<br>9.2.2 枢轴量<br>9.2.3 枢轴化累积分布函数<br>9.2.4 Bayes区间<br>9.3 区间估计量的评价方法<br>9.3.1 尺寸和覆盖概率<br>9.3.2 与检验相关的最优性<br>9.3.3 Bayes最优<br>9.3.4 损失函数最优<br>9.4 习题<br>9.5 杂录<br>9.5.1 置信方法<br>9.5.2 离散分布中的置信区间<br>9.5.3 Fieller定理<br>9.5.4 其他区间如何?<br><strong>第10章 渐近评价</strong><br>10.1 点估计<br>10.1.1 相合性<br>10.1.2 有效性<br>10.1.3 计算与比较<br>10.1.4 自助法标准误差<br>10.2 稳健性<br>10.2.1 均值和中位数<br>10.2.2 M_估计量<br>10.3 假设检验<br>10.3.1 LRT的渐近分布<br>10.3.2 其他大样本检验<br>10.4 区间估计<br>10.4.1 近似极大似然区间<br>10.4.2 其他大样本区间<br>10.5 习题<br>10.6 杂录<br>10.6.1 超有效性<br>10.6.2 适当的正则性条件<br>10.6.3 再谈自助法<br>10.6.4 影响函数<br>10.6.5 自助法区间<br>10.6.6 稳健区间<br><strong>第11章 方差分析和回归分析</strong><br>11.1 引言<br>11.2 一种方式分组的方差分析<br>11.2.1 模型和分布假定<br>11.2.2 经典的ANOVA假设<br>11.2.3 均值的线性组合的推断<br>11.2.4 ANOVAF检验<br>11.2.5 对比的同时估计<br>11.2.6 平方和的分解<br>11.3 简单线性回归<br>11.3.1 最小二乘：数学解<br>11.3.2 最佳线性无偏估计：统计解<br>11.3.3 模型和分布假定<br>11.3.4 正态误差下的估计和检验<br>11.3.5 在给定点x=x0处的估计和预测<br>11.3.6 同时估计和置信带<br>11.4 习题<br>11.5 杂录<br>11.5.1 Cochran定理<br>11.5.2 多重比较<br>11.5.3 随机化完全区组设计<br>11.5.4 其他类型的方差分析<br>11.5.5 置信带的形状<br>11.5.6 Stein悖论<br><strong>第12章 回归模型</strong><br>12.1 引言<br>12.2 变量有误差时的回归<br>12.2.1 函数关系和结构关系<br>12.2.2 最小二乘解<br>12.2.3 极大似然估计<br>12.2.4 置信集<br>12.3 罗吉斯蒂克回归<br>12.3.1 模型<br>12.3.2 估计<br>12.4 稳健回归<br>12.5 习题<br>12.6 杂录<br>12.6.1 函数和结构的意义<br>12.6.2 EIV模型中常规最小乘的相合性<br>12.6.3 EIV模型中的工具变量<br>12.6.4 罗吉斯蒂克似然方程<br>12.6.5 再谈稳健回归<br><strong>附录 计算机代数</strong><br><strong>常用分布表</strong><br><strong>参考文献</strong><br><strong>作者索引</strong><br><strong>名词索引</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《统计推断(翻译版·原书第2版)》从概率论的基础开始，通过例子与习题的旁征博引，引进了大量近代统计处理的新技术和一些国内同类教材中不常见而又广为使用的分布。&lt;/p&gt;
&lt;p&gt;其内容既包括工科概率入门、经典统计和现代统计的基础，又加进了不少近代统计中数据处理的实用方法和思想，例如：Bootstrap再抽样法、刀切(Jackkrlife)估计、EM算法、Logistic回归、稳健(Robest)回归、Markov链、Monte Carlo方法等。&lt;/p&gt;
&lt;p&gt;它的统计内容与国内流行的教材相比，理论较深，模型较多，案例的涉及面要广，理论的应用面要丰富，统计思想的阐述与算法更为具体。&lt;/p&gt;
&lt;p&gt;《统计推断(翻译版·原书第2版)》可作为工科、管理类学科专业本科生、研究生的教材或参考书，也可供教师、工程技术人员自学之用。&lt;/p&gt;
    
    </summary>
    
      <category term="应用统计" scheme="https://paradoxallen.github.io/categories/%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="统计" scheme="https://paradoxallen.github.io/tags/%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>博客搭建——利用GitHub+Hexo</title>
    <link href="https://paradoxallen.github.io/13431/"/>
    <id>https://paradoxallen.github.io/13431/</id>
    <published>2018-03-29T16:00:00.000Z</published>
    <updated>2018-06-03T09:33:15.395Z</updated>
    
    <content type="html"><![CDATA[<p>一直想有一个可以记录的属于自己的博客，最近在知乎上看到了利用<a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>+<a href="https://github.com/" target="_blank" rel="noopener">GitHub</a>搭建博客的教程，于是乎搭建了自己的博客<a href="https://paradoxallen.github.io/">paradoxallen</a>。</p><p>然而在此过程也遇到了一些bug，希望以下图文并茂的教程（虽然已经有挺多类似的教程…但主要是想自己写一篇文章!!!而且保证超级详细!!!）可以帮助大家少走弯路，搭建属于自己的博客。</p><a id="more"></a><hr><h2 id="GitHub配置"><a href="#GitHub配置" class="headerlink" title="GitHub配置"></a>GitHub配置</h2><p><a href="https://github.com/" target="_blank" rel="noopener">https://github.com/ </a>登录GitHub账号,如无GitHub帐号需要注册一个</p><p><img src="http://i1.bvimg.com/647637/c2c87e3a66d3f41c.png" alt=""></p><p>点击GitHub中的New repository创建新仓库</p><p><img src="http://i2.bvimg.com/647637/8cd2118322eeebb8.png" alt=""></p><p>仓库名应该为：用户名.<a href="http://github.io" target="_blank" rel="noopener">http://github.io</a> </p><p>这个用户名使用你的GitHub帐号名称代替，这是固定写法</p><p>如我的域名是<a href="http://github.com/paradoxallen" target="_blank" rel="noopener">github.com/paradoxallen</a>，就填入<a href="http://paradoxallen.github.io">paradoxallen.github.io</a></p><p>然后点击create repository创建仓库</p><p><img src="http://i2.bvimg.com/647637/7dee21bf61c6a269.png" alt=""></p><p>成功之后出现以下画面</p><p><img src="http://i4.bvimg.com/647637/9c9e7a4a292f769b.png" alt=""></p><h2 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h2><h3 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h3><p><a href="https://git-scm.com/download/win" target="_blank" rel="noopener">Git-Downloading Package</a>选择下载Windows版本的64位或32位的安装包（也有MacOSX版本和Linux/Unix版本的，按需求下载）</p><p><img src="http://i4.bvimg.com/647637/c8ab66c94a5b8923.png" alt=""></p><p>下载后安装，基本按默认安装就行，安装成功后，鼠标右键打开Git Bash</p><p><img src="http://i2.bvimg.com/647637/1a613567dfe1c628.png" alt=""></p><p>然后设置user.name和user.email配置信息<br><code>git config --global user.name &quot;你的GitHub用户名&quot;</code><br><code>git config --global user.email &quot;你的GitHub注册邮箱&quot;</code></p><p>生成ssh密钥文件：<br><code>ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot;</code></p><p><img src="http://i2.bvimg.com/647637/d7a3ae0382cff29c.png" alt=""></p><p>然后直接三个回车，无需设置密码</p><p>在找到生成的.ssh的文件夹中的id-rsa.pub密钥，将内容全部复制</p><p><img src="http://i2.bvimg.com/647637/71af645c415b05d6.png" alt=""></p><p>打开<a href="https://github.com/settings/keys/new" target="_blank" rel="noopener">GitHub-Settings-SSHandGPGkeys-newSHHkeys</a>新建</p><p><img src="http://i4.bvimg.com/647637/410f443eedb161d1.png" alt=""></p><p>Title随意，Key粘贴id-rsa.pub内容，最后点击Add SHH key</p><p>在Git Bash中检测GitHub公钥是否设置成功，输入<code>ssh git@github.com</code>如下则说明成功</p><p><img src="http://i4.bvimg.com/647637/2e82b66bd44b0528.png" alt=""></p><h3 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h3><p><a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">Download|Node.js</a>选择下载安装包，也是默认设置安装就好</p><p><img src="http://i4.bvimg.com/647637/c7552174b3bafe3b.png" alt=""></p><h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>这里需要在自己电脑中新建文件夹，如命名为blog</p><p>进入文件夹，按住shift键，右击鼠标点击在此处打开Powershell窗口</p><p>（有些教程是打开命令行，但好像是win10才有的powershell)</p><p><img src="http://i2.bvimg.com/647637/035c259ae9e63ed0.png" alt=""></p><p>然后依次输入：</p><p><code>npm install -g hexo-cli</code>安装Hexo</p><p><code>hexo init blog</code>初始化Hexo</p><p>成功提示<code>INFO  Start blogging with Hexo!</code></p><p>（因为我已经设置好就不重新初始化啦）</p><p>因为你初始化hexo 之后source目录下自带一篇hello world文章, 所以直接执行下方命令<br><code>hexo g</code> </p><p><code>hexo s</code>启动本地服务器</p><p>成功提示<br><code>INFO  Start processing</code><br><code>INFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.</code></p><p><img src="http://i4.bvimg.com/647637/e262c409d50dc6b2.png" alt=""></p><p>在浏览器输入<code>http://localhost:4000/</code>就可以看见网页和模板了（因为我修改过主题可能有点出入）</p><p><img src="http://i4.bvimg.com/647637/1a5c5f97074e6f7d.png" alt=""></p><h2 id="网站推送"><a href="#网站推送" class="headerlink" title="网站推送"></a>网站推送</h2><p>打开blog根目录_config.yml文件，将Hexo与GitHub关联起来</p><p><img src="http://i4.bvimg.com/647637/f4980b87d2c9f2ff.png" alt=""></p><p><code>deploy:</code></p><p><code>type: git</code></p><p><code>repo: GitHub上创建仓库的完整路径，加上 .git</code></p><p><code>branch: master</code></p><p>参考如下：</p><p><img src="http://i4.bvimg.com/647637/c150e9cde0adb7b3.png" alt=""></p><p>然后保存并执行命令：<br><code>npm install hexo-deployer-git --save</code></p><p>再依次输入三条命令:</p><p><code>hexo clean</code> </p><p><code>hexo g</code></p><p><code>hexo d</code></p><p>打开浏览器，在地址栏输入你的放置个人网站的仓库路径，即 <a href="http://xxxx.github.io，如[paradoxallen.github.io](https://paradoxallen.github.io/)即可访问" target="_blank" rel="noopener">http://xxxx.github.io，如[paradoxallen.github.io](https://paradoxallen.github.io/)即可访问</a></p><p>此外，也可以打开_config.yml文件修改参数信息，如排版格式等等</p><h2 id="文章发布"><a href="#文章发布" class="headerlink" title="文章发布"></a>文章发布</h2><p>输入：<code>hexo new &quot;testing&quot;</code>打开文件使用markdown语法输入文字</p><p>保存，执行：</p><p><code>hexo clean</code></p><p><code>hexo g</code></p><p><code>hexo server</code></p><p><code>hexo deploy</code></p><p><img src="http://i2.bvimg.com/647637/408049f3884510e4.png" alt=""></p><p>就可以看到文章发布了~</p><p><img src="http://i2.bvimg.com/647637/bdb5ddb3bea1f960.png" alt=""></p><h2 id="总结陈词"><a href="#总结陈词" class="headerlink" title="总结陈词"></a>总结陈词</h2><p>搭建博客的步骤：</p><p>1、GitHub配置</p><p>2、环境安装</p><p>3、网站推送</p><p>4、文章发布</p><p>发布文章的步骤：</p><p>1、hexo new 创建文章</p><p>2、Markdown语法编辑文章</p><p>3、部署（所有打开CMD都是在blog目录下）</p><p>到这里已经完成了博客的搭建以及文章的发布，但是还有很多需要设置和调整的。</p><p>我也是刚刚搭建好博客，还有待博客界面的优化以及内容的丰富!!!</p><p>今天先到这里啦~</p><p>##参考资料<br>1.<a href="https://zhangslob.github.io/2017/02/28/%E6%95%99%E4%BD%A0%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%8CHexo-Github/" target="_blank" rel="noopener">教你免费搭建个人博客，Hexo&amp;Github</a></p><p>2.<a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">GitHub+Hexo 搭建个人网站详细教程</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一直想有一个可以记录的属于自己的博客，最近在知乎上看到了利用&lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;+&lt;a href=&quot;https://github.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;搭建博客的教程，于是乎搭建了自己的博客&lt;a href=&quot;https://paradoxallen.github.io/&quot;&gt;paradoxallen&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;然而在此过程也遇到了一些bug，希望以下图文并茂的教程（虽然已经有挺多类似的教程…但主要是想自己写一篇文章!!!而且保证超级详细!!!）可以帮助大家少走弯路，搭建属于自己的博客。&lt;/p&gt;
    
    </summary>
    
      <category term="博客开发" scheme="https://paradoxallen.github.io/categories/%E5%8D%9A%E5%AE%A2%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="hexo" scheme="https://paradoxallen.github.io/tags/hexo/"/>
    
      <category term="博客" scheme="https://paradoxallen.github.io/tags/%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="GitHub" scheme="https://paradoxallen.github.io/tags/GitHub/"/>
    
  </entry>
  
</feed>
