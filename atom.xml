<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RP&#39;s Blog</title>
  
  <subtitle>学习总结  思考感悟</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://paradoxallen.github.io/"/>
  <updated>2018-06-13T04:55:26.430Z</updated>
  <id>https://paradoxallen.github.io/</id>
  
  <author>
    <name>LRP</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>关于机器学习在大气科学的应用</title>
    <link href="https://paradoxallen.github.io/21048/"/>
    <id>https://paradoxallen.github.io/21048/</id>
    <published>2018-05-06T16:00:00.000Z</published>
    <updated>2018-06-13T04:55:26.430Z</updated>
    
    <content type="html"><![CDATA[<p>前阵子阅读了院里一位博士研究生师兄的一篇有关基于神经网络算法对北京近五年的常规探空数据进行自组织分类，并揭示出大气污染物在不同边界层结构下的演变规律和相关机制的文章<a href="https://www.atmos-chem-phys.net/18/6771/2018/" target="_blank" rel="noopener">《Self-organized classification of boundary layer meteorology and associated characteristics of air quality in Beijing》</a>，看完顿时心生膜拜之情；</p><p>然后恰巧也是那个时候吕教授在院群上也转发了一篇关于机器学习预测火势甚至天气的公众号文章<a href="https://mp.weixin.qq.com/s?__biz=MjM5ODE1NDYyMA==&amp;mid=2653384819&amp;idx=2&amp;sn=523f27cb9442ab4af27137edd1280248&amp;chksm=bd1cc8608a6b4176d7cae939f794e082cf86b5bf0ac0deb53790f507f563f588b2148687957c&amp;mpshare=1&amp;scene=1&amp;srcid=0517RYcQWk5dWJCcqoI7jLCe#rd" target="_blank" rel="noopener">《机器学习成功解决“蝴蝶效应”！以后你终于可以相信天气预报了》</a>。</p><p>加之自己报名了一个<a href="https://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;mid=2247487345&amp;idx=1&amp;sn=4acb8978a2d95f0a1926c0e07021bec3&amp;chksm=e89455fcdfe3dcea645499d9321177a99ea713ffe06fe4af9d18c8506dd285755be2a1640539&amp;mpshare=1&amp;scene=1&amp;srcid=04151pmRgGqnfZDpvBL9EKKk#rd" target="_blank" rel="noopener">“预测北京和伦敦两个城市的空气质量”的KDD Cup 2018</a>但是因为自己报名太晚，组队不成（其实更深层的是之前关于机器学习的内容已经忘得差不多了。。。）</p><p>如此的机缘巧合，感觉将机器学习应用于大气科学将前途无量。我自己也想在这一方向进行深入了解，接下来我会进行相关内容的学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前阵子阅读了院里一位博士研究生师兄的一篇有关基于神经网络算法对北京近五年的常规探空数据进行自组织分类，并揭示出大气污染物在不同边界层结构下的演变规律和相关机制的文章&lt;a href=&quot;https://www.atmos-chem-phys.net/18/6771/2018/&quot;
      
    
    </summary>
    
      <category term="个人随笔" scheme="https://paradoxallen.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="大气科学" scheme="https://paradoxallen.github.io/tags/%E5%A4%A7%E6%B0%94%E7%A7%91%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>统计推断(零) 章节简介</title>
    <link href="https://paradoxallen.github.io/5451/"/>
    <id>https://paradoxallen.github.io/5451/</id>
    <published>2018-03-31T16:00:00.000Z</published>
    <updated>2018-06-03T16:17:44.275Z</updated>
    
    <content type="html"><![CDATA[<p>《统计推断(翻译版·原书第2版)》从概率论的基础开始，通过例子与习题的旁征博引，引进了大量近代统计处理的新技术和一些国内同类教材中不常见而又广为使用的分布。</p><p>其内容既包括工科概率入门、经典统计和现代统计的基础，又加进了不少近代统计中数据处理的实用方法和思想，例如：Bootstrap再抽样法、刀切(Jackkrlife)估计、EM算法、Logistic回归、稳健(Robest)回归、Markov链、Monte Carlo方法等。</p><p>它的统计内容与国内流行的教材相比，理论较深，模型较多，案例的涉及面要广，理论的应用面要丰富，统计思想的阐述与算法更为具体。</p><p>《统计推断(翻译版·原书第2版)》可作为工科、管理类学科专业本科生、研究生的教材或参考书，也可供教师、工程技术人员自学之用。</p><a id="more"></a><hr><h3 id="章节简介"><a href="#章节简介" class="headerlink" title="章节简介"></a><strong>章节简介</strong></h3><p><strong>出版说明</strong><br><strong>第2版序</strong><br><strong>第1版序</strong><br><strong>译后序</strong><br><strong>第1章 概率论</strong><br>1.1 集合论<br>1.2 概率论基础<br>1.2.1 公理化基础<br>1.2.2 概率演算<br>1.2.3 计数<br>1.2.4 枚举结果<br>1.3 条件概率与独立性<br>1.4 随机变量<br>1.5 分布函数<br>1.6 概率密度函数和概率质量函数<br>1.7 习题<br>1.8 杂录<br><strong>第2章 变换和期望</strong><br>2.1 随机变量函数的分布<br>2.2 期望<br>2.3 矩和矩母函数<br>2.4 积分号下的求导<br>2.5 习题<br>2.6 杂录<br>2.6.1 矩列的唯一性<br>2.6.2 其他母函数<br>2.6.3 矩母函数能否唯一地确定分布？<br><strong>第3章 常见分布族</strong><br>3.1 引言<br>3.2 离散分布<br>3.3 连续分布<br>3.4 指数族<br>3.5 位置与尺度族<br>3.6 不等式与恒等式<br>3.6.1 概率不等式<br>3.6.2 恒等式<br>3.7 习题<br>3.8 杂录<br>3.8.1 Poisson假设<br>3.8.2 Chebychev不等式及其改进<br>3.8.3 再谈指数族<br><strong>第4章 多维随机变量</strong><br>4.1 联合分布与边缘分布<br>4.2 条件分布与独立性<br>4.3 二维变换<br>4.4 多层模型与混合分布<br>4.5 协方差与相关<br>4.6 多维分布<br>4.7 不等式<br>4.7.1 数值不等式<br>4.7.2 函数不等式<br>4.8 习题<br>4.9 杂录<br>4.9.1 交换悖论<br>4.9.2 算术－几何－调和平均值不等式<br>8.3.1 错误概率与功效函数<br>8.3.2 最大功效检验<br>8.3.3 并－检验与交－并检验的真实水平<br>8.3.4 P-值<br>8.3.5 损失函数最优性<br>8.4 习题<br>8.5 杂录<br>8.5.1 单调功效函数<br>8.5.2 似然比作为证据<br>8.5.3 P-值和后验概率<br>8.5.4 置信集P-值<br><strong>第9章 区间估计</strong><br>9.1 引言<br>9.2 区间估计量的求法<br>9.2.1 反转一个检验统计量<br>9.2.2 枢轴量<br>9.2.3 枢轴化累积分布函数<br>9.2.4 Bayes区间<br>9.3 区间估计量的评价方法<br>9.3.1 尺寸和覆盖概率<br>9.3.2 与检验相关的最优性<br>9.3.3 Bayes最优<br>9.3.4 损失函数最优<br>9.4 习题<br>9.5 杂录<br>9.5.1 置信方法<br>9.5.2 离散分布中的置信区间<br>9.5.3 Fieller定理<br>9.5.4 其他区间如何?<br><strong>第10章 渐近评价</strong><br>10.1 点估计<br>10.1.1 相合性<br>10.1.2 有效性<br>10.1.3 计算与比较<br>10.1.4 自助法标准误差<br>10.2 稳健性<br>10.2.1 均值和中位数<br>10.2.2 M_估计量<br>10.3 假设检验<br>10.3.1 LRT的渐近分布<br>10.3.2 其他大样本检验<br>10.4 区间估计<br>10.4.1 近似极大似然区间<br>10.4.2 其他大样本区间<br>10.5 习题<br>10.6 杂录<br>10.6.1 超有效性<br>10.6.2 适当的正则性条件<br>10.6.3 再谈自助法<br>10.6.4 影响函数<br>10.6.5 自助法区间<br>10.6.6 稳健区间<br><strong>第11章 方差分析和回归分析</strong><br>11.1 引言<br>11.2 一种方式分组的方差分析<br>11.2.1 模型和分布假定<br>11.2.2 经典的ANOVA假设<br>11.2.3 均值的线性组合的推断<br>11.2.4 ANOVAF检验<br>11.2.5 对比的同时估计<br>11.2.6 平方和的分解<br>11.3 简单线性回归<br>11.3.1 最小二乘：数学解<br>11.3.2 最佳线性无偏估计：统计解<br>11.3.3 模型和分布假定<br>11.3.4 正态误差下的估计和检验<br>11.3.5 在给定点x=x0处的估计和预测<br>11.3.6 同时估计和置信带<br>11.4 习题<br>11.5 杂录<br>11.5.1 Cochran定理<br>11.5.2 多重比较<br>11.5.3 随机化完全区组设计<br>11.5.4 其他类型的方差分析<br>11.5.5 置信带的形状<br>11.5.6 Stein悖论<br><strong>第12章 回归模型</strong><br>12.1 引言<br>12.2 变量有误差时的回归<br>12.2.1 函数关系和结构关系<br>12.2.2 最小二乘解<br>12.2.3 极大似然估计<br>12.2.4 置信集<br>12.3 罗吉斯蒂克回归<br>12.3.1 模型<br>12.3.2 估计<br>12.4 稳健回归<br>12.5 习题<br>12.6 杂录<br>12.6.1 函数和结构的意义<br>12.6.2 EIV模型中常规最小乘的相合性<br>12.6.3 EIV模型中的工具变量<br>12.6.4 罗吉斯蒂克似然方程<br>12.6.5 再谈稳健回归<br><strong>附录 计算机代数</strong><br><strong>常用分布表</strong><br><strong>参考文献</strong><br><strong>作者索引</strong><br><strong>名词索引</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《统计推断(翻译版·原书第2版)》从概率论的基础开始，通过例子与习题的旁征博引，引进了大量近代统计处理的新技术和一些国内同类教材中不常见而又广为使用的分布。&lt;/p&gt;
&lt;p&gt;其内容既包括工科概率入门、经典统计和现代统计的基础，又加进了不少近代统计中数据处理的实用方法和思想，例如：Bootstrap再抽样法、刀切(Jackkrlife)估计、EM算法、Logistic回归、稳健(Robest)回归、Markov链、Monte Carlo方法等。&lt;/p&gt;
&lt;p&gt;它的统计内容与国内流行的教材相比，理论较深，模型较多，案例的涉及面要广，理论的应用面要丰富，统计思想的阐述与算法更为具体。&lt;/p&gt;
&lt;p&gt;《统计推断(翻译版·原书第2版)》可作为工科、管理类学科专业本科生、研究生的教材或参考书，也可供教师、工程技术人员自学之用。&lt;/p&gt;
    
    </summary>
    
      <category term="应用统计" scheme="https://paradoxallen.github.io/categories/%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="统计" scheme="https://paradoxallen.github.io/tags/%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (5) Neural Network Part2</title>
    <link href="https://paradoxallen.github.io/53070/"/>
    <id>https://paradoxallen.github.io/53070/</id>
    <published>2017-08-29T16:00:00.000Z</published>
    <updated>2018-06-12T15:02:50.216Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><p>上一篇文章，介绍了<strong>前向传播(forward propagation)</strong>的过程，以及神经网络计算非线性问题的例子(XOR问题)</p><p>这一篇文章，开始介绍，如何来计算神经网络中各种参数的方法：<strong>后向传播(backward propagation)</strong></p><h3 id="1-Cost-Function"><a href="#1-Cost-Function" class="headerlink" title="1. Cost Function"></a>1. Cost Function</h3><p>为了拟合神经网络的各个参数，我们首先需要规定一些变量：<br><img src="https://i.imgur.com/GxSynvL.png" alt=""></p><p>不同分类问题：</p><p>若分为2类，其实用一个神经元作为输出层就可以了，用y=0和y=1区分；</p><p>多类问题，利用以前说过的，分为K类则最终的y∈R^K，例如分为三类的问题输出可选择为<img src="https://i.imgur.com/ariPfuP.png" alt=""></p><p>因为同样是分类问题，我们回想一下逻辑回归的cost function，其实神经网络的相同，只不过是对于分为K类问题的版本而已：<br><img src="https://i.imgur.com/9XJbSNE.png" alt=""></p><p>是不是……太复杂了，如果用 cost function 计算梯度之后梯度直接梯度下降法来计算神经网络参数，明显不如接下来介绍的这种方法简单快捷。</p><h3 id="2-Backpropagation"><a href="#2-Backpropagation" class="headerlink" title="2. Backpropagation"></a>2. Backpropagation</h3><p><strong>后向传播</strong>，是神经网络中用于替代直接计算梯度下降法，来设定各层参数的方法。方法的精髓在于，<strong>训练网络时先根据初始化的参数（一般是随机设定），计算得到最后一层（输出层）的输出，计算与实际网络输出之间的差，再根据当前层的差，反推出上一层的差，逐渐反推到第一层。每一层根据自身层的差，来逼近真实参数</strong>。</p><p>首先我们设定某l层的某j个神经元，与真实的神经元的值，两者的差距为<img src="https://i.imgur.com/Nc3lrrH.png" alt="">，the error of node j in layer l.</p><p>对于输出层的每个神经元来说，可以直接计算：<img src="https://i.imgur.com/3Ds9vuD.png" alt="">，向量化表示为<img src="https://i.imgur.com/cgaxaK5.png" alt=""></p><p>对于其他层的每个神经元而言，就需要依靠上一层神经元的计算结果来反推，<strong>反推的过程可以视为原本 Forwardpropagation 过程中分散到各个下一层神经元的水流沿着同样的道路再次汇聚到上一层的神经元中</strong>：<img src="https://i.imgur.com/9XmWAoL.png" alt=""></p><p>对于g′(t)的计算，这里需要一点点小技巧：<br><img src="https://i.imgur.com/CqMnQoj.png" alt=""></p><p>推导过程见下：<br><img src="https://i.imgur.com/Or2ogvB.png" alt=""></p><p>不存在δ(1)，因为<strong>输入层没有误差</strong></p><p>我们首先不考虑正则化项（最小化各个系数<img src="https://i.imgur.com/AbSeYkS.png" alt="">，得到计算各层各个单元的误差项之和</p><p>因此，我们得到求出神经网络各层参数的方法：<br><img src="https://i.imgur.com/SKUhVtK.png" alt=""></p><h3 id="3-Gradient-Checking"><a href="#3-Gradient-Checking" class="headerlink" title="3. Gradient Checking"></a>3. Gradient Checking</h3><p>当我们需要验证求出的神经网络工作的对不对呢？可以使用 gradient checking，通过check梯度判断我们的code有没有问题</p><p>对于下面这个[θ−J(θ)]图，取θ点左右各一点(θ−ϵ)与(θ+ϵ)，则点θ的梯度近似等于<img src="https://i.imgur.com/xliVzIX.png" alt=""></p><p><img src="https://i.imgur.com/XvkBIeh.png" alt=""></p><p>对于神经网络中的情况，则有：<br><img src="https://i.imgur.com/SPV9RtR.png" alt=""></p><p>由于在backpropagation算法中我们一直能得到J(θ)的导数D（derivative），那么就可以将这个近似值与D进行比较</p><p><strong>Summary: </strong><br>(1) 在backpropagation中计算出J(θ)对θ的导数D并组成vector（Dvec） </p><p>(2) 用numerical gradient checking方法计算大概的梯度<img src="https://i.imgur.com/xa4LZR9.png" alt=""></p><p>(3) 看是否得到相同（or相近）的结果 </p><p>(4) <strong>（非常重要）</strong>停止checking，只用 backpropagation 进行神经网络学习，否则会非常非常慢</p><h3 id="4-Backpropagation-in-Practice"><a href="#4-Backpropagation-in-Practice" class="headerlink" title="4. Backpropagation in Practice"></a>4. Backpropagation in Practice</h3><p>这一节我们来看看实际 octave/MATLAB 编程中的一些技巧，例如对于神经网络如下：<br><img src="https://i.imgur.com/e2uWpzX.png" alt=""><br>当s1=10,s2=10时，则有θ(1)∈R10×11,θ(2)∈R10×11，一般来说，为了方便变形与传递参数，我们是将所有θ展开成一个完整的变量：<br>    thetaVec=[Theta1(:);Theta2(:);Theta3(:)] </p><p>再次展开的时候，例如重组为θ(1)时，取出其中前110个重组就好：<br>    Theta1=reshape(thetaVec(1:110),10,11)</p><p>如何初始化各层的参数呢？这同样是一个需要注意的地方：<strong>不能将各层的各个神经元的参数赋值为相同的数</strong>。</p><p>有兴趣的同学可以计算一下，这样神经网络的某一层内的所有神经元计算都变得相同，这样神经网络的非线性程度就降低了。<strong>一般来说，都是在(+ϵ,−ϵ)之间随机赋值</strong></p><p>我的神经网络需要有多少层呢？同样是个有趣的问题，一般来说，<strong>三层结构（仅仅一个隐藏层）</strong>已经足够来处理大部分非线性情况。如果分类效果不好，可以尝试使用更多的隐藏层，但需要保证每个隐藏层的神经元个数相同，层数越多就越慢，当然一般来说分类效果就更好</p><p>每层神经网络需要多少个神经元？能确定的只有输入层与输出层：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">No. of input units: Dimension of features</span><br><span class="line">No. output units: Number of classes</span><br><span class="line">Other units: who knows… usually the more the better</span><br></pre></td></tr></table></figure></p><h3 id="5-Put-it-together"><a href="#5-Put-it-together" class="headerlink" title="5. Put it together"></a>5. Put it together</h3><p>最终我们回顾一下神经网络的主要步骤： </p><p><strong>randomly initialize weights</strong></p><p><strong>(for 1 to m) forward-propagation</strong></p><p><strong>(for 1 to m) cost function</strong></p><p><strong>(for 1 to m) backward-propogation</strong></p><p><strong>gradient checking, then stop</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (4) Neural Network Part1</title>
    <link href="https://paradoxallen.github.io/21187/"/>
    <id>https://paradoxallen.github.io/21187/</id>
    <published>2017-08-22T16:00:00.000Z</published>
    <updated>2018-06-14T00:26:45.751Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-由XOR-Problem想到"><a href="#1-由XOR-Problem想到" class="headerlink" title="1. 由XOR Problem想到"></a>1. 由XOR Problem想到</h3><p>有一种经典的非线性问题：<strong>XOR，也就是异或</strong>。真值表如下： </p><p>0 0 | 0<br>1 0 | 1<br>1 1 | 0<br>0 1 | 1，| 左侧是输入，| 右侧是输出 </p><p>如果在二维坐标系上绘图，可以看出来仅利用一条直线是无法对这个问题分类的，也就是<strong>线性不可分</strong>问题。</p><p>如果利用逻辑回归的方法，可以枚举各种特征的出现可能，即<br>继续想下去，如果基础特征量更多的话？就会出现过拟合的问题，同时带来极大的计算量。</p><p>例如，计算机视觉中处理一张照片，每个像素都需要一个数值表示。对于100*100像素值的图片，仅仅考虑二次项等级，就有特征数量为5000个（与相同，故除以2）。</p><p>于是，这章介绍的非线性分类器，<strong>神经网络（Neural Network，NN）</strong>就发挥了作用。</p><h3 id="2-人工神经网络"><a href="#2-人工神经网络" class="headerlink" title="2. 人工神经网络"></a>2. 人工神经网络</h3><p>神经网络最初提出的初衷，是因为要模拟人类大脑的结构（很初级的模拟，因为人类对于自己大脑究竟是怎样都没有弄清楚）。<strong>通过多个感知机之间的输入输出，从而完成整体的智能行为</strong>。</p><p>在人工神经网络中，“感知机”就是一个有着输入与输出功能的小单元，接收上一层的输入，将输出传给下一层。</p><p>人工神经网络是层级结构，某一层上的单元之间互相不会有输入输出关系，只和上一层或者下一层的单元产生数据传输关系。至少会有两层：<strong>输入层（input layer）与输出层（output layer）</strong>，但是两层的神经网络可以解决的问题很少，一般都是三层或者三层以上，中间的这些层就称为<strong>“隐藏层（hidden layer）”</strong>，我们来看一个最简单的例子：<br><img src="https://i.imgur.com/U02TDKa.jpg" alt=""></p><p>由<strong>输入层，到隐藏层，最终到输出层</strong>。这是一次 <strong>forward propogation</strong> 过程。类似于逻辑回归，但是神经网络的输入是某个样本的所有基础特征，不需要考虑 这一类新加入的特征。</p><h3 id="3-回到XOR-Problem"><a href="#3-回到XOR-Problem" class="headerlink" title="3. 回到XOR Problem"></a>3. 回到XOR Problem</h3><p>先讲几个基础的利用神经网络进行二进制运算分类的问题：</p><ol><li><p>二进制 AND<br><img src="https://i.imgur.com/5pgAvdw.jpg" alt=""><br>即为，只有1,1时返回值才为1，符合 AND 的操作结果。</p></li><li><p>二进制 OR<br><img src="https://i.imgur.com/vNpszeX.jpg" alt=""><br>即为，只有0,0时返回值才为0，符合 OR 的操作结果。</p></li><li><p>二进制 NOT<br><img src="https://i.imgur.com/8ODYZRw.jpg" alt=""></p></li></ol><p>XOR 问题复杂一些，但是如果我们做了如下转换：</p><p><code>XNOR = NOT XOR = AND OR NOT AND NOT</code></p><p>变换的正确性，很容易通过真值表来验证。大家可以分别计算各个括号中的内容，然后通过 OR 连接起来。<br>我们将 AND 的内容视为 ，NOT AND NOT 的内容视为<br><img src="https://i.imgur.com/tcDGEAE.jpg" alt=""></p><h3 id="4-神经网络多类分类"><a href="#4-神经网络多类分类" class="headerlink" title="4. 神经网络多类分类"></a>4. 神经网络多类分类</h3><p>神经网络处理多类的分类问题是很方便的。举个例子，区分手写数字时，有10个类别：0，1，2，……，9。<br>对于某一个训练样本来说，有着特征组合,这个神经网络的输出层有10个单元。当输出层的10个单元全部取 0，意味着输入不是任何一种数字。</p><p>对于手写数字的识别，一直是业界的研究重点之一。视频中举了一篇经典的利用神经网络处理该问题的Paper，有兴趣的同学可以访问作者的个人主页查看Demo与Paper：<a href="http://yann.lecun.com/exdb/lenet/" target="_blank" rel="noopener">Yann LeCun</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (3) Logistic Regression &amp; Regularization</title>
    <link href="https://paradoxallen.github.io/26112/"/>
    <id>https://paradoxallen.github.io/26112/</id>
    <published>2017-08-15T16:00:00.000Z</published>
    <updated>2018-06-13T04:32:05.451Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Logistic-Regression"><a href="#1-Logistic-Regression" class="headerlink" title="1. Logistic Regression"></a>1. Logistic Regression</h3><p>对于分类问题而言，很容易想到利用<strong>线性回归</strong>方法，拟合之后的<strong>hθ(x)&gt;0.5则为True，其余为False</strong>.</p><p>但是线性回归有一个问题，拟合出的值都是离散的，范围不确定。</p><p>为了方便分析，我们希望将拟合出的值限制在0~1之间。因此，出现了<strong>逻辑回归</strong>。</p><p>逻辑回归的模型是一个<strong>非线性模型</strong>：<strong>sigmoid函数，又称逻辑回归函数</strong>。但它本质上又是一个线性回归模型，因为除去sigmoid映射函数关系，其他的步骤，算法都是线性回归的。</p><p>sigmoid函数（或，逻辑回归函数）：<br><img src="https://i.imgur.com/QLwtN5k.png" alt=""><br>其函数图像为：<br><img src="https://i.imgur.com/wC6xW9Z.png" alt=""></p><p>这个函数的特征非常明显<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">函数值一直在0~1范围内；</span><br><span class="line">经过(0,0.5)点。这个很容易作为区分0，1类的分界线。</span><br></pre></td></tr></table></figure></p><p>逻辑回归中，对于原本线性回归中拟合而成的hypothesis函数，需要经过sigmoid函数的修饰：<br><img src="https://i.imgur.com/ElJujkC.png" alt=""></p><p>此时，hθ(x)的含义发生了变化，<br><img src="https://i.imgur.com/6DX46yY.png" alt=""><br>。成为<br>        <strong>‘’the probability that y=1, given x, parameterized by θ’’</strong></p><p>因此有<br><img src="https://i.imgur.com/SbZ7wQN.png" alt=""></p><p><strong>Decision Boundary</strong>。表示的是 hypothesis 函数确定之后，划分数据分类的界限，并不一定可以百分百区分数据集，只是函数的属性之一。下图蓝色曲线即为某个 Desicision Boundary。<br><img src="https://i.imgur.com/nM4tJjr.jpg" alt=""></p><h3 id="2-Cost-Function"><a href="#2-Cost-Function" class="headerlink" title="2. Cost Function"></a>2. Cost Function</h3><p>回忆线性回归的 cost function，我们在其中插入 cost 函数的概念：<br><img src="https://i.imgur.com/2fhUuge.png" alt=""></p><p>完全照搬线性回归的 cost function 到逻辑回归中，因为sigmoid函数的非线性，会造成J(θ)取值的不断震荡，导致其是一个非凸形函数（non-convex）。表示在“J(θ)—θ”二维图中如下：<br><img src="https://i.imgur.com/YxpdAK6.png" alt=""></p><p>我们需要构造一种新的 cost 函数。出发点为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">当y=1时，若hypothesis函数拟合结果为0，即为“重大失误”，cost 趋于无穷大；</span><br><span class="line">当y=0时，若hypothesis函数拟合结果为1，即为“重大失误”，cost 趋于无穷大；</span><br></pre></td></tr></table></figure></p><p>构造的新 cost 函数：<br><img src="https://i.imgur.com/1AZPras.png" alt=""></p><p>如果进一步合并，可以得到最终逻辑回归的cost函数。并且值得指出的是，代入这个cost函数通过梯度下降法得到的 θ 更新函数依然成立：<br><img src="https://i.imgur.com/F16aq5V.png" alt=""></p><h3 id="3-梯度下降法的优化"><a href="#3-梯度下降法的优化" class="headerlink" title="3. 梯度下降法的优化"></a>3. 梯度下降法的优化</h3><p>对于梯度下降法的优化有很多，但是都需要J(θ)与∂J(θ)/∂θj的代码。</p><p>以此为基础的对于梯度下降法的优化（视频中都没有具体介绍，有兴趣的同学可以点击链接）有：<br><a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method" target="_blank" rel="noopener">共轭梯度法</a><br><a href="https://en.wikipedia.org/w/index.php?title=Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm&amp;redirect=no" target="_blank" rel="noopener">BFGS</a><br><a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" target="_blank" rel="noopener">L-BFGS</a></p><p>这些优化方法的特点也很一致：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">不需要人为选择 α，自适应性</span><br><span class="line">更复杂，更慢</span><br></pre></td></tr></table></figure></p><p>这里提到了两个MATLAB的非线性优化函数：<br><strong>optimset：创建或编辑一个最优化参数选项。</strong>具体调用在MATLAB中 <code>help optimset</code> 命令查看；<br><strong>fminunc：最小值优化。</strong>具体调用在MATLAB中 <code>help fminunc</code>命令查看；</p><p><strong>建议</strong>：Ng在优化这一部分讲的过于简略，基本等于什么都没说……还是要根据这几个方法名称在使用时搜索更多。</p><h3 id="4-one-vs-all-one-vs-rest"><a href="#4-one-vs-all-one-vs-rest" class="headerlink" title="4.one vs. all (one vs. rest)"></a>4.one vs. all (one vs. rest)</h3><p>如果需要进行多类的分类，需要一种精妙的修改，使得两类的分类问题得以适用于多类的分类。 </p><ol><li><p>现已知有n类样本需要区分开（1，2，3，……）；</p></li><li><p>以原1类为新1类，剩余的原2，3，……作为新2类。原本的多类问题变成了二类问题:<br><img src="https://i.imgur.com/TKq60Ef.png" alt=""></p></li><li>以原2类为新1类，剩余的原1，3，……作为新2类。再分类:<br><img src="https://i.imgur.com/EioDwff.png" alt=""></li><li><p><img src="https://i.imgur.com/wK9pOb4.png" alt=""></p></li><li><p>对于任意一个 x 而言，如何分辨是哪一类呢？于是，求出所有的<img src="https://i.imgur.com/7U91HVE.png" alt="">值最大对应的i（<strong>表示y=i的概率最大</strong>）即为x的所属分类</p></li></ol><h3 id="5-Regularization（正则化）"><a href="#5-Regularization（正则化）" class="headerlink" title="5. Regularization（正则化）"></a>5. Regularization（正则化）</h3><p>拟合会产生三种情况： </p><ol><li><p><strong>underfitting（欠拟合）=high bias</strong>，大部分训练样本无法拟合</p></li><li><p><strong>overfitting（过拟合）=high variance</strong>，为了拟合几乎每一个训练样本。导致拟合函数极为复杂，易产生波动，泛化（generalize）能力差，虽然训练样本几乎百分百拟合，但是测试样本很可能因为极大波动而极少拟合成功</p></li><li><p><strong>just right</strong>，对于训练样本，拟合得不多不少刚刚好，并且泛化到测试样本拟合效果同样较好</p></li></ol><p>欠拟合，比较好解决，创造并引入更多的特征即可。例如：对于x,y而言，可以引入x2,y2,xy等等新的特征</p><p>过拟合，则比较复杂。可用的方法有两个： </p><ol><li><p><strong>Reduce number of features, 减少特征量</strong></p></li><li><p><strong>Regularization，正则化</strong>。保持所有的特征数量不变，而去改变特征前的度量单位 θj（若 θj 趋于0，则此特征可视为无影响）</p></li></ol><p>解决过拟合的正则化方法，因此需要引入全新的优化目标到 cost function 中。原先的 cost function 只是希望适合拟合更为接近，现在还需要使得特征前的度量单位 θj 的最小。因此有：<br><img src="https://i.imgur.com/Msp5tw4.png" alt=""></p><p>正则化方法处理之后，∂J(θ)/∂θj发生对应变化，因此我们有：<br><img src="https://i.imgur.com/dmhGxHY.png" alt=""></p><p>若λ非常大（例如10^10），则正则化方法会导致结果 underfitting。这也很好理解，因为优化目标中有使得 <img src="https://i.imgur.com/ctA4muM.png" alt=""> 尽可能小，这样会导致 θ 全部趋于 0。</p><p>一般来说，<strong>α,λ,m&gt;0，所以(1−αλ/m)&lt;1，常见使其取值0.99 左右</strong></p><h3 id="6-Regularization-for-Normal-Equation"><a href="#6-Regularization-for-Normal-Equation" class="headerlink" title="6. Regularization for Normal Equation"></a>6. Regularization for Normal Equation</h3><p>课程视频中缺少证明，因此我们仅需掌握结论使用即可<br>对于 Week 2 中的<strong>Normal Equation</strong>方法，原本需要求解的方程<br><img src="https://i.imgur.com/ZiPg749.png" alt=""><br>做一个小小的改动：<br><img src="https://i.imgur.com/cZwlXun.png" alt=""></p><p>若样本拥有n个特征，则<img src="https://i.imgur.com/Gu1x2Z9.png" alt="">表示的是(n+1) * (n+1)维的对角矩阵，除了(0, 0)取值为 0，其余对角位置取 1。</p><p><strong>non-invertibility</strong>：非不可逆性……好拗口，意思就是对于原本的(xTx)矩阵可能会出现不可逆的情况。但是，对于正则化之后的矩阵<img src="https://i.imgur.com/KB79QV2.png" alt="">一定是可逆的（未提供证明）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (2) Linear Regression</title>
    <link href="https://paradoxallen.github.io/58080/"/>
    <id>https://paradoxallen.github.io/58080/</id>
    <published>2017-08-09T16:00:00.000Z</published>
    <updated>2018-06-13T04:32:05.448Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Multivariate-Linear-Regression"><a href="#1-Multivariate-Linear-Regression" class="headerlink" title="1. Multivariate Linear Regression"></a>1. Multivariate Linear Regression</h3><p>Week 1 讨论仅一个特征，即仅有一个未知量x影响了目标y的取值。如果现在有很多特征？现在我们有x1,x2…xn影响了目标y的取值。</p><p>此时需要区分的是变量标记规则：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xi表示的是第i个特征</span><br><span class="line">x(i)表示的是第i个样本，一个样本是由多个特征组成的列向量</span><br></pre></td></tr></table></figure></p><p>例如：<br><img src="https://i.imgur.com/1VQTtI7.png" alt=""><br>综上，我们有<br><img src="https://i.imgur.com/IN8QuHt.png" alt=""><br>可以视为，每个样本都多出一个特征：x0=1，这样表示有利于之后的矩阵表示</p><h3 id="2-多变量梯度下降法："><a href="#2-多变量梯度下降法：" class="headerlink" title="2. 多变量梯度下降法："></a>2. 多变量梯度下降法：</h3><p><img src="https://i.imgur.com/jqtDLIq.png" alt=""></p><h3 id="3-Feature-Scaling（特征缩放）"><a href="#3-Feature-Scaling（特征缩放）" class="headerlink" title="3. Feature Scaling（特征缩放）"></a>3. Feature Scaling（特征缩放）</h3><p>很简单，就是将每种特征的数据范围限定在同一个数量级。例如<br><img src="https://i.imgur.com/cauqMeB.png" alt=""><br>这样会导致迭代次数过多。这时候，如果我们找到一种mapping方式，使得两者属于同一个数量级的范围内，可以有效减少迭代次数</p><p><strong>注意</strong>：无法降低单次的迭代时间，但是却能有效地降低迭代次数</p><p>其实方法很多，这有一种：<br><img src="https://i.imgur.com/egjFhiQ.png" alt=""><br>其中，mean(x)表示向量每个元素的平均值，max(x)表示向量中最大元素，min(x)表示向量中最小元素</p><h3 id="4-Learning-Rate"><a href="#4-Learning-Rate" class="headerlink" title="4. Learning Rate"></a>4. Learning Rate</h3><p>learning rate 是机器学习中的一个不稳定因素，如何判断选取的 learning rate 是合适的？我们可以看看以下这幅图：<br><img src="https://i.imgur.com/81mq8t6.jpg" alt=""></p><p>如果以迭代次数为横坐标，cost function 结果为纵坐标，绘制的图像是递减的，说明 learning rate 选择的是恰当的。如果碰到下图所显示的三种情况，那就只有一条路：<strong>减小 learning rate </strong><br><img src="https://i.imgur.com/DmExZo6.jpg" alt=""></p><p>但是 learning rate 太小同样会导致一个问题：<strong>学习过慢</strong>。所以，只能靠试：0.001，0.003，0.01，0.03，0.1，0.3……</p><h3 id="5-Polynomial-Regression（多项式回归，不同于多变量线性回归）"><a href="#5-Polynomial-Regression（多项式回归，不同于多变量线性回归）" class="headerlink" title="5. Polynomial Regression（多项式回归，不同于多变量线性回归）"></a>5. Polynomial Regression（多项式回归，不同于多变量线性回归）</h3><p>有时候，我们需要自己创造一些“特征”，来拟合一些非线性分布情况<br>例如：<br><img src="https://i.imgur.com/5F2PbWA.png" alt=""><br>看上去只有一个特征x，但我们完全可以理解为x^2和√x都是单独的新特征</p><p>以后的课程会具体讲述如何选择这些特征</p><h3 id="6-Normal-Equation"><a href="#6-Normal-Equation" class="headerlink" title="6. Normal Equation"></a>6. Normal Equation</h3><p>梯度下降法可以用于寻找函数（cost function）的最小值，想一想，初高中的时候我们使用的是什么方法？最小值点的导数为零，然后解方程</p><p>将导数置为零这种方法即<strong> Normal Equation</strong>。<br><img src="https://i.imgur.com/aqLezpl.png" alt=""></p><p>上文提过，增加一个全1分量x0后得到<br><img src="https://i.imgur.com/N6qtlas.png" alt=""></p><p>可以得到：<br><img src="https://i.imgur.com/LU4vw47.png" alt=""></p><p>matlab编程十分简单：<br><img src="https://i.imgur.com/OVUbKD9.png" alt=""></p><p>Normal Equation 有以下<strong>优缺点</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">不需要 learning rate，也就不需要选择；</span><br><span class="line">不需要迭代，不需要考虑收敛的问题；</span><br><span class="line">当特征非常多的时候，因为涉及求逆操作，会非常慢（注：方阵才有逆矩阵）</span><br></pre></td></tr></table></figure></p><h3 id="7-Octave-Tutorial"><a href="#7-Octave-Tutorial" class="headerlink" title="7. Octave Tutorial"></a>7. Octave Tutorial</h3><p>这一部分十分简单，其实就是MATLAB的使用方法。建议不论是否初学者都去看看，会有收获。 </p><p>谈到一个问题：<strong>如果现有的样本数，小于每个样本所有的特征数怎么办？去除多余的特征（PCA？）</strong>。特征过多，也可能会导致矩阵不可逆的情况。 </p><p>下面记录一些觉得挺有趣的命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">~=：不等于号</span><br><span class="line">xor(0, 1)：异或操作</span><br><span class="line">rand(m, n)：0~1之间的大小为m*n的随机数矩阵；randn：产生均值为0，方差为1的符合正态分布的随机数（有负数）</span><br><span class="line">length(A)：返回A中行、列中更大值</span><br><span class="line">A(:)：将矩阵A变为列向量形式，不论A是向量还是矩阵</span><br><span class="line">sum(A,1)：每列求和得到一个行向量；sum(A,2)：每行求和得到一个列向量</span><br><span class="line">pinv：伪求逆；inv：求逆</span><br><span class="line">imagesc(A)：帅爆！根据矩阵中每个值绘制各种颜色的方块</span><br><span class="line">A.^2 ~= A^2，后者是两个矩阵相乘</span><br></pre></td></tr></table></figure></p><h3 id="8-Submitting-Programming-Assignments"><a href="#8-Submitting-Programming-Assignments" class="headerlink" title="8. Submitting Programming Assignments"></a>8. Submitting Programming Assignments</h3><p>其实看看视频就行了，主要要注意，submit() 时输入的Token，不是Coursera 的密码，而是作业的密码：<br><img src="https://i.imgur.com/a8OsCRt.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (1) Introduction</title>
    <link href="https://paradoxallen.github.io/36971/"/>
    <id>https://paradoxallen.github.io/36971/</id>
    <published>2017-08-04T16:00:00.000Z</published>
    <updated>2018-06-13T04:32:05.450Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Environment-Setup-Instructions"><a href="#1-Environment-Setup-Instructions" class="headerlink" title="1. Environment Setup Instructions"></a>1. Environment Setup Instructions</h3><p>这一章介绍课程一般使用的工具。octave或者matlab即可，这两者本质上没有什么区别，都有着丰富的数学库计算库。</p><h3 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h3><ol><li><strong>机器学习定义：</strong>简单来说，让计算机执行一些行为，但是without explicit programmed。</li><li><p><strong>监督学习（supervised learning）：</strong>通过<strong>已知</strong>的正确信息，得到未知的信息。一般用于解决的问题有两类： </p><ol><li><p><strong>回归问题（regression）</strong><br> 例如：房价预测。已知一百平房子售价，两百平房子售价，三百平房子售价。那么，一百五十平房子的售价呢？</p><p> 很容易想到，<strong>拟合</strong>。那么接下来，是直线拟合？二次曲线拟合？</p><p> 这就是回归问题要解决的，通过已知离散信息，<strong>预测</strong>未知的连续信息。</p></li><li><p><strong>分类问题（classification）</strong><br> 例如：肿瘤分类。已知肿瘤size与肿瘤良性（label: 0）还是恶性（label: 1）分类的关系如下：<br><img src="https://i.imgur.com/MlGVT51.jpg" alt=""></p><p> 更进一步，完全可以用一维坐标图来表示：<br><img src="https://i.imgur.com/xI6gTxQ.jpg" alt=""></p><p> 现在需要对于根据size的对某个肿瘤判断它是良性还是恶性，就是分类问题。</p><p> 再进一步，现在我仅仅把size作为feature之一，如果我们加入考虑肿瘤积水（二维坐标图）？肿瘤硬度（三维坐标图）？原则上可以考虑无限种feature，如何处理？这里卖了个关子，可以使用支持向量机（SVM）。</p></li><li><p><strong>无监督学习（unsupervised learning）/ 聚类算法（clustering）：</strong><br> 对于有监督学习而言，我们已知的信息很多：每个data的label是什么，label可以分为多少类，等等。<strong>但是无监督学习中，这些都是未知的。</strong><br> 最常见的应用：浏览新闻时经常会有的相关新闻，或者搜索时的相关搜索结果，又或是weibo中的推荐分组，等等。</p><p> 可以理解为计算机自己理解后将信息分为了若干类。</p><p> 经典案例：<a href="http://soma.mcmaster.ca/papers/Paper_7.pdf" target="_blank" rel="noopener"><strong>cocktail party problem</strong></a>。简而言之，就是在聚会上摆俩麦克风分别记录声音，需要通过这些信息将不同声源的声音分隔开来。</p><p> 如果考虑声音本身的特征与背景知识，解法会比较复杂，需要大量的先验知识。但站在机器学习的角度来思考，就是一类无监督学习的应用场景。matlab实现仅需几行代码而已。</p></li></ol></li></ol><h3 id="3-Bonus-Course-Wiki-Lecture-Notes"><a href="#3-Bonus-Course-Wiki-Lecture-Notes" class="headerlink" title="3. Bonus: Course Wiki Lecture Notes"></a>3. Bonus: Course Wiki Lecture Notes</h3><p><strong>单一变量线性回归</strong>：即通过线性方程来拟合已知data的feature，与label之间的关系。表达为<br><img src="https://i.imgur.com/zMe7YGr.png" alt=""></p><p>不同的线性方程之间，需要挑出来一个拟合更适合的，这就需要一个判断标准。常用的基于<strong>Mean squared error (MSE)</strong>：<br><img src="https://i.imgur.com/adK8AA2.png" alt=""></p><p>如果让计算机来实现这个过程，怎么办呢？<strong>梯度下降法（Gradient Descent）</strong>是一种常见手段。<br>该方法的核心是：<br><img src="https://i.imgur.com/i2kZYPl.png" alt=""></p><p>其中α是下降步长，人为确定。可以看出来，梯度下降法的目的是使J(θi,θj)越来越小，对于其是否线性，参数的数量，都没有限制。</p><p>最后利用梯度下降法代入导数（不论是链式法则，还是单个变量代入）得到最优的拟合函数参数：<br><img src="https://i.imgur.com/VBHlmsF.jpg" alt=""></p><h3 id="4-Model-and-Cost-Function"><a href="#4-Model-and-Cost-Function" class="headerlink" title="4. Model and Cost Function"></a>4. Model and Cost Function</h3><p>再次回顾一下监督学习，两类：<strong>regression和classification</strong>。<br>简单来说，<strong>regression是predict real-valued output，classification是predict discrete-valued output。</strong><br>注明了以下符号，方便以后沟通：<br><img src="https://i.imgur.com/0Lr7GzM.png" alt=""></p><p>我们设计拟合，目的就是为了使得表示拟合数据与已知真实数据直接差距的cost function最小，这个cost function是啥？就是我们之前提到的判断标准，常见的为：<br><img src="https://i.imgur.com/GiYdN6C.png" alt=""><br>当有两个未知量时，cost function就需要表示为三维图：<br><img src="https://i.imgur.com/DJLSXXG.jpg" alt=""></p><p>为了方便表示，也可以把上图表示为类似地理上的“等高线”。术语称为 <strong>contour plot（轮廓图）</strong>：<br><img src="https://i.imgur.com/ZbnTLxm.jpg" alt=""></p><h3 id="5-Parameter-Learning"><a href="#5-Parameter-Learning" class="headerlink" title="5. Parameter Learning"></a>5. Parameter Learning</h3><p>现在，我们开始具体来看看，第一个机器学习算法：<strong>梯度下降法（gradient descent）</strong></p><p><img src="https://i.imgur.com/SAMHpSX.png" alt=""><br><strong>特别注意</strong>，这里的temp是为了保证更新是同一时间发生的，这才是最正宗的梯度下降法。如果先更新了θ0，再用更新之后的θ0去更新θ1，就违背了梯度下降法的初衷。</p><p>可能你已经发现，<strong>这里的最优值，仅仅是局部的（local）而不是全局的（global）</strong>。因此选取的初值不同，可能会导致算法停留在不同的最优值。因此，这是梯度下降法的一个缺陷。但是对于线性回归而言，cost function 是一个convex function（形状为弓形），仅有一个最优值，局部最优值就是全局最优值。</p><p>到此为止，我们说的其实都是：<strong>batch</strong> gradient descent。也就是我的cost function是所有样本的MSE之和。如果不是计算所有样本，仅仅是计算某个重要子集的MSE之和，这个方法在以后的课程中会提到。</p><p>最后，提到线性代数中的寻找极值的方法：<strong>normal equations</strong>。但是在大规模的数据计算中，还是梯度下降法更为适用。</p><h3 id="6-Linear-Algebra-Review"><a href="#6-Linear-Algebra-Review" class="headerlink" title="6. Linear Algebra Review"></a>6. Linear Algebra Review</h3><p>矩阵：<a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Matrix_(mathematics)</a></p><p>向量：特殊的矩阵，注意是 列数 = 1</p><p>1-index：下标从1开始。0-index：下标从0开始。</p><p>矩阵乘法没有交换律，但是有结合律。</p><p>只有方阵（#col=#row）才有逆矩阵。但仅仅满足方阵这一个条件并不足够，同时需要满足行列式不等于0。没有逆矩阵的矩阵，称之为“奇异（singular）矩阵”或“退化（degenerate）矩阵”。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(5) 常见算法优缺点</title>
    <link href="https://paradoxallen.github.io/65434/"/>
    <id>https://paradoxallen.github.io/65434/</id>
    <published>2017-06-09T16:00:00.000Z</published>
    <updated>2018-06-13T05:09:21.235Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p><a href="https://paradoxallen.github.io/62602/">机器学习(三) 模型结果应用</a></p><p><a href="https://paradoxallen.github.io/21484/">机器学习(四) 常见算法优缺点</a></p><p>文章结构：</p><ul><li><p><strong>什么是感知器分类算法</strong></p></li><li><p><strong>在Python中实现感知器学习算法</strong></p></li></ul><p><em>在iris（鸢尾花）数据集上训练一个感知器模型</em></p><ul><li><strong>自适应线性神经元和融合学习</strong></li></ul><p><em>使用梯度下降方法来最小化损失函数</em></p><p><em>在Python中实现一个自适应的线性神经元</em></p><a id="more"></a><hr><h3 id="什么是感知器分类算法"><a href="#什么是感知器分类算法" class="headerlink" title="什么是感知器分类算法"></a><strong>什么是感知器分类算法</strong></h3><p>设想我们改变逻辑回归算法，“迫使”它只能输出-1或1抑或其他定值。在这种情况下，之前的逻辑函数‍‍g就会变成阈值函数sign：</p><p><img src="https://i.imgur.com/TwrMWwh.png" alt=""></p><p><img src="https://i.imgur.com/pDzWDxS.png" alt=""></p><p>如果我们令假设为hθ(x)=g(θTx)hθ(x)=g(θTx)，将其带入之前的迭代法中：</p><p><img src="https://i.imgur.com/r4P3819.png" alt=""></p><p>至此我们就得出了感知器学习算法。简单地来说，感知器学习算法是神经网络中的一个概念，单层感知器是最简单的神经网络，输入层和输出层直接相连。</p><p><img src="https://i.imgur.com/Nb3JtYy.png" alt=""></p><p>每一个输入端和其上的权值相乘，然后将这些乘积相加得到乘积和，这个结果与阈值相比较（一般为0），若大于阈值输出端就取1，反之，输出端取-1。</p><p>初始权重向量W=[0,0,0]，更新公式W(i)=W(i)+ΔW(i)；ΔW(i)=η<em>(y-y’)</em>X(i)； </p><p>η：学习率，介于[0,1]之间 </p><p>y：输入样本的正确分类 </p><p>y’：感知器计算出来的分类 </p><p>通过上面公式不断更新权值，直到达到分类要求。</p><p><img src="https://i.imgur.com/RlHERhT.jpg" alt=""></p><p>初始化权重向量W，与输入向量做点乘，将结果与阈值作比较，得到分类结果1或-1。</p><hr><h3 id="在Python中实现感知器学习算法"><a href="#在Python中实现感知器学习算法" class="headerlink" title="在Python中实现感知器学习算法"></a><strong>在Python中实现感知器学习算法</strong></h3><p>下面直接贴上实现代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Perceptron(object):</span><br><span class="line">    &quot;&quot;&quot;Perceptron classifier.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ------------</span><br><span class="line">    eta : float</span><br><span class="line">        Learning rate (between 0.0 and 1.0)</span><br><span class="line">    n_iter : int</span><br><span class="line">        Passes over the training dataset.</span><br><span class="line"></span><br><span class="line">    Attributes</span><br><span class="line">    -----------</span><br><span class="line">    w_ : 1d-array</span><br><span class="line">        Weights after fitting.</span><br><span class="line">    errors_ : list</span><br><span class="line">        Number of misclassifications (updates) in each epoch.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, eta=0.01, n_iter=10):</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y):</span><br><span class="line">        &quot;&quot;&quot;Fit training data.</span><br><span class="line"></span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span><br><span class="line">            Training vectors, where n_samples is the number of samples and</span><br><span class="line">            n_features is the number of features.</span><br><span class="line">        y : array-like, shape = [n_samples]</span><br><span class="line">            Target values.</span><br><span class="line"></span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        self : object</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.w_ = np.zeros(1 + X.shape[1])</span><br><span class="line">        self.errors_ = []</span><br><span class="line"></span><br><span class="line">        for _ in range(self.n_iter):</span><br><span class="line">            errors = 0</span><br><span class="line">            for xi, target in zip(X, y):</span><br><span class="line">                update = self.eta * (target - self.predict(xi))</span><br><span class="line">                self.w_[1:] += update * xi</span><br><span class="line">                self.w_[0] += update</span><br><span class="line">                errors += int(update != 0.0)</span><br><span class="line">            self.errors_.append(errors)</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def net_input(self, X):</span><br><span class="line">        &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span><br><span class="line">        return np.dot(X, self.w_[1:]) + self.w_[0]</span><br><span class="line"></span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span><br><span class="line">        return np.where(self.net_input(X) &gt;= 0.0, 1, -1)</span><br></pre></td></tr></table></figure><p><strong>特别说明：</strong></p><p>学习速率η(eta)只有在权重（一般取值0或者很小的数）为非零值的时候，才会对分类结果产生作用。如果所有的权重都初始化为0，学习速率参数eta只影响权重向量的大小，而不影响其方向，为了使学习速率影响分类结果，权重需要初始化为非零值。需要更改的代码中的相应行在下面突出显示:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, eta=0.01, n_iter=50, random_seed=1): # add random_seed=1</span><br><span class="line">    ...</span><br><span class="line">    self.random_seed = random_seed # add this line</span><br><span class="line">def fit(self, X, y):</span><br><span class="line">    ...</span><br><span class="line">    # self.w_ = np.zeros(1 + X.shape[1]) ## remove this line</span><br><span class="line">    rgen = np.random.RandomState(self.random_seed) # add this line</span><br><span class="line">    self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1]) # add this line</span><br></pre></td></tr></table></figure></p><p><strong>在iris（鸢尾）数据集上训练一个感知器模型</strong></p><p><strong>读取iris数据集</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import collections</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(&apos;https://archive.ics.uci.edu/ml/&apos;</span><br><span class="line">        &apos;machine-learning-databases/iris/iris.data&apos;, header=None)</span><br><span class="line">print (df.head())</span><br><span class="line">print (&quot;\n&quot;)</span><br><span class="line">print (df.describe())</span><br><span class="line">print (&quot;\n&quot;)</span><br><span class="line">print (collections.Counter(df[4]))</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/tRDUNXi.jpg" alt=""></p><p><strong>可视化iris数据</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 为了显示中文(这里是Mac的解决方法，其他的大家可以去百度一下)</span><br><span class="line">from matplotlib.font_manager import FontProperties</span><br><span class="line">font = FontProperties(fname=&apos;/System/Library/Fonts/STHeiti Light.ttc&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 选择 setosa and versicolor类型的花</span><br><span class="line">y = df.iloc[0:100, 4].values</span><br><span class="line">y = np.where(y == &apos;Iris-setosa&apos;, -1, 1)</span><br><span class="line"></span><br><span class="line"># 提取它们的特征 （sepal length and petal length）</span><br><span class="line">X = df.iloc[0:100, [0, 2]].values</span><br><span class="line"></span><br><span class="line"># 可视化数据，因为数据有经过处理，总共150行数据，1-50行是setosa花，51-100是versicolor花，101-150是virginica花</span><br><span class="line">plt.scatter(X[:50, 0], X[:50, 1],</span><br><span class="line">            color=&apos;red&apos;, marker=&apos;o&apos;, label=&apos;setosa&apos;)</span><br><span class="line">plt.scatter(X[50:100, 0], X[50:100, 1],</span><br><span class="line">            color=&apos;blue&apos;, marker=&apos;x&apos;, label=&apos;versicolor&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlabel(&apos;sepal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;petal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/wR17A5s.png" alt=""></p><p><strong>训练感知器模型</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Perceptron是我们前面定义的感知器算法函数，这里就直接调用就好</span><br><span class="line">ppn = Perceptron(eta=0.1, n_iter=10)</span><br><span class="line"></span><br><span class="line">ppn.fit(X, y)</span><br><span class="line"></span><br><span class="line">plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=&apos;o&apos;)</span><br><span class="line">plt.xlabel(&apos;迭代次数&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;权重更新次数（错误次数）&apos;,FontProperties=font,fontsize=14)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/e6o2LBT.png" alt=""></p><p><strong>绘制函数决策区域</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib.colors import ListedColormap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot_decision_regions(X, y, classifier, resolution=0.02):</span><br><span class="line"></span><br><span class="line">    # setup marker generator and color map</span><br><span class="line">    markers = (&apos;s&apos;, &apos;x&apos;, &apos;o&apos;, &apos;^&apos;, &apos;v&apos;)</span><br><span class="line">    colors = (&apos;red&apos;, &apos;blue&apos;, &apos;lightgreen&apos;, &apos;gray&apos;, &apos;cyan&apos;)</span><br><span class="line">    cmap = ListedColormap(colors[:len(np.unique(y))])</span><br><span class="line"></span><br><span class="line">    # plot the decision surface</span><br><span class="line">    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),</span><br><span class="line">                           np.arange(x2_min, x2_max, resolution))</span><br><span class="line">    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br><span class="line">    Z = Z.reshape(xx1.shape)</span><br><span class="line">    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)</span><br><span class="line">    plt.xlim(xx1.min(), xx1.max())</span><br><span class="line">    plt.ylim(xx2.min(), xx2.max())</span><br><span class="line"></span><br><span class="line">    # plot class samples</span><br><span class="line">    for idx, cl in enumerate(np.unique(y)):</span><br><span class="line">        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],</span><br><span class="line">                    alpha=0.8, c=cmap(idx),</span><br><span class="line">                    edgecolor=&apos;black&apos;,</span><br><span class="line">                    marker=markers[idx], </span><br><span class="line">                    label=cl)</span><br><span class="line">plot_decision_regions(X, y, classifier=ppn)</span><br><span class="line">plt.xlabel(&apos;sepal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;petal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/AFltDVw.png" alt=""></p><hr><h3 id="自适应线性神经元和融合学习"><a href="#自适应线性神经元和融合学习" class="headerlink" title="自适应线性神经元和融合学习"></a><strong>自适应线性神经元和融合学习</strong></h3><p><strong>使用梯度下降方法来最小化损失函数</strong></p><p>梯度下降的方法十分常见，具体的了解可以参考附录的文章[2]，如今，梯度下降主要用于在神经网络模型中进行权重更新，即在一个方向上更新和调整模型的参数，来最小化损失函数。</p><p><img src="https://i.imgur.com/pYoV9cF.jpg" alt=""><br>图：梯度下降原理过程演示</p><p><strong>在Python中实现一个自适应的线性神经元</strong></p><p>先贴上定义的python函数，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"># 定义神经元函数</span><br><span class="line">class AdalineGD(object):</span><br><span class="line">    &quot;&quot;&quot;ADAptive LInear NEuron classifier.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ------------</span><br><span class="line">    eta : float</span><br><span class="line">        Learning rate (between 0.0 and 1.0)</span><br><span class="line">    n_iter : int</span><br><span class="line">        Passes over the training dataset.</span><br><span class="line"></span><br><span class="line">    Attributes</span><br><span class="line">    -----------</span><br><span class="line">    w_ : 1d-array</span><br><span class="line">        Weights after fitting.</span><br><span class="line">    cost_ : list</span><br><span class="line">        Sum-of-squares cost function value in each epoch.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, eta=0.01, n_iter=50):</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y):</span><br><span class="line">        &quot;&quot;&quot; Fit training data.</span><br><span class="line"></span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span><br><span class="line">            Training vectors, where n_samples is the number of samples and</span><br><span class="line">            n_features is the number of features.</span><br><span class="line">        y : array-like, shape = [n_samples]</span><br><span class="line">            Target values.</span><br><span class="line"></span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        self : object</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.w_ = np.zeros(1 + X.shape[1])</span><br><span class="line">        self.cost_ = []</span><br><span class="line"></span><br><span class="line">        for i in range(self.n_iter):</span><br><span class="line">            net_input = self.net_input(X)</span><br><span class="line">            # Please note that the &quot;activation&quot; method has no effect</span><br><span class="line">            # in the code since it is simply an identity function. We</span><br><span class="line">            # could write `output = self.net_input(X)` directly instead.</span><br><span class="line">            # The purpose of the activation is more conceptual, i.e.,  </span><br><span class="line">            # in the case of logistic regression, we could change it to</span><br><span class="line">            # a sigmoid function to implement a logistic regression classifier.</span><br><span class="line">            output = self.activation(X)</span><br><span class="line">            errors = (y - output)</span><br><span class="line">            self.w_[1:] += self.eta * X.T.dot(errors)</span><br><span class="line">            self.w_[0] += self.eta * errors.sum()</span><br><span class="line">            cost = (errors**2).sum() / 2.0</span><br><span class="line">            self.cost_.append(cost)</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def net_input(self, X):</span><br><span class="line">        &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span><br><span class="line">        return np.dot(X, self.w_[1:]) + self.w_[0]</span><br><span class="line"></span><br><span class="line">    def activation(self, X):</span><br><span class="line">        &quot;&quot;&quot;Compute linear activation&quot;&quot;&quot;</span><br><span class="line">        return self.net_input(X)</span><br><span class="line"></span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span><br><span class="line">        return np.where(self.activation(X) &gt;= 0.0, 1, -1)</span><br></pre></td></tr></table></figure></p><p><strong>查看不同学习率下的错误率随迭代次数的变化情况：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))</span><br><span class="line"></span><br><span class="line"># 可视化W调整的过程中，错误率随迭代次数的变化</span><br><span class="line">ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)</span><br><span class="line">ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker=&apos;o&apos;)</span><br><span class="line">ax[0].set_xlabel(&apos;Epochs&apos;)</span><br><span class="line">ax[0].set_ylabel(&apos;log(Sum-squared-error)&apos;)</span><br><span class="line">ax[0].set_title(&apos;Adaline - Learning rate 0.01&apos;)</span><br><span class="line"></span><br><span class="line">ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)</span><br><span class="line">ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker=&apos;o&apos;)</span><br><span class="line">ax[1].set_xlabel(&apos;Epochs&apos;)</span><br><span class="line">ax[1].set_ylabel(&apos;Sum-squared-error&apos;)</span><br><span class="line">ax[1].set_title(&apos;Adaline - Learning rate 0.0001&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/g6mKKU3.png" alt=""></p><p><strong>iris数据的应用情况：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 标准化特征</span><br><span class="line">X_std = np.copy(X)</span><br><span class="line">X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()</span><br><span class="line">X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()</span><br><span class="line"># 调用函数开始训练</span><br><span class="line">ada = AdalineGD(n_iter=15, eta=0.01)</span><br><span class="line">ada.fit(X_std, y)</span><br><span class="line"># 绘制效果</span><br><span class="line">plot_decision_regions(X_std, y, classifier=ada)</span><br><span class="line">plt.title(&apos;Adaline - Gradient Descent&apos;)</span><br><span class="line">plt.xlabel(&apos;sepal length [standardized]&apos;)</span><br><span class="line">plt.ylabel(&apos;petal length [standardized]&apos;)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"># 可视化W调整的过程中，错误率随迭代次数的变化</span><br><span class="line">plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=&apos;o&apos;)</span><br><span class="line">plt.xlabel(&apos;Epochs&apos;)</span><br><span class="line">plt.ylabel(&apos;Sum-squared-error&apos;)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/kRorVXJ.png" alt=""></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>1）<a href="https://blog.csdn.net/u013719780/article/details/51755409" target="_blank" rel="noopener">机器学习系列：感知器</a><br>2）<a href="https://blog.csdn.net/zyq522376829/article/details/66632699" target="_blank" rel="noopener">机器学习入门系列04，Gradient Descent（梯度下降法）</a><br>3）<a href="https://zhuanlan.zhihu.com/p/27449596?utm_source=weibo&amp;utm_medium=social" target="_blank" rel="noopener">一文看懂各种神经网络优化算法：从梯度下降到Adam方法</a><br>4）<a href="https://blog.csdn.net/huakai16/article/details/77701020" target="_blank" rel="noopener">机器学习与神经网络（三）：自适应线性神经元的介绍和Python代码实现</a><br>5）<a href="http://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb" target="_blank" rel="noopener">《Training Machine Learning Algorithms for Classification》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/62602/&quot;&gt;机器学习(三) 模型结果应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/21484/&quot;&gt;机器学习(四) 常见算法优缺点&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文章结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;什么是感知器分类算法&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;在Python中实现感知器学习算法&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;在iris（鸢尾花）数据集上训练一个感知器模型&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;自适应线性神经元和融合学习&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;使用梯度下降方法来最小化损失函数&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;在Python中实现一个自适应的线性神经元&lt;/em&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="算法" scheme="https://paradoxallen.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(4) 常见算法优缺点</title>
    <link href="https://paradoxallen.github.io/21484/"/>
    <id>https://paradoxallen.github.io/21484/</id>
    <published>2017-06-04T16:00:00.000Z</published>
    <updated>2018-06-13T05:09:08.547Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p><a href="https://paradoxallen.github.io/62602/">机器学习(三) 模型结果应用</a></p><p>机器学习算法我们了解了很多，但是放在一起来比较优缺点是缺少的，本篇文章就一些常见的算法来进行一次优缺点梳理。</p><a id="more"></a><hr><h3 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a><strong>决策树算法</strong></h3><h4 id="一、决策树优点"><a href="#一、决策树优点" class="headerlink" title="一、决策树优点"></a><strong>一、决策树优点</strong></h4><p>1、决策树易于理解和解释，可以可视化分析，容易提取出规则。</p><p>2、可以同时处理标称型和数值型数据。</p><p>3、测试数据集时，运行速度比较快。</p><p>4、决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。</p><h4 id="二、决策树缺点"><a href="#二、决策树缺点" class="headerlink" title="二、决策树缺点"></a><strong>二、决策树缺点</strong></h4><p>1、对缺失数据处理比较困难。</p><p>2、容易出现过拟合问题。</p><p>3、忽略数据集中属性的相互关联。</p><p>4、ID3算法计算信息增益时结果偏向数值比较多的特征。</p><h4 id="三、改进措施"><a href="#三、改进措施" class="headerlink" title="三、改进措施"></a><strong>三、改进措施</strong></h4><p>1、对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。</p><p>2、使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题</p><h4 id="四、常见算法"><a href="#四、常见算法" class="headerlink" title="四、常见算法"></a><strong>四、常见算法</strong></h4><h5 id="一）C4-5算法"><a href="#一）C4-5算法" class="headerlink" title="一）C4.5算法"></a><strong>一）C4.5算法</strong></h5><p>ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定的测试属性。</p><p>C4.5算法核心思想是ID3算法，是ID3算法的改进，改进方面有：</p><ul><li><p>用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；</p></li><li><p>在树构造过程中进行剪枝；</p></li><li><p>能处理非离散的数据；</p></li><li><p>能处理不完整的数据。</p></li></ul><p><strong>优点</strong>：产生的分类规则易于理解，准确率较高。</p><p><strong>缺点</strong>：</p><p>1）在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效；</p><p>2）C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</p><h5 id="二）CART分类与回归树"><a href="#二）CART分类与回归树" class="headerlink" title="二）CART分类与回归树"></a><strong>二）CART分类与回归树</strong></h5><p>是一种决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数<br>据集生成的决策树的拓展形。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。</p><p><strong>优点</strong></p><p>1）非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。</p><p>2）在面对诸如存在缺失值、变量数多等问题时CART 显得非常稳健。</p><hr><h3 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a><strong>分类算法</strong></h3><h4 id="一、KNN算法"><a href="#一、KNN算法" class="headerlink" title="一、KNN算法"></a><strong>一、KNN算法</strong></h4><p><strong>KNN算法的优点</strong> </p><p>1、KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练</p><p>2、KNN理论简单，容易实现</p><p><strong>KNN算法的缺点</strong></p><p>1、对于样本容量大的数据集计算量比较大。</p><p>2、样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。</p><p>3、KNN每一次分类都会重新进行一次全局运算。</p><p>4、k值大小的选择。</p><p><strong>KNN算法应用领域</strong></p><p>文本分类、模式识别、聚类分析，多分类领域</p><h4 id="二、支持向量机（SVM）"><a href="#二、支持向量机（SVM）" class="headerlink" title="二、支持向量机（SVM）"></a><strong>二、支持向量机（SVM）</strong></h4><p>支持向量机是一种基于分类边界的方法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。基于分类边界的分类算法的目标是，通过训练，找到这些分类之间的边界（直线的――称为线性划分，曲线的――称为非线性划分）。对于多维数据（如N维），可以将它们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面（超面比N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。</p><p>支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。</p><p><strong>SVM优点</strong></p><p>1、解决小样本下机器学习问题。<br>2、解决非线性问题。<br>3、无局部极小值问题。（相对于神经网络等算法）<br>4、可以很好的处理高维数据集。<br>5、泛化能力比较强。</p><p><strong>SVM缺点</strong></p><p>1、对于核函数的高维映射解释力不强，尤其是径向基函数。<br>2、对缺失数据敏感。</p><p><strong>SVM应用领域</strong></p><p>文本分类、图像识别、主要二分类领域</p><h4 id="三、朴素贝叶斯算法"><a href="#三、朴素贝叶斯算法" class="headerlink" title="三、朴素贝叶斯算法"></a><strong>三、朴素贝叶斯算法</strong></h4><p><strong>朴素贝叶斯算法优点</strong></p><p>1、对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。<br>2、支持增量式运算。即可以实时的对新增的样本进行训练。<br>3、朴素贝叶斯对结果解释容易理解。</p><p><strong>朴素贝叶斯缺点</strong></p><p>1、由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。</p><p><strong>朴素贝叶斯应用领域</strong></p><p>文本分类、欺诈检测中使用较多</p><h4 id="四、Logistic回归算法"><a href="#四、Logistic回归算法" class="headerlink" title="四、Logistic回归算法"></a><strong>四、Logistic回归算法</strong></h4><p><strong>logistic回归优点</strong></p><p>1、计算代价不高，易于理解和实现</p><p><strong>logistic回归缺点</strong></p><p>1、容易产生欠拟合。</p><p>2、分类精度不高。</p><p><strong>logistic回归应用领域</strong></p><p>用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。</p><p>Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。</p><hr><h3 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a><strong>聚类算法</strong></h3><h4 id="一、K-means-算法"><a href="#一、K-means-算法" class="headerlink" title="一、K means 算法"></a><strong>一、K means 算法</strong></h4><p>是一个简单的聚类算法，把n的对象根据他们的属性分为k个分割，k&lt; n。 算法的核心就是要优化失真函数J,使其收敛到局部最小值但不是全局最小值。<br>其中N为样本数，K是簇数，rnk b表示n属于第k个簇，uk 是第k个中心点的值。然后求出最优的uk</p><p><strong>优点</strong>：算法速度很快</p><p><strong>缺点</strong>：分组的数目k是一个输入参数，不合适的k可能返回较差的结果。</p><h4 id="二、EM最大期望算法"><a href="#二、EM最大期望算法" class="headerlink" title="二、EM最大期望算法"></a><strong>二、EM最大期望算法</strong></h4><p>EM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。</p><p>EM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。</p><hr><h3 id="集成算法（AdaBoost算法）"><a href="#集成算法（AdaBoost算法）" class="headerlink" title="集成算法（AdaBoost算法）"></a><strong>集成算法（AdaBoost算法）</strong></h3><h4 id="一、-AdaBoost算法优点"><a href="#一、-AdaBoost算法优点" class="headerlink" title="一、  AdaBoost算法优点"></a><strong>一、  AdaBoost算法优点</strong></h4><p>1、很好的利用了弱分类器进行级联。</p><p>2、可以将不同的分类算法作为弱分类器。</p><p>3、AdaBoost具有很高的精度。</p><p>4、相对于bagging算法和Random Forest算法，AdaBoost充分考虑的每个分类器的权重。</p><h4 id="二、Adaboost算法缺点"><a href="#二、Adaboost算法缺点" class="headerlink" title="二、Adaboost算法缺点"></a><strong>二、Adaboost算法缺点</strong></h4><p>1、AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。</p><p>2、数据不平衡导致分类精度下降。</p><p>3、训练比较耗时，每次重新选择当前分类器最好切分点。</p><h4 id="三、AdaBoost应用领域"><a href="#三、AdaBoost应用领域" class="headerlink" title="三、AdaBoost应用领域"></a><strong>三、AdaBoost应用领域</strong></h4><p>模式识别、计算机视觉领域，用于二分类和多分类场景</p><hr><h3 id="人工神经网络算法"><a href="#人工神经网络算法" class="headerlink" title="人工神经网络算法"></a><strong>人工神经网络算法</strong></h3><h4 id="一、神经网络优点"><a href="#一、神经网络优点" class="headerlink" title="一、神经网络优点"></a><strong>一、神经网络优点</strong></h4><p>1、分类准确度高，学习能力极强。</p><p>2、对噪声数据鲁棒性和容错性较强。</p><p>3、有联想能力，能逼近任意非线性关系。</p><h4 id="二、神经网络缺点"><a href="#二、神经网络缺点" class="headerlink" title="二、神经网络缺点"></a><strong>二、神经网络缺点</strong></h4><p>1、神经网络参数较多，权值和阈值。</p><p>2、黑盒过程，不能观察中间结果。</p><p>3、学习过程比较长，有可能陷入局部极小值。</p><h4 id="三、人工神经网络应用领域"><a href="#三、人工神经网络应用领域" class="headerlink" title="三、人工神经网络应用领域"></a><strong>三、人工神经网络应用领域</strong></h4><p>目前深度神经网络已经应用与计算机视觉，自然语言处理，语音识别等领域并取得很好的效果。</p><hr><h3 id="排序算法（PageRank）"><a href="#排序算法（PageRank）" class="headerlink" title="排序算法（PageRank）"></a><strong>排序算法（PageRank）</strong></h3><p>PageRank是google的页面排序算法，是基于从许多优质的网页链接过来的网页，必定还是优质网页的回归关系，来判定所有网页的重要性。（也就是说，一个人有着越多牛X朋友的人，他是牛X的概率就越大。）</p><h4 id="一、PageRank优点"><a href="#一、PageRank优点" class="headerlink" title="一、PageRank优点"></a><strong>一、PageRank优点</strong></h4><p>完全独立于查询，只依赖于网页链接结构，可以离线计算。</p><h4 id="二、PageRank缺点"><a href="#二、PageRank缺点" class="headerlink" title="二、PageRank缺点"></a><strong>二、PageRank缺点</strong></h4><p>1）PageRank算法忽略了网页搜索的时效性。</p><p>2）旧网页排序很高，存在时间长，积累了大量的in-links，拥有最新资讯的新网页排名却很低，因为它们几乎没有in-links。</p><hr><h3 id="关联规则算法（Apriori算法）"><a href="#关联规则算法（Apriori算法）" class="headerlink" title="关联规则算法（Apriori算法）"></a><strong>关联规则算法（Apriori算法）</strong></h3><p>Apriori算法是一种挖掘关联规则的算法，用于挖掘其内含的、未知的却又实际存在的数据关系，其核心是基于两阶段频集思想的递推算法 。</p><p><strong>Apriori算法分为两个阶段：</strong></p><p>1）寻找频繁项集</p><p>2）由频繁项集找关联规则</p><p><strong>算法缺点：</strong></p><p>1）在每一步产生侯选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；</p><p>2） 每次计算项集的支持度时，都对数据库中    的全部记录进行了一遍扫描比较，需要很大的I/O负载。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1）Jason Brownlee  《How To Use Machine Learning Results》</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/62602/&quot;&gt;机器学习(三) 模型结果应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;机器学习算法我们了解了很多，但是放在一起来比较优缺点是缺少的，本篇文章就一些常见的算法来进行一次优缺点梳理。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="算法" scheme="https://paradoxallen.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(3) 模型结果应用</title>
    <link href="https://paradoxallen.github.io/62602/"/>
    <id>https://paradoxallen.github.io/62602/</id>
    <published>2017-05-31T16:00:00.000Z</published>
    <updated>2018-06-13T05:08:51.908Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p>当你有了一个相当不错的模型结果了，这个时间就需要上线应用了，但实际上这个过程也是需要注意很多东西的呢，比如汇报你的项目结果、上线计划沟通、上线后的监控等等，这都是相当重要的。</p><a id="more"></a><hr><p>永远要记得，建立模型只是为了解决业务问题，<strong>模型只是一个工具而已</strong>，所以，脱离具体业务场景的模型都是假的。所以，在一开始，就要对自己的目标进行明确，在做完了模型后再度审视自己目标，看下自己做出来的模型是否仍是解决这个目标。</p><p>根据你试图解决的问题类型，我们可以大致分为两种呈现方式：</p><ul><li><p><strong>报告汇报式</strong></p></li><li><p><strong>部署上线式</strong></p></li></ul><p>当然了，实际上更多的是两种方式的融合，即两个都需要做，那么下面我们就分别来说一下这两种方式在实际操作上都需要做些什么呗。</p><h2 id="报告汇报式"><a href="#报告汇报式" class="headerlink" title="报告汇报式"></a>报告汇报式</h2><p>一旦你发现了一个很不错的模型，并且训练的结果也很不错，你此时就需要总结这一切内容，并很好的展示给你的观众（可以是老板、客户或者是同事等），而此时如何完美地展示显得格外重要。</p><p>展示的最好方式我个人觉得是ppt，但有些地方更偏好于单页报告，不过也不影响，下面罗列的内容，都是可以在这两种方式内容上展示的，不管你现在做的模型是什么，比赛的、教程的，还是工作的，都可以试着去总结这些关键点，完成一次报告的撰写。</p><ul><li><p><strong><em>Context</em></strong> (Why): 定义问题存在的大背景，并且说明研究的动机或目的。</p></li><li><p><strong><em>Problem</em></strong> (Question): 简单扼要地把问题描述一个具体需要解决的问题并回答它。</p></li><li><p><strong><em>Solution</em></strong> (Answer): 简单扼要地描述关于上一个环节提出的问题的解决方案，而且要详细具体。</p></li><li><p><strong><em>Findings</em></strong>: 罗列一下你在建模过程中发现的一些有价值的点，比如在数据上的发现，又或者是原先方式的缺点及现有方式的优点，也可以是模型在性能方面的优势等等。</p></li><li><p><strong><em>Limitations</em></strong>: 考虑模型能力所不能覆盖的点，或者是方案不能解决的点。不要回避这些问题，你不说别人也会问，而且，只有你重新认识模型的短处，才能知道模型的优点。</p></li><li><p><strong><em>Conclusions</em></strong> (Why+Question+Answer): 回顾目的、研究的问题及解决方案，并将它们尽可能压缩在几句话，让人能够记住。</p></li></ul><p>如果是自己平时做练习的项目，我觉得可以多按照上面的点来描述自己的项目结果，并且将报告上传到社区网络，让更多的人来评价，你从中也可以得到更多的反馈，这对你下一次的报告有很大的帮助。</p><h2 id="部署上线式"><a href="#部署上线式" class="headerlink" title="部署上线式"></a>部署上线式</h2><p>同样的，你有一个训练得很不错的模型，这时候需要将它部署到生产系统中，你需要确定很多东西，比如调用的环节、入参出参以及各种接口开发，下面有3个方面的内容需要在做这些事情之前进行考虑，分别是：<strong>算法实现、模型自动化测试、模型效果追踪</strong>。</p><p><strong>1）算法实现</strong></p><p>其实python里有很多算法都是可以直接通过库来调用的，但这对于一般情况下是很好用的，但是如果涉及到要具体部署应用，这要考虑的东西就多了。</p><p>在你考虑部署一个新模型在现有的生产系统上，你需要非常仔细地研究这可能需要产生的依赖项和“技术负债”（这里可以理解为一些所需的技术，包括硬软件）。所以，在建模前，需要考虑去查找能匹配你的方法的公司生产级别的库，要不然，等到要上线的时候，你就需要重复模型调优的过程了哦。</p><p><strong>2）模型自动化测试</strong></p><p>编写自动化测试代码，对模型的应用进行验证，监控在实际的使用过程中，并且能够重复实现模型效果的最低水平，尽可能是可以对不同的数据都可以随机性地测试。</p><p><strong>3）模型效果追踪</strong></p><p>增添一些基础设施来监控模型的性能，并且可以在精度低于最低水平的时候发出警报，追踪模型实时或者离线的数据样本的效果，包括入参。当你发现不仅仅是模型效果发生了很大的变化，就连入参也有很大的变化，那这个时候就需要考虑模型的更新或者重构了。</p><p>当然，有一些模型是可以实现在线自我学习并且更新自己的，但并不是所有的生产系统可以支持这种操作，毕竟这种还只是一个比较先进的办法，仍存在很多不太完善的地方。比较传统的方式还是对现有的模型进行人工管理，人工更新与切换，这样子显得更加明智而且稳健。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1）Jason Brownlee  《How To Use Machine Learning Results》</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当你有了一个相当不错的模型结果了，这个时间就需要上线应用了，但实际上这个过程也是需要注意很多东西的呢，比如汇报你的项目结果、上线计划沟通、上线后的监控等等，这都是相当重要的。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="模型" scheme="https://paradoxallen.github.io/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(2) 模型调优</title>
    <link href="https://paradoxallen.github.io/4840/"/>
    <id>https://paradoxallen.github.io/4840/</id>
    <published>2017-05-24T16:00:00.000Z</published>
    <updated>2018-06-13T05:08:37.892Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一)算法介绍</a></p><p>前面讲了一些机器学习的算法的介绍，如果有一些数据这时候也可以建立出自己的模型了，但是，如果模型的效果不尽人意，那么应该如何调整呢？</p><p>以下是一份关于模型调优的方法，每当出现效果不好的时候或者是在建模前，都可以按照这个来进行检查，话不多说，一起来看～</p><p><img src="https://i.imgur.com/MHeHLhM.jpg" alt=""></p><a id="more"></a><hr><p>为了文章阅读的清晰，先在前面简单说明一下文章的目录框架。</p><p>本文存在的意义在于辅助大家提升机器学习模型的效果，方法有很多，如果你在其中的一个办法中找到了突破，仍可以回头再找其他，直到满足你的模型效果要求，主要从4个角度来进行方法的阐述，分别是：</p><ul><li>Improve Performance With Data.（数据）</li><li>Improve Performance With Algorithms.（算法选择）</li><li>Improve Performance With Algorithm Tuning.（算法调优）</li><li>Improve Performance With Ensembles.（效果集成）</li></ul><p>好的，下面就分别从这4个角度来说一下。</p><h3 id="1-Improve-Performance-With-Data（数据）"><a href="#1-Improve-Performance-With-Data（数据）" class="headerlink" title="1. Improve Performance With Data（数据）"></a>1. Improve Performance With Data（数据）</h3><p>事实上，你直接改变训练数据或者改变目标的定义，好效果会来得更加“不费吹灰之力”，有的时候还可能是最好的操作，所以有一句话说得很有道理：<strong>选择比努力重要哈哈哈</strong>。</p><p>话不多说，说下Strategy: 创建不同的目标样本并且尽量使用最底层的特征来训练模型算法。</p><p><strong>具体策略：</strong></p><p>获得更多的数据：一个好的深度学习模型需要更多的数据来训练，其他非线性的机器学习模型也是如此。</p><ul><li>开发更多变量：如果你实在不能获得更多的数据亦或是更好质量的数据，也许可以通过概率模型、统计模型来生成新的变量。</li><li>清洗数据：通过数据清洗，你可以对缺失值、异常值进行合理的填补与修复，从而提升数据整体的质量。</li><li>重新采样数据：其实可以通过对数据的重新采样来改变数据的分布和大小。对某一特定类型的样本进行采样，说不定可以更好滴表现出效果。又或者是使用更小的数据，从而加快速度。</li><li>问题解决思路的重新思考：有的时候，你可以把你目前正在“焦头烂耳”想要解决的“预测性”问题，换成回归、分类、时间序列、异常检测、排序、推荐等等的问题。</li><li>调整变量再入模：这里指的是对数据进行离散化、标准化等的操作。</li><li>改变现有的变量：这里相信大家也很常见，就是对变量进行对数转换或指数转换，让其特性能更好地表现。</li><li>对变量进行降维：有的时候降维后的变量有更好的表现哦。</li><li>特征选择：这个就是特征工程了，简单来说，就是你对特征（变量）进行重要性的排序，选择相对预测力强的特征进入模型。</li></ul><h3 id="2-Improve-Performance-With-Algorithms（算法选择）"><a href="#2-Improve-Performance-With-Algorithms（算法选择）" class="headerlink" title="2. Improve Performance With Algorithms（算法选择）"></a>2. Improve Performance With Algorithms（算法选择）</h3><p>机器学习其实都是关于算法的学习。</p><p>Strategy: 识别出优于平均值的算法，但要对其实验过程以及结果抱着怀疑态度，并反复思考。</p><p><strong>具体策略：</strong></p><ul><li>重采样方法：使用什么方法来估计效果？有个原则就是要充分利用可用的数据来验证，这里，k-fold交叉验证方法可以是最好的哦。</li><li>评价指标：不同的目标需要使用不同的评价指标，这个相信大家在学习混淆矩阵的时候应该有所了解，什么pv+，命中率等等，都是对于特定类型的目标有着非常有效的识别。如果是一个排序性的问题，而你却用了准确度的指标来衡量模型的好坏似乎也说不过去把？</li><li>关注线性算法：线性算法通常会不那么好用，但是却更好地被人类理解且可以快速测试，如果你发现某个线性算法表现地还行，请继续优化它。</li><li>关注非线性算法：非线性算法往往会需要更多的数据，通过更加复杂的计算来获得一个不错的效果。</li><li>从文献中找ideas：这个方法还经常做，从文献中可以了解到更多的经典算法在特定需要下的应用，通过对文献的阅读来扩充你的“解题”思路把。</li></ul><h3 id="3-Improve-Performance-With-Algorithm-Tuning（算法调优）"><a href="#3-Improve-Performance-With-Algorithm-Tuning（算法调优）" class="headerlink" title="3. Improve Performance With Algorithm Tuning（算法调优）"></a>3. Improve Performance With Algorithm Tuning（算法调优）</h3><p>模型调参也是一个非常费时间的环节，有的时候“好运”可以马上抽查出表现还不错的结果，并持续调参，就可以得到一个不错的结果。但如果要对其他所有的算法进行优化，那么需要的时间就可能是几天、几个星期或者几个月了。</p><p>Strategy: 充分利用性能良好的机器学习算法。</p><p><strong>具体策略：</strong></p><ul><li>诊断方法：不同的算法需要提供不同的可视化和诊断的方法。</li><li>调参的直觉：这个就很“玄学”了，但其实都是一些经验，当你调的参足够多，也可以大致可以对这些不同算法的参数有了自己的理解，自然就有了这些所谓的“直觉”。</li><li>随机搜索：在N维参数空间按某种分布（如正态分布）随机取值，因为参数空间的各个维度的重要性是不等的，随机搜索方法可以在不重要的维度上取巧。</li><li>网格搜索：先固定一个超参，然后对其他各个超参依次进行穷举搜索。</li><li>从文献中找ideas：从文献中了解这个算法用到了哪些算法，并且这些算法主要的取值值域，有益于自身工作的开展哦。</li><li>从知名网站中找ideas：国内我个人觉得知乎还是蛮可以的，关于这节的参数调参，也是有好多好文章，其外还有csdn也不错。</li></ul><h3 id="4-Improve-Performance-With-Ensembles（效果集成）"><a href="#4-Improve-Performance-With-Ensembles（效果集成）" class="headerlink" title="4. Improve Performance With Ensembles（效果集成）"></a>4. Improve Performance With Ensembles（效果集成）<code></code></h3><p>这个算法集成的方法也是非常常用的，你可以结合多个模型的结果，综合输出一个更加稳定且效果不错的结果。</p><p>Strategy: 结合各种模型的预测结果并输出。</p><p><strong>具体策略：</strong></p><ul><li>混合模型的预测值：你可以把多个模型的预测结果结合起来，你可以将多个训练效果还不错的模型的预测结合综合起来，输出一个“平均”结果。</li><li>混合不同数据的预测值：你也可以把不同的数据集训练出来模型的结果进行结合，作为一个输出。（这个与上面的区别在于数据集的特征不同）</li><li>混合数据样本：很拗口，其实意思就是将数据集拆分成不同的子数据集，用于训练同一个算法，最后输出综合的预测结果。这个也被称之为bootstrap aggregation 或 bagging。</li><li>使用模型的方法集成：你也可以使用一个新的模型来学习如何结合多个性能不错的模型结果，输出一个最优的结合。这被称之为堆叠泛化或叠加，通常在子模型有技巧时很有效，但在不同的方式下，聚合器模型是预测的一个简单的线性加权。这个过程可以重复多层深度。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1）<a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" target="_blank" rel="noopener">A Tour of Machine Learning Algorithms</a></p><p>2）<a href="https://www.zhihu.com/question/34470160?sort=created" target="_blank" rel="noopener">机器学习各种算法怎么调参?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一)算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前面讲了一些机器学习的算法的介绍，如果有一些数据这时候也可以建立出自己的模型了，但是，如果模型的效果不尽人意，那么应该如何调整呢？&lt;/p&gt;
&lt;p&gt;以下是一份关于模型调优的方法，每当出现效果不好的时候或者是在建模前，都可以按照这个来进行检查，话不多说，一起来看～&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/MHeHLhM.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="模型" scheme="https://paradoxallen.github.io/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>A Tour of Machine Learning Algorithms(1) 算法介绍</title>
    <link href="https://paradoxallen.github.io/40392/"/>
    <id>https://paradoxallen.github.io/40392/</id>
    <published>2017-05-21T16:00:00.000Z</published>
    <updated>2018-06-13T05:03:57.846Z</updated>
    
    <content type="html"><![CDATA[<p>接下来的文章基于来自Jason Brownlee 的文章《A Tour of Machine Learning Algorithms》</p><a id="more"></a><hr><p>算法很多，如何做好分组有助于我们更好记住它们，主要有2条算法分组的方式：</p><ul><li><p>The first is a grouping of algorithms by the learning style.（通过算法的学习方式）</p></li><li><p>The second is a grouping of algorithms by similarity in form or function (like grouping similar animals together).（通过算法的功能）</p></li></ul><p>下面就会从这2个角度来阐述一下机器学习的算法。</p><p><img src="https://i.imgur.com/yQR3iN4.png" alt=""></p><h2 id="Algorithms-Grouped-by-Learning-Style（通过算法的学习方式）"><a href="#Algorithms-Grouped-by-Learning-Style（通过算法的学习方式）" class="headerlink" title="Algorithms Grouped by Learning Style（通过算法的学习方式）"></a>Algorithms Grouped by Learning Style（通过算法的学习方式）</h2><p>关于机器学习算法，有三种不同的学习方式：</p><h3 id="1-Supervised-Learning（监督学习）"><a href="#1-Supervised-Learning（监督学习）" class="headerlink" title="1. Supervised Learning（监督学习）"></a>1. Supervised Learning（监督学习）</h3><p>当输入的数据集（我们称之为训练集）的数据有标签，如好坏标签，分类标签等，那么通过这些数据来建立的预测或者分类模型，属于监督学习模型。</p><ul><li><p>经典问题：classification and regression.（分类与回归）</p></li><li><p>经典算法：Logistic Regression and the Back Propagation Neural Network.（逻辑回归算法与BP神经网络算法）</p></li></ul><p><img src="https://i.imgur.com/pBeHrAV.png" alt=""></p><h3 id="2-Unsupervised-Learning（无监督学习）"><a href="#2-Unsupervised-Learning（无监督学习）" class="headerlink" title="2. Unsupervised Learning（无监督学习）"></a>2. Unsupervised Learning（无监督学习）</h3><p>与监督学习相反，训练集中的数据并没有标签，这意味着你需要从这堆没有标签的数据中去提炼它们的特点规则等等，可能是通过数学推理过程来系统地减少冗余，又或者是通过数据相似度来组织数据。</p><ul><li><p>经典问题：clustering, dimensionality reduction and association rule learning.（聚类、降维、规则学习）</p></li><li><p>经典算法：the Apriori algorithm and k-Means.（这个专用名词就不翻译了）</p></li></ul><p><img src="https://i.imgur.com/uXgOV5r.png" alt=""></p><h3 id="3-Semi-Supervised-Learning（半监督学习）"><a href="#3-Semi-Supervised-Learning（半监督学习）" class="headerlink" title="3. Semi-Supervised Learning（半监督学习）"></a>3. Semi-Supervised Learning（半监督学习）</h3><p>顾名思义，半监督学习意味着训练数据有一部分有标签，而一些没有，一般而言，当训练数据量过少时，监督学习得到的模型效果不能满足需求，因此用半监督学习来增强效果。</p><ul><li><p>经典问题：classification and regression.</p></li><li><p>经典算法：半监督SVM，高斯模型，KNN模型</p></li></ul><p><img src="https://i.imgur.com/FGSCTCb.png" alt=""></p><h2 id="Algorithms-Grouped-By-Similarity（通过算法的功能）"><a href="#Algorithms-Grouped-By-Similarity（通过算法的功能）" class="headerlink" title="Algorithms Grouped By Similarity（通过算法的功能）"></a>Algorithms Grouped By Similarity（通过算法的功能）</h2><p>根据算法的功能相似性来区分算法也是一种很好的办法，如基于树结构的算法或者基于神经网络的算法。所以我觉得从这个角度来了解这些算法会更加好。<br>即便这是一个很好的方式，但也绝非完美，仍会有一些算法不能简单地被归类，比如Learning Vector Quantization（LVQ，学习矢量量化算法），它既是神经网络，也是基于距离的算法，所以下面的归类也只是适用于大多数算法，但是常用的算法。</p><h3 id="1-Regression-Algorithms（回归算法）"><a href="#1-Regression-Algorithms（回归算法）" class="headerlink" title="1. Regression Algorithms（回归算法）"></a>1. Regression Algorithms（回归算法）</h3><p>回归更多地关注自变量与因变量之间的关系，并通过对误差的测算来建模，回归算法是对于数学统计的一个很好应用，也被纳入统计机器学习中。</p><p>常见的回归算法包括：</p><ul><li><p>Ordinary Least Squares Regression (OLSR，普通最小二乘回归)</p></li><li><p>Linear Regression（线性回归）</p></li><li><p>Logistic Regression（逻辑回归）</p></li><li><p>Stepwise Regression（逐步回归）</p></li><li><p>Adaptive Regression Splines (MARS，多元自适应回归)</p></li><li><p>Locally Estimated Scatterplot Smoothing (LOESS，本地散点平滑估计)</p></li></ul><p><img src="https://i.imgur.com/68VmdwF.png" alt=""></p><h3 id="2-Instance-based-Algorithms（基于距离的算法）"><a href="#2-Instance-based-Algorithms（基于距离的算法）" class="headerlink" title="2. Instance-based Algorithms（基于距离的算法）"></a>2. Instance-based Algorithms（基于距离的算法）</h3><p>基于距离学习的模型非常常见，这类的模型是对训练集数据进行建模并比较新数据与之的距离，而距离的衡量有很多，常见的是欧氏距离、曼哈顿距离等。</p><p>常见的算法包括：</p><ul><li><p>k-Nearest Neighbor (kNN)</p></li><li><p>Learning Vector Quantization (LVQ，学习矢量量化)</p></li><li><p>Self-Organizing Map (SOM，自组织映射)</p></li><li><p>Locally Weighted Learning (LWL，局部加权学习)</p></li></ul><p><img src="https://i.imgur.com/0L3V9S0.png" alt=""></p><h3 id="3-Regularization-Algorithms（正则化算法）"><a href="#3-Regularization-Algorithms（正则化算法）" class="headerlink" title="3. Regularization Algorithms（正则化算法）"></a>3. Regularization Algorithms（正则化算法）</h3><p>正则化是对另一种方法(通常是回归方法)的扩展，使基于其复杂性的模型受到惩罚，支持更简单的模型，这些模型在泛化能力方面也比较好。</p><p>常见的正则化算法包括：<br>Ridge Regression（岭回归算法）<br>Least Absolute Shrinkage and Selection Operator (LASSO算法，稀疏约束)<br>Elastic Net（弹性网络）<br>Least-Angle Regression (LARS，最小角回归算法)</p><p><img src="https://i.imgur.com/zAMpcRi.png" alt=""></p><h3 id="4-Decision-Tree-Algorithms（决策树算法）"><a href="#4-Decision-Tree-Algorithms（决策树算法）" class="headerlink" title="4. Decision Tree Algorithms（决策树算法）"></a>4. Decision Tree Algorithms（决策树算法）</h3><p>决策树方法构建基于数据中属性的实际值来建模的，决策树经常被训练用于分类和回归问题，决策树通常是快速和准确的，并且是机器学习中最受欢迎的。</p><p>常见的决策树算法包括：</p><ul><li><p>Classification and Regression Tree (CART，分类回归树算法)</p></li><li><p>Iterative Dichotomiser 3 (ID3)</p></li><li><p>C4.5 and C5.0 (不同版本的区别)</p></li><li><p>Chi-squared Automatic Interaction Detection (CHAID)</p></li><li><p>Decision Stump（决策树桩）</p></li><li><p>MD5（Message-Digest Algorithm，讯息摘要算法）</p></li><li><p>Decision Trees（条件决策树）</p></li></ul><p><img src="https://i.imgur.com/gYqnyoA.png" alt=""></p><h3 id="5-Bayesian-Algorithms（贝叶斯算法）"><a href="#5-Bayesian-Algorithms（贝叶斯算法）" class="headerlink" title="5. Bayesian Algorithms（贝叶斯算法）"></a>5. Bayesian Algorithms（贝叶斯算法）</h3><p>基于贝叶斯定理的方式来构建的算法，常用语分类与回归问题。</p><p>常见的贝叶斯算法包括：</p><ul><li><p>Naive Bayes（朴素贝叶斯）</p></li><li><p>Gaussian Naive Bayes（高斯朴素贝叶斯）</p></li><li><p>Multinomial Naive Bayes（多项式朴素贝叶斯）</p></li><li><p>Averaged One-Dependence Estimators (AODE)</p></li><li><p>Belief Network (BBN，贝叶斯定理网络)</p></li><li><p>Bayesian Network (BN，贝叶斯网络)</p></li></ul><p><img src="https://i.imgur.com/qCapCEh.png" alt=""></p><h3 id="6-Clustering-Algorithms（聚类算法）"><a href="#6-Clustering-Algorithms（聚类算法）" class="headerlink" title="6. Clustering Algorithms（聚类算法）"></a>6. Clustering Algorithms（聚类算法）</h3><p>聚类分析又称群分析，它是研究（样品或指标）分类问题的一种统计分析方法，同时也是数据挖掘的一个重要算法。<br>聚类（Cluster）分析是由若干模式（Pattern）组成的，通常，模式是一个度量（Measurement）的向量，或者是多维空间中的一个点。<br>聚类分析以相似性为基础，在一个聚类中的模式之间比不在同一聚类中的模式之间具有更多的相似性。</p><p>常见的聚类算法包括：<br>k-Means<br>k-Medians<br>Expectation Maximisation (EM，Expectation Maximization Algorithm，是一种迭代算法)<br>Hierarchical Clustering（层次聚类）</p><p><img src="https://i.imgur.com/M9fOxmf.png" alt=""></p><h3 id="7-Association-Rule-Learning-Algorithms（关联规则学习算法）"><a href="#7-Association-Rule-Learning-Algorithms（关联规则学习算法）" class="headerlink" title="7. Association Rule Learning Algorithms（关联规则学习算法）"></a>7. Association Rule Learning Algorithms（关联规则学习算法）</h3><p>关联规则学习方法提取的规则最能解释数据中变量之间的关系，这些规则可以在大型多维数据集中发现重要和商业有用的关联，而被组织利用。</p><p>最常见的算法包括：</p><ul><li><p>Apriori algorithm</p></li><li><p>Eclat algorithm</p></li></ul><p><img src="https://i.imgur.com/xPgGsZt.png" alt=""></p><h3 id="8-Artificial-Neural-Network-Algorithms（人工神经网络算法）"><a href="#8-Artificial-Neural-Network-Algorithms（人工神经网络算法）" class="headerlink" title="8. Artificial Neural Network Algorithms（人工神经网络算法）"></a>8. Artificial Neural Network Algorithms（人工神经网络算法）</h3><p>人工神经网络是受生物神经网络结构和/或功能启发的模型，它们是一类模式匹配，通常用于回归和分类问题，但实际上是一个巨大的子字段，包含数百种算法和各种类型的问题类型。</p><p>最常见的算法包括：</p><ul><li><p>Perceptron（感知器）</p></li><li><p>Back-Propagation（反向传播法）</p></li><li><p>Hopfield Network（霍普菲尔网络）</p></li><li><p>Radial Basis Function Network (RBFN，径向基函数网络)</p></li></ul><p><img src="https://i.imgur.com/EPMHKEF.png" alt=""></p><h3 id="9-Deep-Learning-Algorithms（深度学习算法）"><a href="#9-Deep-Learning-Algorithms（深度学习算法）" class="headerlink" title="9. Deep Learning Algorithms（深度学习算法）"></a>9. Deep Learning Algorithms（深度学习算法）</h3><p>深度学习方法是利用大量廉价计算的人工神经网络的更新，它关心的是构建更大更复杂的神经网络，正如上面所提到的，许多方法都与半监督学习问题有关，在这些问题中，大型数据集包含的标签数据非常少。</p><p>最常见的算法包括：</p><ul><li><p>Deep Boltzmann Machine (DBM)</p></li><li><p>Deep Belief Networks (DBN)</p></li><li><p>Convolutional Neural Network (CNN)</p></li><li><p>Stacked Auto-Encoders</p></li></ul><p><img src="https://i.imgur.com/cPZsY1E.png" alt=""></p><h3 id="10-Dimensionality-Reduction-Algorithms（降维算法）"><a href="#10-Dimensionality-Reduction-Algorithms（降维算法）" class="headerlink" title="10. Dimensionality Reduction Algorithms（降维算法）"></a>10. Dimensionality Reduction Algorithms（降维算法）</h3><p>像聚类方法一样，维数的减少有利于寻找到数据的关联关系，但在这种情况下，是不受监督的方式，或者用较少的信息来概括或描述数据。<br>这些方法中的许多可以用于分类和回归。</p><p>常见的算法包括：</p><ul><li><p>Principal Component Analysis (PCA)</p></li><li><p>Principal Component Regression (PCR)</p></li><li><p>Partial Least Squares Regression (PLSR)</p></li><li><p>Sammon Mapping</p></li><li><p>Multidimensional Scaling (MDS)</p></li><li><p>Projection Pursuit</p></li><li><p>Linear Discriminant Analysis (LDA)</p></li><li><p>Mixture Discriminant Analysis (MDA)</p></li><li><p>Quadratic Discriminant Analysis (QDA)</p></li><li><p>Flexible Discriminant Analysis (FDA)</p></li></ul><p><img src="https://i.imgur.com/EKVzeTU.png" alt=""></p><h3 id="11-Ensemble-Algorithms（集成算法）"><a href="#11-Ensemble-Algorithms（集成算法）" class="headerlink" title="11. Ensemble Algorithms（集成算法）"></a>11. Ensemble Algorithms（集成算法）</h3><p>集成方法是由多个较弱的模型而组成的模型，这些模型是独立训练的，它们的预测在某种程度上是结合在一起来进行总体预测的。<br>这类算法是把更多精力放到了弱学习器身上，以及如何将它们结合起来。这是一门非常强大的技术，因此非常受欢迎。</p><p>常见的算法包括：</p><ul><li><p>Boosting</p></li><li><p>Bootstrapped Aggregation (Bagging)</p></li><li><p>AdaBoost</p></li><li><p>Stacked Generalization (blending)</p></li><li><p>Gradient Boosting Machines (GBM)</p></li><li><p>Gradient Boosted Regression Trees (GBRT)</p></li><li><p>Random Forest</p></li></ul><p><img src="https://i.imgur.com/FpJLEyH.png" alt=""></p><h3 id="12-Other-Algorithms（其他算法）"><a href="#12-Other-Algorithms（其他算法）" class="headerlink" title="12. Other Algorithms（其他算法）"></a>12. Other Algorithms（其他算法）</h3><p>还有很多算法没有被覆盖到，大概还有下面的算法：</p><p>Feature selection algorithms（特征选择算法）</p><ul><li><p>Algorithm accuracy evaluation（算法精度估计）</p></li><li><p>Performance measures（效果评估）</p></li><li><p>Computational intelligence (evolutionary algorithms, etc.)</p></li><li><p>Computer Vision (CV)</p></li><li><p>Natural Language Processing (NLP)</p></li><li><p>Recommender Systems</p></li><li><p>Reinforcement Learning</p></li><li><p>Graphical Models</p></li><li><p>And more…</p></li></ul><p>Further Reading<br>网络上对这些算法有更加详细的讲解，需要大家自己动手去查了，这样子才会更加了解这些算法内容，本文内容来自网络，还有一些我觉得很有用的资料也在下面，大家可以抽时间去细细研究哈。</p><p>##参考资料<br>1）<a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/" target="_blank" rel="noopener">A Tour of Machine Learning Algorithms</a></p><p>2）<a href="https://www.zhihu.com/question/20691338/answer/53910077" target="_blank" rel="noopener">机器学习该如何入门——张松阳的回答</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接下来的文章基于来自Jason Brownlee 的文章《A Tour of Machine Learning Algorithms》&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="模型" scheme="https://paradoxallen.github.io/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>数据可视化(一) 图表简介</title>
    <link href="https://paradoxallen.github.io/41070/"/>
    <id>https://paradoxallen.github.io/41070/</id>
    <published>2017-02-11T16:00:00.000Z</published>
    <updated>2018-06-07T16:37:33.999Z</updated>
    
    <content type="html"><![CDATA[<h4 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h4><p>数据分析界有一句经典名言，字不如表，表不如图。数据可视化是数据分析的主要方向之一。除掉数据挖掘这类高级分析，不少数据分析就是监控数据观察数据。</p><p>数据分析的最终都是要兜售自己的观点和结论的。兜售的最好方式就是做出观点清晰数据详实的PPT给老板看。如果没人认同分析结果，那么分析也不会被改进和优化，不落地的数据分析价值又在哪里？</p><p>各类图表的详细介绍可以查看这篇文章。</p><p>温馨提示：如果您已经熟悉数据可视化，大可不必再看这篇文章，或只挑选部分。</p><a id="more"></a><hr><p>数据可视化是一个热门的概念，是分析师手中的优秀工具。好的可视化是会讲故事的，它向我们揭示了数据背后的规律。</p><p>大家对可视化的使用认知或许来源于下面这张图。虽然结构很清晰，但它更多针对Excel的图表，不够丰富。本文会结合数据分析师的使用场景展示更多的可视化案例。</p><p><img src="https://i.imgur.com/UwcqFXj.jpg" alt=""></p><blockquote><p>为方便演示，文中绝大多数图表为ECharts.js的范例。</p></blockquote><hr><p>了解可视化前，先知悉基础概念。</p><h3 id="维度"><a href="#维度" class="headerlink" title="维度"></a>维度</h3><p>数据分析中经常会提及维度。维度是观察数据的角度和对数据的描述。我们可以说地区是一种维度，这个维度包含上海北京这些城市。也可以认为销售额是一个维度，里面有各类销售数据。</p><p>维度可以用时间、数值表示，也可以用文本，文本常作为类别。数据分析的本质是各种维度的组合，我想了解和分析全国各地的销售额，就需要将地区维度和销售维度结合，如果想知道各个年份的变化，那么再加入时间维度。</p><p>说的再透彻点，Excel首行各字段就可以理解成维度。</p><p><img src="https://i.imgur.com/mOX36Y2.jpg" alt=""></p><p>互联网行业的PV、UV、活跃数也能算作维度。</p><p>图表的绘制依赖多个维度的组合。</p><h3 id="维度类型和转换"><a href="#维度类型和转换" class="headerlink" title="维度类型和转换"></a>维度类型和转换</h3><p>维度主要是三大类的数据结构：文本、时间、数值。地区的上海、北京就是文本维度（也可以称为类别维度），销售额度就是数值维度，时间更好理解了。不同图表有维度使用限制。</p><p>数值维度可以通过其他维度加工计算得出，例如按地区维度，count出有多少是上海的，有多少是北京的。</p><p>维度可以互相转换。比如年龄原本是数值型的维度，但是可以通过对年龄的划分，将其分类为小孩、青年、老年三个年龄段，此时就转换为文本维度。具体按照分析场景使用。</p><p>接下来介绍主要的可视化图表。</p><hr><h3 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h3><p>散点图在报表中不常用到，但是在数据分析中可以算出镜率最高的。散点图通过坐标轴，表示两个变量之间的关系。绘制它依赖大量数据点的分布。</p><p>散点图的优势是揭示数据间的关系，发觉变量与变量之间的关联。</p><p>散点图需要两个数值维度表示X轴、Y轴，下图范例就是身高和体重两个维度。</p><p><img src="https://i.imgur.com/vBTCNJl.jpg" alt=""></p><p>为了进行分析，该图又引入性别维度，通过颜色来区分。</p><p>当我们想知道两个指标互相之间有没有关系，散点图是最好的工具之一。因为它直观。尤其是大数据量，散点图会有更精准的结果。</p><p>后续的学习中，我们也会多次借用到散点图，比如统计中的回归分析，比如数据挖掘中的聚类。</p><h3 id="折线图"><a href="#折线图" class="headerlink" title="折线图"></a>折线图</h3><p>折线图是观察数据的趋势，它和时间是好基友，当我们想要了解某一维度在时间上的规律或者趋势时，就用折线图吧。</p><p><img src="https://i.imgur.com/AgemBW9.jpg" alt=""></p><p>折线图一般使用时间维度作为X轴，数值维度作为Y轴。</p><h3 id="柱形图"><a href="#柱形图" class="headerlink" title="柱形图"></a>柱形图</h3><p>柱形图是分析师最常用到的图表之一，常用于多个维度的比较和变化。</p><p>文本维度／时间维度通常作为X轴。数值型维度作为Y轴。柱形图至少需要一个数值型维度。</p><p>下图就是柱形图的对比分析，通过颜色区分类别。当需要对比的维度过多，柱形图是力不从心的。</p><p><img src="https://i.imgur.com/q4B7jy9.jpg" alt=""></p><p>柱形图和折线图在时间维度的分析中是可以互换的。但推荐使用折线图，因为它对趋势的变化表达更清晰。</p><p>柱形图还有许多丰富的应用。例如堆积柱形图，瀑布图，横向条形图，横轴正负图等。</p><p><img src="https://i.imgur.com/hHrVs1H.jpg" alt=""></p><p>直方图是柱形图的特殊形式。它的数值坐标轴是连续的，专用于统计，表达的是数据分布情况。在统计学的内容会专门讲解。</p><h3 id="地理图"><a href="#地理图" class="headerlink" title="地理图"></a>地理图</h3><p>一切和空间属性有关的分析都可以用到地理图。比如各地区销量，或者某商业区域店铺密集度等。</p><p>地理图一定需要用到坐标维度。可以是经纬度、也可以是地域名称（上海市、北京市）。坐标粒度即能细到具体某条街道，也能宽到世界各国范围。</p><p><img src="https://i.imgur.com/5HeGS3C.jpg" alt=""></p><p>除了经纬度，地理图的绘制离不开地图数据，POI是很重要的要素。POI是“Point of Information”的缩写，可以翻译成信息点，每个POI包含四方面信息，名称、类别、经度纬度、附近的酒店饭店商铺等信息。借助POI，才能按地理维度展现数据。</p><h3 id="饼图"><a href="#饼图" class="headerlink" title="饼图"></a>饼图</h3><p>饼图经常表示一组数据的占比。可以用扇面、圆环、或者多圆环嵌套。商务类的汇报中应用较多。</p><p>为了表示占比，拼图需要数值维度。</p><p><img src="https://i.imgur.com/MyuuzCb.jpg" alt=""></p><p>饼图是有缺陷的，它擅长表达某一占比较大的类别。但是不擅长对比。30%和35%在饼图上凭肉眼是难以分辨出区别的。当类别过多，也不适宜在饼图上表达。</p><p>对数据分析师来说，除了做报告，饼图没啥用。</p><h3 id="雷达图"><a href="#雷达图" class="headerlink" title="雷达图"></a>雷达图</h3><p>也叫蛛网图。可能男同胞们在游戏中看到它比较多。它在商务、财务领域应用较大，适合用在固定的框架内表达某种已知的结果。常见于经营状况，财务健康程度。</p><p>比如我对企业财务进行分析，划分出六大类：销售、市场、研发、客服、技术、管理。通过雷达图绘制出预算和实际开销的维度对比，会很清晰。如下图：</p><p><img src="https://i.imgur.com/cFy4uqW.jpg" alt=""></p><h3 id="箱线图"><a href="#箱线图" class="headerlink" title="箱线图"></a>箱线图</h3><p>箱线图一般人了解的不多，它能准确地反映数据维度的离散（最大数、最小数、中位数、四分数）情况。凡是离散的数据都适用箱线图。</p><p>下图就是箱线图的典型应用。线的上下两端表示某组数据的最大值和最小值。箱的上下两端表示这组数据中排在前25%位置和75%位置的数值。箱中间的横线表示中位数。</p><p><img src="https://i.imgur.com/RUj6v32.jpg" alt=""></p><p>假如你是一位互联网电商分析师，你想知道某商品每天的卖出情况：该商品被用户最多购买了几个，大部分用户购买了几个，用户最少购买了几个。箱线图就能很清晰的表示出上面的几个指标以及变化。</p><p>绘制箱线图，新人需要了解统计的基础概念：最大值，最小值，中位数，四分位数。这个会在后续讲解。</p><h3 id="热力图"><a href="#热力图" class="headerlink" title="热力图"></a>热力图</h3><p>以高亮形式展现数据。</p><p>最常见的例子就是用热力图表现道路交通状况。老司机一眼就知道怎么开车了。</p><p><img src="https://i.imgur.com/vEHwRch.jpg" alt=""></p><p>互联网产品中，热力图可以用于网站／APP的用户行为分析，将浏览、点击、访问页面的操作以高亮的可视化形式表现。下图就是用户在Google搜索结果的点击行为。</p><p><img src="https://i.imgur.com/Dsfnarm.jpg" alt=""></p><p>热力图需要位置信息，比如经纬度坐标，或者屏幕位置坐标。</p><h3 id="关系图"><a href="#关系图" class="headerlink" title="关系图"></a>关系图</h3><p>展现事物相关性和关联性的图表，比如社交关系链、品牌传播、或者某种信息的流动。</p><p><img src="https://i.imgur.com/HWfnNtE.jpg" alt=""></p><p>有一条微博，现在想研究它的传播链：它是经由哪几个大V分享扩散开来，大V前又有谁分享过等，以此为基础可以绘制出一幅发散的网状图，分析病毒营销的过程。</p><p><img src="https://i.imgur.com/pqCU9kR.jpg" alt=""></p><p>关系图依赖大量的数据，它本身没有维度的概念。</p><h3 id="矩形树图"><a href="#矩形树图" class="headerlink" title="矩形树图"></a>矩形树图</h3><p>上文说过，柱形图不适合表达过多类目（比如上百）的数据，那应该怎么办?矩形树图出现了。它直观地以面积表示数值，以颜色表示类目。</p><p>下图中各颜色系代表各个类目维度，类目维度下又有多个二级类目。如果用柱形图表达，简直是灾难。用矩形树图则轻轻松松。</p><p><img src="https://i.imgur.com/cZ1C97C.jpg" alt=""></p><p>电子商务、产品销售等涉及大量品类的分析，都可以用到矩形树图。</p><h3 id="桑基图-Sankey-Diagram"><a href="#桑基图-Sankey-Diagram" class="headerlink" title="桑基图 Sankey Diagram"></a>桑基图 Sankey Diagram</h3><p>比较冷门的图表，它常表示信息的变化和流动状态。</p><p><img src="https://i.imgur.com/7HPOoTv.jpg" alt=""></p><p>在我曾经写过的教你读懂活跃数据中，用桑基图绘制了用户活跃状态的变化，这是用户分层的可视化应用。<br>其实数据分析师经常接触到桑基图，只是不知道它的正式名字，它就是Google网站分析中的用户行为和流量分析。用户从哪里来，去了哪个页面，在哪个页面离开，最后停留在哪个页面等。下图就是桑基图非常直观的解释。</p><p><img src="https://i.imgur.com/MKn7i1j.jpg" alt=""></p><h3 id="漏斗图"><a href="#漏斗图" class="headerlink" title="漏斗图"></a>漏斗图</h3><p>大名鼎鼎的转化率可视化，它适用在固定流程的转化分析，你也可以认为它是桑基图的简化版。说实话，随着个性化推荐和精准运营越来越多，漏斗转化有它的局限性。</p><p>转化率也可以用几组数字表示，不一定做成漏斗图。</p><p><img src="https://i.imgur.com/0mMFCLZ.jpg" alt=""></p><p>除了上述可视化图表，还有其他很多经典，例如词云图、气泡图、K线图等。我们使用图表，不只是为了好看，虽然好看的报告面向老板和合作方很有优势。更多的是围绕业务进行分析，得到我们想要的结果。</p><p>没有最好的可视化图表，只有更好的分析方法。</p><p>有些数据可视化，Excel就能完成，有些则必须借助第三方工具或者编程。下一篇文章会挑选部分图表教大家如何Excel绘制。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>本文内容来源于网络，版权归原作者</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h4&gt;&lt;p&gt;数据分析界有一句经典名言，字不如表，表不如图。数据可视化是数据分析的主要方向之一。除掉数据挖掘这类高级分析，不少数据分析就是监控数据观察数据。&lt;/p&gt;
&lt;p&gt;数据分析的最终都是要兜售自己的观点和结论的。兜售的最好方式就是做出观点清晰数据详实的PPT给老板看。如果没人认同分析结果，那么分析也不会被改进和优化，不落地的数据分析价值又在哪里？&lt;/p&gt;
&lt;p&gt;各类图表的详细介绍可以查看这篇文章。&lt;/p&gt;
&lt;p&gt;温馨提示：如果您已经熟悉数据可视化，大可不必再看这篇文章，或只挑选部分。&lt;/p&gt;
    
    </summary>
    
      <category term="数据分析" scheme="https://paradoxallen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="可视化" scheme="https://paradoxallen.github.io/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>Excel学习(三) 实战篇</title>
    <link href="https://paradoxallen.github.io/17615/"/>
    <id>https://paradoxallen.github.io/17615/</id>
    <published>2017-02-09T16:00:00.000Z</published>
    <updated>2018-06-07T16:21:33.514Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/19174/">Excel学习(一) 函数篇</a><br><a href="https://paradoxallen.github.io/12853/">Excel学习(二) 技巧篇</a></p><h4 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h4><p>此文针对数据分析EXCEL部分的入门。</p><p>重点是了解各种函数，包括但不限于sum，count，sumif，countif，find，if，left/right，时间转换等。Excel函数不需要学全，<strong>重要的是学会搜索</strong>。即如何将遇到的问题在搜索引擎上描述清楚。掌握vlookup和数据透视表足够，是最具性价比的两个技巧。</p><p>学会vlookup，SQL中的join，Python中的merge很容易理解。</p><p>学会数据透视表，SQL中的group，Python中的pivot_table也是同理。</p><p>这两个搞定，基本10万条以内的数据统计没啥难度。Excel是熟能生巧，多找练习题。还有需要养成好习惯，不要合并单元格，不要过于花哨。表格按照原始数据（sheet1）、加工数据（sheet2），图表（sheet3）的类型管理。</p><p>第三篇数据分析—技巧篇。主要将前两篇的内容以实战方式进行，简单地进行了一次数据分析。数据源采用了真实的爬虫数据，是5000行数据分析师岗位数据。</p><p>温馨提示：如果您已经熟悉Excel，大可不必再看这篇文章，或只挑选部分。</p><a id="more"></a><hr><p>这篇文章讲解实战，如何运用上两篇文章的知识进行分析。</p><p>演示过程分为五个步骤：<strong>明确目的，观察数据，清洗数据，分析过程，得出结论。</strong></p><p>这也是通常数据分析的简化流程。</p><hr><h3 id="明确目的"><a href="#明确目的" class="headerlink" title="明确目的"></a>明确目的</h3><p>数据分析的大忌是不知道分析方向和目的，拿着一堆数据不知所措。<strong>一切数据分析都是以业务为核心目的</strong>，而不是以数据为目的。</p><p>数据用来解决什么问题？</p><p>是进行汇总统计制作成报表？</p><p>是进行数据可视化，作为一张信息图？</p><p>是验证某一类业务假设？</p><p>是希望提高某一个指标的KPI？</p><p>永远不要妄图在一堆数据中找出自己的结论，太难。目标在前，数据在后。哪怕给自己设立一个很简单的目标，例如计算业务的平均值，也比没有方向好。因为有了平均值可以想数字比预期是高了还是低了，原因在哪里，数据靠谱吗？为了找出原因还需要哪些数据。</p><p>既然有五千多条数据分析师的岗位数据。不妨在看数据前想一下自己会怎么运用数据。</p><p>数据分析师是一个什么样的岗位？</p><p>它的工资和薪酬是多少？</p><p>它有什么特点，需要掌握哪些能力？</p><p>哪类公司更会招聘数据分析师？</p><p>等等。有了目标和方向后，后续则是将目标拆解为实际过程。</p><hr><h3 id="观察数据"><a href="#观察数据" class="headerlink" title="观察数据"></a>观察数据</h3><p><img src="https://i.imgur.com/OKfRHrB.jpg" alt=""></p><p>拿出数据别急切计算，先观察数据。</p><p>字段名称都是英文，我是通过Json获取的数据，所以整体数据都较为规整。往后绝大部分的数据源的字段名都是英文。因为比起拼音和汉字，它更适合编程环境下。</p><p>先看一下columns的含义。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">city：城市</span><br><span class="line"></span><br><span class="line">companyFullName：公司全名</span><br><span class="line"></span><br><span class="line">companyId：公司ID</span><br><span class="line"></span><br><span class="line">companyLabelList：公司介绍标签</span><br><span class="line"></span><br><span class="line">companyShortName：公司简称</span><br><span class="line"></span><br><span class="line">companySize：公司大小</span><br><span class="line"></span><br><span class="line">businessZones：公司所在商区</span><br><span class="line"></span><br><span class="line">firstType：职位所属一级类目</span><br><span class="line"></span><br><span class="line">secondType：职业所属二级类目</span><br><span class="line"></span><br><span class="line">education：教育要求</span><br><span class="line"></span><br><span class="line">industryField：公司所属领域</span><br><span class="line"></span><br><span class="line">positionId：职位ID</span><br><span class="line"></span><br><span class="line">positionAdvantage：职位福利</span><br><span class="line"></span><br><span class="line">positionName：职位名称</span><br><span class="line"></span><br><span class="line">positionLables：职位标签</span><br><span class="line"></span><br><span class="line">salary：薪水</span><br><span class="line"></span><br><span class="line">workYear：工作年限要求</span><br></pre></td></tr></table></figure><p>数据基本涵盖了职位分析的所需。职位中的职位描述没有抓下来，一来纯文本不适合这次初级分析，二来文本需要分词以及文本挖掘，后续有机会再讲。</p><p>首先看一下哪些字段数据可以去除。<code>companyId</code>和<code>positionId</code>是数据的唯一标示，类似该职位的身份证号，这次分析用不到关联vlookup，我们先隐藏。<code>companyFullName</code>和<code>companyShortName</code>则重复了，只需要留一个公司名称，<code>companyFullName</code>依旧隐藏。</p><p>尽量不删除数据，而是隐藏，保证原始数据的完整，谁知道以后会不会用到呢？</p><p><img src="https://i.imgur.com/B3voXgf.jpg" alt=""></p><p>接下来进行数据清洗和转换。因为只是Excel级别的数据分析，不会有哑变量离散化标准化的操作。我简单归纳一下。</p><p><strong>数据有无缺失值</strong></p><p>数据的缺失值很大程度上影响分析结果。引起缺失的原因很多，例如技术原因，爬虫没有完全抓去，例如本身的缺失，该岗位的HR没有填写。</p><p>如果某一字段缺失数据较多（超过50%），分析过程中要考虑是否删除该字段，因为缺失过多就没有业务意义了。</p><p>Excel中可以通过选取该列，在屏幕的右下角查看计数，以此判别有无缺失。</p><p><code>companyLabelList、businessZones、positionLables</code>都有缺失，但不多。不影响实际分析。</p><p><strong>数据是否一致化</strong></p><p>一致化指的是数据是否有统一的标准或命名。例如上海市数据分析有限公司和上海数据分析有限公司，差别就在一个市字，主观上肯定会认为是同一家公司，但是对机器和程序依旧会把它们认成两家。会影响计数、数据透视的结果。</p><p>我们看一下表格中的<code>positionName</code></p><p><img src="https://i.imgur.com/EVbGN3f.jpg" alt=""></p><p>各类职位千奇百怪啊，什么品牌保护分析师实习生、足球分析师、商业数据分析、大数据业务分析师、数据合同管理助理。并不是纯粹的数据分析岗位。</p><p>为什么呢？这是招聘网站的原因，有些职位明确为数据分析师，有些职位要求具备数据分析能力，但是又干其他活。招聘网站为了照顾这种需求，采用关联法，只要和数据分析相关职位，都会在数据分析师的搜索结果中出现。我的爬虫没有过滤其他数据，这就需要手动清洗。</p><p>这会不会影响我们的分析？当然会。像大数据工程师是数据的另外发展方向，但不能归纳到数据分析岗位下，后续我们需要将数据分析强相关的职位挑选出来。</p><p><strong>数据是否有脏数据</strong></p><p>脏数据是分析过程中很讨厌的环节。例如乱码，错位，重复值，未匹配数据，加密数据等。能影响到分析的都算脏数据，没有一致化也可以算。</p><p>我们看表格中有没有重复数据。</p><p>这里有一个快速窍门，使用Excel的删除重复项功能，快速定位是否有重复数据，还记得<code>positionId</code>么？因为它是唯一标示，如果重复了，就说明有重复的职位数据。看来不删除它是正确的。</p><p>对<code>positionId</code>列进行重复项删除操作</p><p><img src="https://i.imgur.com/NKWu2cR.jpg" alt=""></p><p>有1845个重复值。数据重复了。这是我当时爬取完数据时，将北京地区多爬取一次人为制作出的脏数据。接下来全选所有数据，进行删除重复项，保留5032行（含表头字段）数据。</p><p><strong>数据标准结构</strong></p><p>数据标准结构，就是将特殊结构的数据进行转换和规整。</p><p>表格中，<code>companyLableList</code>就是以数组形式保存（JSON中的数组）</p><p><img src="https://i.imgur.com/uU7AVNo.jpg" alt=""></p><p>看来福利倒是不错，哈哈，不过这会影响我们的分析。<code>businessZones、positionAdvantage和positionLables</code>也是同样问题，我们后续得将这类格式拆分开来。</p><p><img src="https://i.imgur.com/pop4oIH.jpg" alt=""></p><p>薪水的话用了几K表示，但这是文本，并不能直接用于计算。而且是一个范围，后续得按照最高薪水和最低薪水拆成两列。</p><p>OK，数据大概都了解了，那么下一步就是将数据洗干净。</p><hr><h3 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h3><p>数据清洗可以新建Sheet，方便和原始数据区分开来。</p><p>先清洗薪水吧，大家肯定对钱感兴趣。将<code>salary</code>拆成最高薪水和最低薪水有三种办法。</p><p><strong>一是直接分列</strong>，以<code>&quot;-&quot;</code>为拆分符，得到两列数据，然后利用替换功能删除 k这个字符串。得到结果。</p><p><strong>二是自动填充功能</strong>，填写已填写的内容自动计算填充所有列。但我这个版本没有，就不演示了。</p><p><strong>三是利用文本查找的思想</strong>，重点讲一下这个。先用<code>=FIND(&quot;k&quot;,O2,1)</code>。查找第一个<code>K</code>（最低薪酬）出现的位置。</p><p><img src="https://i.imgur.com/zUVtDC4.jpg" alt=""></p><p>我们知道第一个k出现的位置，此时<code>=LEFT(O2,FIND(&quot;k&quot;,O2,1))</code>得到的结果就是<code>7K</code>，要去除掉<code>k</code>，<code>FIND(&quot;k&quot;,O2,1)</code>再减去1即可。</p><p><img src="https://i.imgur.com/443spNJ.jpg" alt=""></p><p>最高薪水也是同样的思路，但不能使用<code>k</code>，因为第二个薪水位置不固定。需要利用<code>find</code>查找<code>&quot;-&quot;</code>位置,然后截取 从<code>&quot;-&quot;</code> 到最后第二个位置的字符串。</p><pre><code>=MID(O2,FIND(&quot;-&quot;,O2,1)+1,LEN(O2)-FIND(&quot;-&quot;,O2,1)-1)</code></pre><p><img src="https://i.imgur.com/61k6qsj.jpg" alt=""></p><p>因为薪水是一个范围，我们不可能拿范围计算平均工资。那怎么办呢？我们只能取最高薪水和最低薪水的平均数作为该岗位薪资。这是数据来源的缺陷，因为我们并不能知道应聘者实际能拿多少。这是薪水计算的误差。</p><p><img src="https://i.imgur.com/eOw0B0w.jpg" alt=""></p><p>我们检查一下有没有错误，利用筛选功能快速定位。</p><p><img src="https://i.imgur.com/OuAa3zN.jpg" alt=""></p><p>居然有<code>#VALUE！</code>错误，看一下原因。</p><p><img src="https://i.imgur.com/BQvT1z6.jpg" alt=""></p><p>原来是大写<code>K</code>，因为<code>find</code>对大小写敏感，此时用<code>search</code>函数，或者将K替换成k都能解决。</p><p>另外还有一个错误是很多HR将工资写成<code>5K</code>以上，这样就无法计算<code>topSalar</code>。为了计算方便，将<code>topSalary</code>等于<code>bottomSalary</code>，虽然也有误差。</p><p>这就是我强调数据一致性的原因。</p><p><code>companyLabelList</code>是公司标签，诸如技能培训啊、五险一金啊等等。直接用分列即可。大家需要注意，分列会覆盖掉右列单元格，所以记得复制到最后一列再分。</p><p><img src="https://i.imgur.com/UpTMB87.jpg" alt=""></p><p>符号用搜索替换法删除即可。</p><p><code>positionLables、positionAdvantage、businessZones</code>同样也可以用分列法。如果观察过数据会知道，<code>companyLabelList</code>公司标签都是固定的内容，而其他三个不是。这些都是HR自己填写，所以就会有各种乱七八糟不统一的描述。</p><p><img src="https://i.imgur.com/AoHOT6x.jpg" alt=""></p><p>这些内容均是自定义，没有特别大的分析价值。如果要分析，必须花费很长的时间在清洗过程。主要思路是把这些内容统一成几十个固定标签。在这里我将不浪费时间讲解了，主要利用Python分词和词典进行快速清洗。</p><p>因为时间和性价比问题，<code>positionAdvantage</code>和<code>businessZones</code>我就不分列了。只清洗<code>positionLables</code>职位标签。某一个职位最多的标签有13个。</p><p><code>[&#39;实习生&#39;, &#39;主管&#39;, &#39;经理&#39;,&#39;顾问&#39;, &#39;销售&#39;, &#39;客户代表&#39;, &#39;分析师&#39;, &#39;职业培训&#39;, &#39;教育&#39;, &#39;培训&#39;, &#39;金融&#39;, &#39;证券&#39;, &#39;讲师&#39;]</code><br>这个职位叫金融证券分析师助理讲师助理，我真不知道为什么实习生、主管、经理这三个标签放在一起，我也是哔了狗了。反正大家数据分析做久了，会遇到很多<code>Magic Data</code>。</p><p>接下来是<code>positionName</code>，上文已经讲过有各种乱七八糟或非数据分析师职位，所以我们需要排除掉明显不是数据分析师的岗位。</p><p>单独针对<code>positionName</code>用数据透视表。统计各名称出现的次数。</p><p><img src="https://i.imgur.com/5U6EQdf.jpg" alt=""></p><p>出现次数为3次以下的职位，有约一千，都是各类特别称谓，HR你们为什么要这样写…要这样写…这样写。更改职位名称似乎不现实，那就用关键词查找的思路，找出包含有数据分析、分析师、数据运营等关键词的岗位。虽然依旧会有金融分析师这类非纯数据的岗位。</p><p>用<code>find</code>和数组函数结合 <code>=IF(COUNT(FIND({&quot;数据分析&quot;,&quot;数据运营&quot;,&quot;分析师&quot;},M33)),&quot;1&quot;,&quot;0&quot;)，shift+ctrl+enter</code>输入。就得到了多条件查找后的结果。</p><p>单纯的<code>find</code>只会查找数据分析这个词，必须嵌套<code>count</code>才会变成真数组。</p><p><img src="https://i.imgur.com/zJFvq0n.jpg" alt=""></p><p><code>1为包含，0不包含</code>。将1过滤出来，这就是需要分析的最终数据。</p><p>当然大家如果感兴趣，也可以看一下大数据工程师，数据产品经理这些岗位。</p><hr><h3 id="分析过程-amp-得出结论"><a href="#分析过程-amp-得出结论" class="headerlink" title="分析过程&amp;得出结论"></a>分析过程&amp;得出结论</h3><p>分析过程有很多玩法。因为主要数据均是文本格式，所以偏向汇总统计的计算。如果数值型的数据比较多，就会涉及到统计、比例等概念。如果有时间类数据，那么还会有趋势、变化的概念。</p><p>整体分析使用数据透视表完成，先利用数据透视表获得汇总型统计。</p><p><img src="https://i.imgur.com/W4w1AsB.jpg" alt=""></p><p>看来北京的数据分析岗位机会远较其他城市多。1-3年和3-5年两个时间段的缺口更大。应届毕业生似乎比1年一下经验的更吃香。爬取时间为11月，这时候校招陆续开始，大公司会有闲暇校招，实际岗位应该更多。小公司则倾向发布。这是招聘网站的限制。</p><p>看一下公司对数据分析师的缺口如何。</p><p><img src="https://i.imgur.com/8xBiZqa.jpg" alt=""></p><p>似乎是公司越大，需要的数据分析师越多。</p><p>但这样的分析并不准确。因为这只是一个汇总数据，而不是比例数据，我们需要计算的是不同类型企业人均招聘数。</p><p>如果北京的互联网公司特别多，那么即使有1000多个岗位发布也不算缺口大，如果南京的互联网公司少，即使只招聘30个，也是充满需求的。</p><p>还有一种情况是企业刚好招聘满数据分析师，就不发布岗位了，数据包含的只是正在招聘数据分析师的企业，这些都是限制分析的因素。我们要明确。</p><p>有兴趣大家可以深入研究。</p><p>看一下各城市招聘Top5公司。</p><p><img src="https://i.imgur.com/j4lsjT3.jpg" alt=""></p><p>北京的美团以78个数据分析职位招聘力压群雄，甚至一定程度上拉高了北京的数据。而个推则在上海和杭州都发布了多个数据分析师职位，不知道是HR的意外，还是要大规模补充业务线（在我写这篇文章的时候，约有一半职位已经下线）。</p><p>比较奇怪的是阿里巴巴并没有在杭州上榜，看来是该阶段招聘需求不大，或者数据分析师有其他招聘渠道。</p><p>没有上榜不代表不要数据分析师，但是上榜的肯定现阶段对数据分析师有需求。</p><p>我们看一下数据分析师的薪水，可能是大家最感兴趣的了。</p><p><img src="https://i.imgur.com/ifz87wL.jpg" alt=""></p><p>我们看到南京、西安在应届生中数据最高，是因为招聘职位不多，因为单独一两个企业的高薪影响了平均数，其余互联网二线城市同理。当工作年限达到3年以上，北上深杭的数据分析师薪资则明显高于其他城市。</p><p>数据会有误差性么？会的，因为存在薪资极值影响。而数据透视表没有中位数选项。我们也可以单独用分位数进行计算，降低误差。</p><p>薪资可以用更细的维度计算，比如学历、比如公司行业领域，是否博士生远高于本科生，是否金融业薪资高于O2O。</p><p>另外数据分析师的薪资，可能包括奖金、年终奖、季度奖等隐形福利。部分企业会在<code>positionAdvantage</code>的内容上说明，大家可以用筛选过滤出16薪这类关键词。作为横向对比。</p><p><img src="https://i.imgur.com/iy0Gy0U.jpg" alt=""></p><p>我们看一下数据分析的职位标签，数据透视后汇总。</p><p><img src="https://i.imgur.com/B63AFJS.jpg" alt=""></p><p><code>分析师、数据、数据分析</code>是最多的标签。除此以外，<code>需求分析，BI，数据挖掘</code>也出现在前列。看来不少数据分析师的要求掌握数据挖掘，将标签和薪水关联，是另外一种分析思路。职位标签并不是最优的解法，了解一个职位最好的必然是职位描述。</p><p>分析过程不多做篇幅了，主要使用数据透视表进行多维度分析，没有其他复杂的技巧。下图很直观的展现了多维度的应用。</p><p><img src="https://i.imgur.com/vRSysg5.jpg" alt=""></p><p>我们的分析也属于多维度，<code>城市、工作年限、企业大小、企业领域</code>等，利用不同维度形成一个直观的二位表格，而维度则是通过早期的数据清洗统一化标准化。这是一种很常见的分析技巧。</p><p>后续的数据报告，涉及到可视化制作，因为字不如表、表不如图，就放在后面讲解了。</p><p>最后多说几下：</p><p><strong>1.最好的分析，是拿数据分析师们的在职数据，而不是企业招聘数据。</strong></p><p><strong>2.承认招聘数据的非客观性，招聘要求与对数据分析师的实际要求是有差异的。</strong></p><hr><p>####写在最后<br>除了Excel的这三部分内容，还有一些也需要进一步了解的：</p><p>了解单元格格式，后期的数据类型包括各类timestamp，date，string，int，bigint，char，factor，float等。</p><p>了解数组，以及怎么用（excel的数组挺难用），Python和R也会涉及到 list。</p><p>了解函数和参数，当进阶为编程型的数据分析师时，会让你更快的掌握。</p><p>了解中文编码，UTF8和ASCII，包括CSV的delimiter等。</p><p>养成一个好习惯，不要合并单元格，不要过于花哨。表格按照原始数据、加工数据，图表的类型管理。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>本文内容来源于网络，版权归原作者</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/19174/&quot;&gt;Excel学习(一) 函数篇&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://paradoxallen.github.io/12853/&quot;&gt;Excel学习(二) 技巧篇&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h4&gt;&lt;p&gt;此文针对数据分析EXCEL部分的入门。&lt;/p&gt;
&lt;p&gt;重点是了解各种函数，包括但不限于sum，count，sumif，countif，find，if，left/right，时间转换等。Excel函数不需要学全，&lt;strong&gt;重要的是学会搜索&lt;/strong&gt;。即如何将遇到的问题在搜索引擎上描述清楚。掌握vlookup和数据透视表足够，是最具性价比的两个技巧。&lt;/p&gt;
&lt;p&gt;学会vlookup，SQL中的join，Python中的merge很容易理解。&lt;/p&gt;
&lt;p&gt;学会数据透视表，SQL中的group，Python中的pivot_table也是同理。&lt;/p&gt;
&lt;p&gt;这两个搞定，基本10万条以内的数据统计没啥难度。Excel是熟能生巧，多找练习题。还有需要养成好习惯，不要合并单元格，不要过于花哨。表格按照原始数据（sheet1）、加工数据（sheet2），图表（sheet3）的类型管理。&lt;/p&gt;
&lt;p&gt;第三篇数据分析—技巧篇。主要将前两篇的内容以实战方式进行，简单地进行了一次数据分析。数据源采用了真实的爬虫数据，是5000行数据分析师岗位数据。&lt;/p&gt;
&lt;p&gt;温馨提示：如果您已经熟悉Excel，大可不必再看这篇文章，或只挑选部分。&lt;/p&gt;
    
    </summary>
    
      <category term="数据分析" scheme="https://paradoxallen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Excel" scheme="https://paradoxallen.github.io/tags/Excel/"/>
    
  </entry>
  
  <entry>
    <title>Excel学习(二) 技巧篇</title>
    <link href="https://paradoxallen.github.io/12853/"/>
    <id>https://paradoxallen.github.io/12853/</id>
    <published>2017-02-05T16:00:00.000Z</published>
    <updated>2018-06-07T13:53:58.495Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/19174/">Excel学习(一) 函数篇</a></p><h4 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h4><p>此文针对数据分析EXCEL部分的入门。</p><p>重点是了解各种函数，包括但不限于sum，count，sumif，countif，find，if，left/right，时间转换等。Excel函数不需要学全，<strong>重要的是学会搜索</strong>。即如何将遇到的问题在搜索引擎上描述清楚。掌握vlookup和数据透视表足够，是最具性价比的两个技巧。</p><p>学会vlookup，SQL中的join，Python中的merge很容易理解。</p><p>学会数据透视表，SQL中的group，Python中的pivot_table也是同理。</p><p>这两个搞定，基本10万条以内的数据统计没啥难度。Excel是熟能生巧，多找练习题。还有需要养成好习惯，不要合并单元格，不要过于花哨。表格按照原始数据（sheet1）、加工数据（sheet2），图表（sheet3）的类型管理。</p><p>第二篇数据分析—技巧篇。主要简单讲解很有性价比的功能，提高工作效率。</p><p>温馨提示：如果您已经熟悉Excel，大可不必再看这篇文章，或只挑选部分。</p><a id="more"></a><hr><p>本次讲解依然是提纲，图文部分引用自百度经验。内容方面照旧会补充SQL和Python。</p><hr><h3 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h3><p>Excel的快捷键很多，以下主要是能提高效率。</p><p><code>Ctrl+方向键</code>，对单元格光标快速移动，移动到数据边缘（空格位置）。</p><p><code>Ctrl+Shift+方向键</code>，对单元格快读框选，选择到数据边缘（空格位置）。</p><p><code>Ctrl+空格键</code>，选定整列。</p><p><code>Shift+空格键</code>，选定整行。</p><p><code>Ctrl+A</code>，选择整张表。</p><p><code>Alt+Enter</code>，换行。</p><p><code>Ctrl+Enter</code>，以当前单元格为始，往下填充数据和函数。</p><p><code>Ctrl+S</code>，快读保存。</p><p><code>Ctrl+Z</code>，撤回当前操作。</p><p>如果是效率达人，可以学习更多快捷键。Mac用户的Ctrl一般需要用command替换。</p><h3 id="格式转换"><a href="#格式转换" class="headerlink" title="格式转换"></a>格式转换</h3><p>Excel的格式及转换很容易忽略，但格式会如影随形伴随数据分析者的一切场景，是后续SQL和Python数据类型的基础。</p><p>通常我们将Excel格式分为数值、文本、时间。</p><p>数值常见整数型 Int和小数/浮点型 Float。两者的界限很模糊。在SQL和Python中，则会牵扯的复杂，涉及运算效率，计算精度等。</p><p>文本分为中文和英文，存储字节，字符长度不同。中文很容易遇到编码问题，尤其是Python2。Win和Mac环境也有差异。大家遇到的乱码一般都属于中文编码错误。</p><p>时间格式在Excel中可以和数值直接互换，也能用加减法进行天数换算。</p><p>时间格式有不同表达。例如<code>2016年11月11日，2016/11/11，2016-11-11</code>等。当数据源多就会变得混乱。我们可以用自定义格式规范时间。</p><p>这里了解一下时间格式的概念，列举是一些较通用的范例（<strong>不同编程语言还是有差异的</strong>）。</p><p><code>YYYY</code>代表通配的四位数年格式</p><p><code>MM</code>代表通配的两位数月格式</p><p><code>DD</code>代表通配的两位数日格式</p><p><code>HH</code>代表通配的的两位数小时（24小时）格式</p><p><code>hh</code>代表通配的两位数小（12小时制）格式</p><p><code>mm</code>代表通配的两位数分格式</p><p><code>ss</code>代表通配的两位数秒格式</p><p>例如<code>2016/11/11</code>可以写成：<code>yyyy/MM/dd</code></p><p><code>2016-11-11 23:59:59</code>可以写成：<code>yyyy-MM-dd HH:mm:ss</code></p><p><img src="https://i.imgur.com/nlur0Cl.jpg" alt=""></p><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>数组很多人都不会用到，甚至不知道有这个功能。依旧是数据分析越往后用到越多，它类似R语言的Array和Python的<code>List</code>。</p><p>数组由多个元素组成。普通函数的计算结果是一个值，数组类函数的计算结果返回多个值。</p><p>数组用大括号表示，当函数中使用到数组，应该用<code>Ctrl+Shift+Enter</code>输入，不然会报错。</p><p>先看数组的最基础使用。选择<code>A1:D1</code>区域，输入<code>={1,2,3,4}</code>。记住是大括号。然后<code>Ctrl+Shift+Enter</code>。我们发现数组里的四个值被分别传到四个单元格中，这是数组的独有用法。</p><p><img src="https://i.imgur.com/BPaet6B.jpg" alt=""></p><p>我们再来看一下数组和函数的应用。利用{}，我们能做到1匹配a，2匹配b，3匹配c。也就是一一对应。专业说法是<code>Mapping</code>。</p><pre><code>=lookup(查找值，{1,2,3}，{&quot;a&quot;,&quot;b&quot;,&quot;c&quot;})</code></pre><p>Excel的数组具体应用，大家可以搜索学习，可以提高一定的效率。但是Python的数组更为强大，我的重点就不放在这块了。</p><h3 id="分列"><a href="#分列" class="headerlink" title="分列"></a>分列</h3><p>Excel可以将多个单元格的内容合并，但是不擅长拆分。分列功能可以将某一列按照特定规则拆分。常常用来进行数据清洗。</p><p><img src="https://i.imgur.com/vimBLy5.jpg" alt=""></p><p>上文我有一列地区的数据，我想要将市和区分成两列。我们可以用mid和find函数查找市截取字符。但最快的做法就是用“市”分列。</p><p><img src="https://i.imgur.com/5FhN3mL.jpg" alt=""></p><p>出一个思考题，如果市和自治区区都存在应该如何分列？</p><p>SQL和Python中有类似的<code>spilt ( )</code>函数。</p><h3 id="合并单元个格"><a href="#合并单元个格" class="headerlink" title="合并单元个格"></a>合并单元个格</h3><p>单元格作为报表整理使用，除非是最终输出格式，例如打印。否则不要随意合并单元格。</p><p>一旦使用合并单元格，绝大多数函数都不能正常使用，影响批量的数据处理和格式转换。合并单元格也会造成Python和SQL的读取错误。</p><h3 id="数据透视表"><a href="#数据透视表" class="headerlink" title="数据透视表"></a>数据透视表</h3><p>数据透视表是非常强大的功能，当初学会时惊为天人。</p><p>数据透视表的主要功能是将数据聚合，按照各子段进行<code>sum( )，count( )</code>的运算。</p><p>下图我选择我选择想要计算的数据，然后点击创建透视表。</p><p><img src="https://i.imgur.com/n9BMOzA.jpg" alt=""></p><p>此时会新建一个Sheet，这是数据透视表的优点，将原始数据和汇总计算数据分离。</p><p>数据透视表的核心思想是聚合运算，将字段名相同的数据聚合起来，所谓数以类分。</p><p>列和行的设置，则是按不同轴向展现数据。简单说，你想要什么结构的报表，就用什么样的拖拽方式。</p><p><img src="https://i.imgur.com/7TGmIVc.jpg" alt=""></p><p>聚合功能有一点类似SQL中的<code>gorup by</code>，python中则有更为强大的<code>pandas.pivot_table( )</code>。</p><h3 id="删除重复项"><a href="#删除重复项" class="headerlink" title="删除重复项"></a>删除重复项</h3><p>一种数据清洗和检验的快速方式。想要验证某一列有多少个唯一值，或者数据清洗，都可以使用。</p><p>类似SQL中的<code>distinct</code> ,python中的<code>set</code></p><h3 id="条件格式"><a href="#条件格式" class="headerlink" title="条件格式"></a>条件格式</h3><p>条件格式可以当作数据可视化的应用。如果我们要使用函数在大量数据中找出前三的值，可能会用到<code>rank( )</code>函数，排序，然后过滤出<code>1，2，3</code>。</p><p>用条件格式则是另外一种快速方法，直接用颜色标出，非常直观。</p><p><img src="https://i.imgur.com/NlLrhKr.jpg" alt=""><br><img src="https://i.imgur.com/xxoRsIE.jpg" alt=""></p><h3 id="冻结首行首列"><a href="#冻结首行首列" class="headerlink" title="冻结首行首列"></a>冻结首行首列</h3><p>Excel的首行一般是各字段名<code>Header</code>，俗称表头，当行数和列数过多的时候，观察数据比较麻烦。我们可以通过固定住首行，方便浏览和操作。</p><p><code>Header</code>是一个较为重要的概念。在Python和R中，<code>read_csv</code>函数，会有一个专门的参数<code>header=true</code>，来判断是否读取表头作为<code>columns</code>的名字。</p><h3 id="自定义下拉菜单（数据有效性）"><a href="#自定义下拉菜单（数据有效性）" class="headerlink" title="自定义下拉菜单（数据有效性）"></a>自定义下拉菜单（数据有效性）</h3><p>数据有效性是一种约束，针对单元格限制其输入，也就是让其只能固定几个值。下拉菜单是一种高阶应用，通过允许下拉箭头即可。</p><p><img src="https://i.imgur.com/QO0Ru7S.jpg" alt=""><br><img src="https://i.imgur.com/hfJAJVC.jpg" alt=""></p><h3 id="自定义名称"><a href="#自定义名称" class="headerlink" title="自定义名称"></a>自定义名称</h3><p>自定义名称是一个很好用的技巧，我们可以为一个区域，变量、或者数组定义一个名称。后续要经常使用的话，直接引用即可，无需再次定位。这是复用的概念。</p><p><img src="https://i.imgur.com/DROuDqO.jpg" alt=""></p><p>我们将<code>A1:A3</code>区域命名为<code>NUM</code><br>直接使用<code>=sum(NUM)</code> ，等价于<code>sum(A1:A3)</code>。</p><p><img src="https://i.imgur.com/UFyXjOg.jpg" alt=""></p><p>新手们理解数据库，可以将其想象成无数张表sheet。每一张表都有自己唯一的名字，就像上图的<code>NUM</code>一样。数据库操作就是引用表名进行查找、关联等操作。使用<code>sum，count</code>等函数。</p><h3 id="查找公式错误"><a href="#查找公式错误" class="headerlink" title="查找公式错误"></a>查找公式错误</h3><p>公式报错也不知道错在哪里的时候可以使用，尤其是各类<code>IF</code>嵌套或者多表关联，逻辑复杂时。查找公式错误是逐步运算的，方便定位。</p><p><img src="https://i.imgur.com/7RfPtPt.jpg" alt=""><br><img src="https://i.imgur.com/8xm8ffn.jpg" alt=""></p><h3 id="分组和分级显示"><a href="#分组和分级显示" class="headerlink" title="分组和分级显示"></a>分组和分级显示</h3><p>分组和分级显示，常用在报表中，在报表行数多到一定程度时，通过分组达到快速切换和隐藏的目的。越是专业度的报表（咨询、财务等），越可以学习这块。在数据菜单下。</p><p><img src="https://i.imgur.com/ckuyEmM.jpg" alt=""></p><h3 id="分析工具库"><a href="#分析工具库" class="headerlink" title="分析工具库"></a>分析工具库</h3><p>分析工具库是高阶分析的利器，包含很多统计计算，检验功能等工具。Excel是默认不安装的，要安装需要加载项，在工具菜单下（不同版本安装方式会有一点小差异）。</p><p><img src="https://i.imgur.com/769vRzG.jpg" alt=""></p><p>分析工具库是统计包，规划求解是计算最优解，类似决策树。这两者的分析方法以后详细论述。</p><p><img src="https://i.imgur.com/NeSkSr0.jpg" alt=""></p><p>Mac似乎有阉割。</p><h3 id="第三方应用"><a href="#第三方应用" class="headerlink" title="第三方应用"></a>第三方应用</h3><p>Excel是支持第三方插件的，第三方插件拥有非常强大的功能。甚至完成BI的工作。</p><p><img src="https://i.imgur.com/QdlkYp0.jpg" alt=""></p><p>应用商店里微软的<code>Power系列</code>都挺好的。下图就是<code>Power Map</code></p><p><img src="https://i.imgur.com/JLzHZg7.jpg" alt=""></p><p>第三方应用商店Mac没有，非常可惜。Win用户请用最新版本，老版本是没有插件的。</p><p>Excel更多技巧可以在<a href="https://www.zhihu.com/topic/19567930/hot" target="_blank" rel="noopener">知乎Microsoft Excel-热门问答-知乎</a> 下搜索。主要是和数据分析相关的。</p><hr><p>主要的Excel技巧和函数已经都已经讲解完毕。Excel博大精深，有一句说的挺好，我们大部分实际用到的功能只有20%。熟练掌握这20%功能，日常工作足够应付。重要的还是解决问题的能力。</p><p>接下来是Excel实战内容，下一篇文章会直接用到5000行真实的数据分析师的职位数据。没错，用数据分析师的数据进行分析，有点拗口。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>本文内容来源于网络，版权归原作者</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/19174/&quot;&gt;Excel学习(一) 函数篇&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h4&gt;&lt;p&gt;此文针对数据分析EXCEL部分的入门。&lt;/p&gt;
&lt;p&gt;重点是了解各种函数，包括但不限于sum，count，sumif，countif，find，if，left/right，时间转换等。Excel函数不需要学全，&lt;strong&gt;重要的是学会搜索&lt;/strong&gt;。即如何将遇到的问题在搜索引擎上描述清楚。掌握vlookup和数据透视表足够，是最具性价比的两个技巧。&lt;/p&gt;
&lt;p&gt;学会vlookup，SQL中的join，Python中的merge很容易理解。&lt;/p&gt;
&lt;p&gt;学会数据透视表，SQL中的group，Python中的pivot_table也是同理。&lt;/p&gt;
&lt;p&gt;这两个搞定，基本10万条以内的数据统计没啥难度。Excel是熟能生巧，多找练习题。还有需要养成好习惯，不要合并单元格，不要过于花哨。表格按照原始数据（sheet1）、加工数据（sheet2），图表（sheet3）的类型管理。&lt;/p&gt;
&lt;p&gt;第二篇数据分析—技巧篇。主要简单讲解很有性价比的功能，提高工作效率。&lt;/p&gt;
&lt;p&gt;温馨提示：如果您已经熟悉Excel，大可不必再看这篇文章，或只挑选部分。&lt;/p&gt;
    
    </summary>
    
      <category term="数据分析" scheme="https://paradoxallen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Excel" scheme="https://paradoxallen.github.io/tags/Excel/"/>
    
  </entry>
  
  <entry>
    <title>Excel学习(一) 函数篇</title>
    <link href="https://paradoxallen.github.io/19174/"/>
    <id>https://paradoxallen.github.io/19174/</id>
    <published>2017-02-02T16:00:00.000Z</published>
    <updated>2018-06-07T13:53:21.293Z</updated>
    
    <content type="html"><![CDATA[<h4 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h4><p>此文针对数据分析EXCEL部分的入门。</p><p>重点是了解各种函数，包括但不限于sum，count，sumif，countif，find，if，left/right，时间转换等。Excel函数不需要学全，<strong>重要的是学会搜索</strong>。即如何将遇到的问题在搜索引擎上描述清楚。掌握vlookup和数据透视表足够，是最具性价比的两个技巧。</p><p>学会vlookup，SQL中的join，Python中的merge很容易理解。</p><p>学会数据透视表，SQL中的group，Python中的pivot_table也是同理。</p><p>这两个搞定，基本10万条以内的数据统计没啥难度。Excel是熟能生巧，多找练习题。还有需要养成好习惯，不要合并单元格，不要过于花哨。表格按照原始数据（sheet1）、加工数据（sheet2），图表（sheet3）的类型管理。</p><p>第一篇数据分析—函数篇。主要简单讲解常用的函数，以及与之对应的SQL/Python函数。</p><p>温馨提示：如果您已经熟悉Excel，大可不必再看这篇文章，或只挑选部分。</p><a id="more"></a><hr><p>世界上的数据分析师分为两类，使用Excel的分析师，和其他分析师。</p><p>每一个数据新人的入门工具都离不开Excel。因为Excel涵盖的功能足够多。</p><p>很多传统行业的数据分析师只要求掌握Excel即可，会SPSS/SAS是加分项。即使在挖掘满街走，Python不如狗的互联网数据分析界，Excel也是不可替代的。</p><p>Excel有很多强大的函数，这篇文章主要介绍各种函数的用途。实战会后续文章讲解。</p><p>函数可以被我们想象成一个盒子，专门负责将输入转换成输出，不同的函数对应不同的输出。</p><pre><code>=Vlookup( lookup_value ,table_array,col_index_num,[range_lookup] )</code></pre><p>上文的Vlookup就是一个经典函数。函数中包含参数，括号里的部分都是参数。我们可以把参数想象成盒子上的开关。vlookup就有四个开关，不同开关组合决定了函数的输入和输出。</p><pre><code>=Vlookup( 参数1，参数2，参数3，参数4)</code></pre><p>复杂的原理不需要了解。这篇文章是常用函数汇总。甚至你不需要特别记忆怎么使用函数，<strong>应用Excel函数最重要的能力是学会搜索</strong>。因为绝大部分函数网上已经有相应的解释，图文结合，非常详尽。</p><p>学会将遇到的问题转换成搜索语句，如不会vlookup，不会关联多张表的数据，在网上搜索：excel怎么匹配多张表的数据。于是就学会了。这里推荐使用百度，因为前三行的结果基本是百度经验，对新人学习很友好。（后续图片均引用自百度经验）</p><p>在理解函数的基础上，会适当引入高层次的内容，SQL和Python（内建函数）。将其和Excel结合学习，如果大家吃透了Excel的函数，那么后续学习会轻松不少。</p><hr><h3 id="清洗处理类"><a href="#清洗处理类" class="headerlink" title="清洗处理类"></a>清洗处理类</h3><p>主要是文本、格式以及脏数据的清洗和转换。很多数据并不是直接拿来就能用的，需要经过数据分析人员的清理。数据越多，这个步骤花费的时间越长。</p><p><strong>Trim</strong></p><p>清除掉字符串两边的空格。</p><p><em>MySQL有同名函数，Python有近似函数strip。</em></p><p><strong>Concatenate</strong></p><pre><code>=Concatenate(单元格1，单元格2……)</code></pre><p>合并单元格中的内容，还有另一种合并方式是&amp; 。”我”&amp;”很”&amp;”帅” ＝ 我很帅。当需要合并的内容过多时，concatenate的效率快也优雅。</p><p><em>MySQL有近似函数concat。</em></p><p><strong>Replace</strong></p><pre><code>=Replace（指定字符串，哪个位置开始替换，替换几个字符，替换成什么）</code></pre><p>替换掉单元格的字符串，清洗使用较多。</p><p><em>MySQL中有同名函数，Python中有同名函数。</em></p><p><strong>Substitute</strong></p><p>和replace接近，区别是替换为全局替换，没有起始位置的概念</p><p><strong>Left／Right／Mid</strong></p><pre><code>=Mid(指定字符串，开始位置，截取长度)</code></pre><p>截取字符串中的字符。Left/Right（指定字符串，截取长度）。left为从左，right为从右，mid如上文示意。</p><p><em>MySQL中有同名函数。</em></p><p><strong>Len／Lenb</strong></p><p>返回字符串的长度，在len中，中文计算为一个，在lenb中，中文计算为两个。<br><em>MySQL中有同名函数，Python中有同名函数。</em></p><p><strong>Find</strong></p><pre><code>=Find（要查找字符，指定字符串，第几个字符）</code></pre><p>查找某字符串出现的位置，可以指定为第几次出现，与Left／Right／Mid结合能完成简单的文本提取<br><em>MySQL中有近似函数 find_in_set，Python中有同名函数。</em></p><p><strong>Search</strong></p><p>和Find类似，区别是Search大小写不敏感，但支持＊通配符</p><p><strong>Text</strong></p><p>将数值转化为指定的文本格式，可以和时间序列函数一起看</p><hr><h3 id="关联匹配类"><a href="#关联匹配类" class="headerlink" title="关联匹配类"></a>关联匹配类</h3><p>在进行多表关联或者行列比对时用到的函数，越复杂的表用得越多。多说一句，良好的表习惯可以减少这类函数的使用。</p><p><strong>Lookup</strong></p><pre><code>=Lookup（查找的值，值所在的位置，返回相应位置的值）</code></pre><p>最被忽略的函数，功能性和Vlookup一样，但是引申有数组匹配和二分法。</p><p><strong>Vlookup</strong></p><pre><code>=Vlookup(查找的值，哪里找，找哪个位置的值，是否精准匹配)</code></pre><p>Excel第一大难关，因为涉及的逻辑对新手较复杂，通俗的理解是查找到某个值然后黏贴过来。</p><p><strong>Index</strong></p><pre><code>＝Index（查找的区域，区域内第几行，区域内第几列）</code></pre><p>和Match组合，媲美Vlookup，但是功能更强大。</p><p><strong>Match</strong></p><pre><code>＝Match（查找指定的值，查找所在区域，查找方式的参数）</code></pre><p>和Lookup类似，但是可以按照指定方式查找，比如大于、小于或等于。返回值所在的位置。</p><p><strong>Row</strong></p><p>返回单元格所在的行</p><p><strong>Column</strong></p><p>返回单元格所在的列</p><p><strong>Offset</strong></p><pre><code>＝Offset（指定点，偏移多少行，偏移多少列，返回多少行，返回多少列）</code></pre><p>建立坐标系，以坐标系为原点，返回距离原点的值或者区域。正数代表向下或向右，负数则相反。</p><hr><h3 id="逻辑运算类"><a href="#逻辑运算类" class="headerlink" title="逻辑运算类"></a>逻辑运算类</h3><p>数据分析中不得不用到逻辑运算，逻辑运算返回的均是布尔类型，True和False。很多复杂的数据分析会牵扯到较多的逻辑运算</p><p><strong>IF</strong></p><p>经典的如果但是，在后期的Python中，也会经常用到，当然会有许多更优雅的写法。也有ifs用法，取代if(and())的写法。</p><p><em>MySQL中有同名函数，Python中有同名函数。</em></p><p><strong>And</strong></p><p>全部参数为True，则返回True，经常用于多条件判断。</p><p><em>MySQL中有同名函数，Python中有同名函数。</em></p><p><strong>Or</strong></p><p>只要参数有一个True，则返回Ture，经常用于多条件判断。</p><p><em>MySQL中有同名函数，Python中有同名函数。</em></p><p><strong>IS系列</strong></p><p>常用判断检验，返回的都是布尔数值True和False。常用ISERR，ISERROR，ISNA，ISTEXT，可以和IF嵌套使用。</p><hr><h3 id="计算统计类"><a href="#计算统计类" class="headerlink" title="计算统计类"></a>计算统计类</h3><p>常用的基础计算、分析、统计函数，以描述性统计为准。具体含义在后续的统计章节再展开。<br><strong>Sum／Sumif／Sumifs</strong></p><p>统计满足条件的单元格总和，SQL有中同名函数。</p><p><em>MySQL中有同名函数，Python中有同名函数。</em></p><p><strong>Sumproduct</strong></p><p>统计总和相关，如果有两列数据销量和单价，现在要求卖出增加，用sumproduct是最方便的。</p><p><em>MySQL中有同名函数。</em></p><p><strong>Count／Countif／Countifs</strong></p><p>统计满足条件的字符串个数</p><p><em>MySQL中有同名函数，Python中有同名函数。</em></p><p><strong>Max</strong></p><p>返回数组或引用区域的最大值</p><p><em>MySQL中有同名函数，Python中有同名函数。</em></p><p><strong>Min</strong></p><p>返回数组或引用区域的最小值</p><p><em>MySQL中有同名函数，Python中有同名函数。</em></p><p><strong>Rank</strong></p><p>排序，返回指定值在引用区域的排名，重复值同一排名。</p><p><em>SQL中有近似函数row_number() </em>。</p><p><strong>Rand／Randbetween</strong></p><p>常用随机抽样，前者返回0~1之间的随机值，后者可以指定范围。</p><p><em>MySQL中有同名函数。</em></p><p><strong>Averagea</strong></p><p>求平均值，也有Averageaif，Averageaifs</p><p><em>MySQL中有同名函数，python有近似函数mean。</em></p><p><strong>Quartile</strong></p><pre><code>=Quartile（指定区域，分位参数）</code></pre><p>计算四分位数，比如1~100的数字中，25分位就是按从小到大排列，在25%位置的数字，即25。参数0代表最小值，参数4代表最大值，1~3对应25、50（中位数）、75分位</p><p><strong>Stdev</strong></p><p>求标准差，统计型函数，后续数据分析再讲到</p><p><strong>Substotal</strong></p><pre><code>=Substotal（引用区域，参数）</code></pre><p>汇总型函数，将平均值、计数、最大最小、相乘、标准差、求和、方差等参数化，换言之，只要会了这个函数，上面的都可以抛弃掉了。</p><p><strong>Int／Round</strong></p><p>取整函数，int向下取整，round按小数位取数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">round(3.1415,2) =3.14 ;</span><br><span class="line">round(3.1415,1)=3.1</span><br></pre></td></tr></table></figure></p><hr><h3 id="时间序列类"><a href="#时间序列类" class="headerlink" title="时间序列类"></a>时间序列类</h3><p>专门用于处理时间格式以及转换，时间序列在金融、财务等数据分析中占有较大比重。时机序列的处理函数比我列举了还要复杂，比如时区、分片、复杂计算等。这里只做一个简单概述。</p><p><strong>Year</strong></p><p>返回日期中的年</p><p><em>MySQL中有同名函数。</em></p><p><strong>Month</strong></p><p>返回日期中的月</p><p><em>MySQL中有同名函数。</em></p><p><strong>Weekday</strong></p><pre><code>=Weekday(指定时间，参数)</code></pre><p>返回指定时间为一周中的第几天，参数为1代表从星期日开始算作第一天，参数为2代表从星期一开始算作第一天（中西方差异）。我们中国用2为参数即可。</p><p><em>MySQL中有同名函数。</em></p><p><strong>Weeknum</strong></p><pre><code>=Weeknum(指定时间，参数)</code></pre><p>返回一年中的第几个星期，后面的参数类同weekday，意思是从周日算还是周一。</p><p><em>MySQL中有近似函数 week。</em></p><p><strong>Day</strong></p><p>返回日期中的日（第几号）</p><p><em>MySQL中有同名函数。</em></p><p><strong>Date</strong></p><pre><code>=Date（年，月，日）</code></pre><p>时间转换函数，等于将year()，month()，day()合并</p><p><em>MySQL中有近似函数 date_format。</em></p><p><strong>Now</strong></p><p>返回当前时间戳，动态函数</p><p><em>MySQL中有同名函数。</em></p><p><strong>Today</strong></p><p>返回今天的日期，动态函数</p><p><em>MySQL中有同名函数。</em></p><p><strong>Datedif</strong></p><pre><code>=Datedif（开始日期，结束日期，参数）</code></pre><p>日期计算函数，计算两日期的差。参数决定返回的是年还是月等。</p><p><em>MySQL中有近似函数 DateDiff。</em></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>本文内容来源于网络，版权归原作者</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h4&gt;&lt;p&gt;此文针对数据分析EXCEL部分的入门。&lt;/p&gt;
&lt;p&gt;重点是了解各种函数，包括但不限于sum，count，sumif，countif，find，if，left/right，时间转换等。Excel函数不需要学全，&lt;strong&gt;重要的是学会搜索&lt;/strong&gt;。即如何将遇到的问题在搜索引擎上描述清楚。掌握vlookup和数据透视表足够，是最具性价比的两个技巧。&lt;/p&gt;
&lt;p&gt;学会vlookup，SQL中的join，Python中的merge很容易理解。&lt;/p&gt;
&lt;p&gt;学会数据透视表，SQL中的group，Python中的pivot_table也是同理。&lt;/p&gt;
&lt;p&gt;这两个搞定，基本10万条以内的数据统计没啥难度。Excel是熟能生巧，多找练习题。还有需要养成好习惯，不要合并单元格，不要过于花哨。表格按照原始数据（sheet1）、加工数据（sheet2），图表（sheet3）的类型管理。&lt;/p&gt;
&lt;p&gt;第一篇数据分析—函数篇。主要简单讲解常用的函数，以及与之对应的SQL/Python函数。&lt;/p&gt;
&lt;p&gt;温馨提示：如果您已经熟悉Excel，大可不必再看这篇文章，或只挑选部分。&lt;/p&gt;
    
    </summary>
    
      <category term="数据分析" scheme="https://paradoxallen.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Excel" scheme="https://paradoxallen.github.io/tags/Excel/"/>
    
  </entry>
  
  <entry>
    <title>常用数据类型的字节数</title>
    <link href="https://paradoxallen.github.io/18844/"/>
    <id>https://paradoxallen.github.io/18844/</id>
    <published>2017-01-29T16:00:00.000Z</published>
    <updated>2018-06-07T04:42:46.364Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1 G = 1024^3</span><br><span class="line">1 M = 1024^2</span><br><span class="line">1 K = 1024</span><br><span class="line">1 GB = 1024 MB = 1024^3 Byte(字节)</span><br><span class="line">1 MB = 1024 KB = 1024^2 Byte(字节)</span><br><span class="line">1 KB = 1024 Byte(字节)</span><br><span class="line">1 Byte = 8 Bit(位)二进制</span><br></pre></td></tr></table></figure><p>1个整数占4个字节，1个字节为8位二进制(1个字节能表示2^8=256个数字范围，大小可以从-2^7-1~2^7)，因此1个整数为32位二进制(1个整数表示2^32个数字范围)</p><p>8进制必须以0开头，16进制数必须以0x开头。 </p><p>使用ASCII编码，可以将一个字节Byte转换成一个字符Character。数据类型的长度跟编译器和系统有关。</p><p><strong>在Windows下32位编译器的数据类型字节数：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">char：1个字节</span><br><span class="line">char\*(即指针变量)：4个字节(32位的寻址空间是2^32，即32个bit，也就是4个字节。)</span><br><span class="line">short int：2个字节</span><br><span class="line">int：4个字节</span><br><span class="line">unsigned int：4个字节</span><br><span class="line">float：4个字节</span><br><span class="line">double：8个字节</span><br><span class="line">long：4个字节</span><br><span class="line">long long：8个字节</span><br><span class="line">unsigned long：4个字节</span><br></pre></td></tr></table></figure></p><p><strong>在Windows下64位编译器的数据类型字节数：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">char：1个字节</span><br><span class="line">char\*(即指针变量)：8个字节(64位的寻址空间是2^64，即64个bit，也就是8个字节。)</span><br><span class="line">short int：2个字节</span><br><span class="line">int：4个字节</span><br><span class="line">unsigned int：4个字节</span><br><span class="line">float：4个字节</span><br><span class="line">double：8个字节</span><br><span class="line">long：4个字节</span><br><span class="line">long long：8个字节</span><br><span class="line">unsigned long：4个字节</span><br></pre></td></tr></table></figure></p><p>但如果是linux系统的话，在64位编译器下long和unsigned long类型将变成8个字节。</p><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p><a href="http://www.lining0806.com/c%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%AD%97%E8%8A%82%E6%95%B0/" target="_blank" rel="noopener">宁哥的小站 » 常用数据类型的字节数</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class
      
    
    </summary>
    
      <category term="数据结构" scheme="https://paradoxallen.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="数据类型" scheme="https://paradoxallen.github.io/tags/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>ASCII、Unicode和UTF-8编码的区别</title>
    <link href="https://paradoxallen.github.io/61074/"/>
    <id>https://paradoxallen.github.io/61074/</id>
    <published>2017-01-26T16:00:00.000Z</published>
    <updated>2018-06-07T04:45:27.669Z</updated>
    
    <content type="html"><![CDATA[<h3 id="归纳"><a href="#归纳" class="headerlink" title="归纳"></a>归纳</h3><table><thead><tr><th>编码</th><th style="text-align:center">大小</th><th style="text-align:right">支持语言</th></tr></thead><tbody><tr><td>ASCII</td><td style="text-align:center">1个字节</td><td style="text-align:right">英文</td></tr><tr><td>Unicode</td><td style="text-align:center">2个字节（生僻字4个）</td><td style="text-align:right">所有语言</td></tr><tr><td>UTF-8</td><td style="text-align:center">1-6个字节，英文字母1个字节，汉字3个字节，生僻字4-6个字节</td><td style="text-align:right">所有语言</td></tr></tbody></table><h3 id="具体解释："><a href="#具体解释：" class="headerlink" title="具体解释："></a>具体解释：</h3><p>最早只有127个字母被编码到计算机里，也就是大小写英文字母、数字和一些符号，这个编码表被称为ASCII编码，比如大写字母A的编码是65，小写字母z的编码是122。</p><p>但是要处理中文显然一个字节是不够的，至少需要两个字节，而且还不能和ASCII编码冲突，所以，中国制定了GB2312编码，用来把中文编进去。</p><p>你可以想得到的是，全世界有上百种语言，日本把日文编到Shift_JIS里，韩国把韩文编到Euc-kr里，各国有各国的标准，就会不可避免地出现冲突，结果就是，在多语言混合的文本中，显示出来会有乱码。</p><p>因此，Unicode应运而生。Unicode把所有语言都统一到一套编码里，这样就不会再有乱码问题了。</p><p>Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。现代操作系统和大多数编程语言都直接支持Unicode。</p><p>新的问题又出现了：如果统一成Unicode编码，乱码问题从此消失了。但是，如果你写的文本基本上全部是英文的话，用Unicode编码比ASCII编码需要多一倍的存储空间，在存储和传输上就十分不划算。</p><p>所以，本着节约的精神，又出现了把Unicode编码转化为“可变长编码”的UTF-8编码。UTF-8编码把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。如果你要传输的文本包含大量英文字符，用UTF-8编码就能节省空间。</p><p>UTF-8编码有一个额外的好处，就是ASCII编码实际上可以被看成是UTF-8编码的一部分，所以，大量只支持ASCII编码的历史遗留软件可以在UTF-8编码下继续工作。</p><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p><a href="https://baike.baidu.com/item/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BC%96%E7%A0%81/9127611?fr=aladdin" target="_blank" rel="noopener">计算机编码_百度百科</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;归纳&quot;&gt;&lt;a href=&quot;#归纳&quot; class=&quot;headerlink&quot; title=&quot;归纳&quot;&gt;&lt;/a&gt;归纳&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;编码&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;大小&lt;/th&gt;
&lt;t
      
    
    </summary>
    
      <category term="计算机原理" scheme="https://paradoxallen.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%8E%9F%E7%90%86/"/>
    
    
      <category term="编码" scheme="https://paradoxallen.github.io/tags/%E7%BC%96%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>博客搭建——利用GitHub+Hexo</title>
    <link href="https://paradoxallen.github.io/13431/"/>
    <id>https://paradoxallen.github.io/13431/</id>
    <published>2017-01-20T16:00:00.000Z</published>
    <updated>2018-06-09T05:35:01.482Z</updated>
    
    <content type="html"><![CDATA[<p>一直想有一个可以记录的属于自己的博客，最近在知乎上看到了利用<a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>+<a href="https://github.com/" target="_blank" rel="noopener">GitHub</a>搭建博客的教程，于是乎搭建了自己的博客<a href="https://paradoxallen.github.io/">paradoxallen</a>。</p><p>然而在此过程也遇到了一些bug，希望以下图文并茂的教程（虽然已经有挺多类似的教程…但主要是想自己写一篇文章!!!而且保证超级详细!!!）可以帮助大家少走弯路，搭建属于自己的博客。</p><a id="more"></a><hr><h2 id="GitHub配置"><a href="#GitHub配置" class="headerlink" title="GitHub配置"></a>GitHub配置</h2><p><a href="https://github.com/" target="_blank" rel="noopener">https://github.com/ </a>登录GitHub账号,如无GitHub帐号需要注册一个</p><p><img src="https://i.imgur.com/stc576q.png" alt=""></p><p>点击GitHub中的New repository创建新仓库</p><p><img src="https://i.imgur.com/03FzRP1.png" alt=""></p><p>仓库名应该为：用户名.<a href="http://github.io" target="_blank" rel="noopener">http://github.io</a> </p><p>这个用户名使用你的GitHub帐号名称代替，这是固定写法</p><p>如我的域名是<a href="http://github.com/paradoxallen" target="_blank" rel="noopener">github.com/paradoxallen</a>，就填入<a href="http://paradoxallen.github.io">paradoxallen.github.io</a></p><p>然后点击create repository创建仓库</p><p><img src="https://i.imgur.com/i2kuLuv.png" alt=""></p><p>成功之后出现以下画面</p><p><img src="https://i.imgur.com/pssW3Ty.png" alt=""></p><h2 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h2><h3 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h3><p><a href="https://git-scm.com/download/win" target="_blank" rel="noopener">Git-Downloading Package</a>选择下载Windows版本的64位或32位的安装包（也有MacOSX版本和Linux/Unix版本的，按需求下载）</p><p><img src="https://i.imgur.com/gMvgJu4.png" alt=""></p><p>下载后安装，基本按默认安装就行，安装成功后，鼠标右键打开Git Bash</p><p><img src="https://i.imgur.com/RSyUDIo.png" alt=""></p><p>然后设置user.name和user.email配置信息<br><code>git config --global user.name &quot;你的GitHub用户名&quot;</code><br><code>git config --global user.email &quot;你的GitHub注册邮箱&quot;</code></p><p>生成ssh密钥文件：<br><code>ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot;</code></p><p><img src="https://i.imgur.com/iw4LaX4.png" alt=""></p><p>然后直接三个回车，无需设置密码</p><p>在找到生成的.ssh的文件夹中的id-rsa.pub密钥，将内容全部复制</p><p><img src="https://i.imgur.com/8NGi2sJ.png" alt=""></p><p>打开<a href="https://github.com/settings/keys/new" target="_blank" rel="noopener">GitHub-Settings-SSHandGPGkeys-newSHHkeys</a>新建</p><p><img src="https://i.imgur.com/CGfHsyf.png" alt=""></p><p>Title随意，Key粘贴id-rsa.pub内容，最后点击Add SHH key</p><p>在Git Bash中检测GitHub公钥是否设置成功，输入<code>ssh git@github.com</code>如下则说明成功</p><p><img src="https://i.imgur.com/a0UzTyC.png" alt=""></p><h3 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h3><p><a href="https://nodejs.org/en/download/" target="_blank" rel="noopener">Download|Node.js</a>选择下载安装包，也是默认设置安装就好</p><p><img src="https://i.imgur.com/pIdHJSl.png" alt=""></p><h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>这里需要在自己电脑中新建文件夹，如命名为blog</p><p>进入文件夹，按住shift键，右击鼠标点击在此处打开Powershell窗口</p><p>（有些教程是打开命令行，但好像是win10才有的powershell)</p><p><img src="https://i.imgur.com/acTMY1r.png" alt=""></p><p>然后依次输入：</p><p><code>npm install -g hexo-cli</code>安装Hexo</p><p><code>hexo init blog</code>初始化Hexo</p><p>成功提示<code>INFO  Start blogging with Hexo!</code></p><p>（因为我已经设置好就不重新初始化啦）</p><p>因为你初始化hexo 之后source目录下自带一篇hello world文章, 所以直接执行下方命令<br><code>hexo g</code> </p><p><code>hexo s</code>启动本地服务器</p><p>成功提示<br><code>INFO  Start processing</code><br><code>INFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.</code></p><p><img src="https://i.imgur.com/efRF9eP.png" alt=""></p><p>在浏览器输入<code>http://localhost:4000/</code>就可以看见网页和模板了（因为我修改过主题可能有点出入）</p><p><img src="https://i.imgur.com/vdd9Yqu.png" alt=""></p><h2 id="网站推送"><a href="#网站推送" class="headerlink" title="网站推送"></a>网站推送</h2><p>打开blog根目录_config.yml文件，将Hexo与GitHub关联起来</p><p><img src="https://i.imgur.com/xeLbhec.png" alt=""></p><p><code>deploy:</code></p><p><code>type: git</code></p><p><code>repo: GitHub上创建仓库的完整路径，加上 .git</code></p><p><code>branch: master</code></p><p>参考如下：</p><p><img src="https://i.imgur.com/GlMtiYl.png" alt=""></p><p>然后保存并执行命令：<br><code>npm install hexo-deployer-git --save</code></p><p>再依次输入三条命令:</p><p><code>hexo clean</code> </p><p><code>hexo g</code></p><p><code>hexo d</code></p><p>打开浏览器，在地址栏输入你的放置个人网站的仓库路径，即 <a href="http://xxxx.github.io，如[paradoxallen.github.io](https://paradoxallen.github.io/)即可访问" target="_blank" rel="noopener">http://xxxx.github.io，如[paradoxallen.github.io](https://paradoxallen.github.io/)即可访问</a></p><p>此外，也可以打开_config.yml文件修改参数信息，如排版格式等等</p><h2 id="文章发布"><a href="#文章发布" class="headerlink" title="文章发布"></a>文章发布</h2><p>输入：<code>hexo new &quot;testing&quot;</code>打开文件使用markdown语法输入文字</p><p>保存，执行：</p><p><code>hexo clean</code></p><p><code>hexo g</code></p><p><code>hexo server</code></p><p><code>hexo deploy</code></p><p>就可以看到文章发布了~</p><p><img src="https://i.imgur.com/17nnmmw.png" alt=""></p><h2 id="总结陈词"><a href="#总结陈词" class="headerlink" title="总结陈词"></a>总结陈词</h2><p>搭建博客的步骤：</p><p>1、GitHub配置</p><p>2、环境安装</p><p>3、网站推送</p><p>4、文章发布</p><p>发布文章的步骤：</p><p>1、hexo new 创建文章</p><p>2、Markdown语法编辑文章</p><p>3、部署（所有打开CMD都是在blog目录下）</p><p>到这里已经完成了博客的搭建以及文章的发布，但是还有很多需要设置和调整的。</p><p>我也是刚刚搭建好博客，还有待博客界面的优化以及内容的丰富!!!</p><p>今天先到这里啦~</p><p>##参考资料<br>1.<a href="https://zhangslob.github.io/2017/02/28/%E6%95%99%E4%BD%A0%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%EF%BC%8CHexo-Github/" target="_blank" rel="noopener">教你免费搭建个人博客，Hexo&amp;Github</a></p><p>2.<a href="https://zhuanlan.zhihu.com/p/26625249" target="_blank" rel="noopener">GitHub+Hexo 搭建个人网站详细教程</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一直想有一个可以记录的属于自己的博客，最近在知乎上看到了利用&lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;+&lt;a href=&quot;https://github.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;搭建博客的教程，于是乎搭建了自己的博客&lt;a href=&quot;https://paradoxallen.github.io/&quot;&gt;paradoxallen&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;然而在此过程也遇到了一些bug，希望以下图文并茂的教程（虽然已经有挺多类似的教程…但主要是想自己写一篇文章!!!而且保证超级详细!!!）可以帮助大家少走弯路，搭建属于自己的博客。&lt;/p&gt;
    
    </summary>
    
      <category term="博客开发" scheme="https://paradoxallen.github.io/categories/%E5%8D%9A%E5%AE%A2%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="hexo" scheme="https://paradoxallen.github.io/tags/hexo/"/>
    
      <category term="博客" scheme="https://paradoxallen.github.io/tags/%E5%8D%9A%E5%AE%A2/"/>
    
      <category term="GitHub" scheme="https://paradoxallen.github.io/tags/GitHub/"/>
    
  </entry>
  
</feed>
