<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>RP&#39;s Blog</title>
  
  <subtitle>学习总结  思考感悟</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://paradoxallen.github.io/"/>
  <updated>2018-06-13T04:55:26.430Z</updated>
  <id>https://paradoxallen.github.io/</id>
  
  <author>
    <name>LRP</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>关于机器学习在大气科学的应用</title>
    <link href="https://paradoxallen.github.io/21048/"/>
    <id>https://paradoxallen.github.io/21048/</id>
    <published>2018-05-06T16:00:00.000Z</published>
    <updated>2018-06-13T04:55:26.430Z</updated>
    
    <content type="html"><![CDATA[<p>前阵子阅读了院里一位博士研究生师兄的一篇有关基于神经网络算法对北京近五年的常规探空数据进行自组织分类，并揭示出大气污染物在不同边界层结构下的演变规律和相关机制的文章<a href="https://www.atmos-chem-phys.net/18/6771/2018/" target="_blank" rel="noopener">《Self-organized classification of boundary layer meteorology and associated characteristics of air quality in Beijing》</a>，看完顿时心生膜拜之情；</p><p>然后恰巧也是那个时候吕教授在院群上也转发了一篇关于机器学习预测火势甚至天气的公众号文章<a href="https://mp.weixin.qq.com/s?__biz=MjM5ODE1NDYyMA==&amp;mid=2653384819&amp;idx=2&amp;sn=523f27cb9442ab4af27137edd1280248&amp;chksm=bd1cc8608a6b4176d7cae939f794e082cf86b5bf0ac0deb53790f507f563f588b2148687957c&amp;mpshare=1&amp;scene=1&amp;srcid=0517RYcQWk5dWJCcqoI7jLCe#rd" target="_blank" rel="noopener">《机器学习成功解决“蝴蝶效应”！以后你终于可以相信天气预报了》</a>。</p><p>加之自己报名了一个<a href="https://mp.weixin.qq.com/s?__biz=MzIzMjQyNzQ5MA==&amp;mid=2247487345&amp;idx=1&amp;sn=4acb8978a2d95f0a1926c0e07021bec3&amp;chksm=e89455fcdfe3dcea645499d9321177a99ea713ffe06fe4af9d18c8506dd285755be2a1640539&amp;mpshare=1&amp;scene=1&amp;srcid=04151pmRgGqnfZDpvBL9EKKk#rd" target="_blank" rel="noopener">“预测北京和伦敦两个城市的空气质量”的KDD Cup 2018</a>但是因为自己报名太晚，组队不成（其实更深层的是之前关于机器学习的内容已经忘得差不多了。。。）</p><p>如此的机缘巧合，感觉将机器学习应用于大气科学将前途无量。我自己也想在这一方向进行深入了解，接下来我会进行相关内容的学习。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前阵子阅读了院里一位博士研究生师兄的一篇有关基于神经网络算法对北京近五年的常规探空数据进行自组织分类，并揭示出大气污染物在不同边界层结构下的演变规律和相关机制的文章&lt;a href=&quot;https://www.atmos-chem-phys.net/18/6771/2018/&quot;
      
    
    </summary>
    
      <category term="个人随笔" scheme="https://paradoxallen.github.io/categories/%E4%B8%AA%E4%BA%BA%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="大气科学" scheme="https://paradoxallen.github.io/tags/%E5%A4%A7%E6%B0%94%E7%A7%91%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>统计推断(零) 章节简介</title>
    <link href="https://paradoxallen.github.io/5451/"/>
    <id>https://paradoxallen.github.io/5451/</id>
    <published>2018-03-31T16:00:00.000Z</published>
    <updated>2018-06-03T16:17:44.275Z</updated>
    
    <content type="html"><![CDATA[<p>《统计推断(翻译版·原书第2版)》从概率论的基础开始，通过例子与习题的旁征博引，引进了大量近代统计处理的新技术和一些国内同类教材中不常见而又广为使用的分布。</p><p>其内容既包括工科概率入门、经典统计和现代统计的基础，又加进了不少近代统计中数据处理的实用方法和思想，例如：Bootstrap再抽样法、刀切(Jackkrlife)估计、EM算法、Logistic回归、稳健(Robest)回归、Markov链、Monte Carlo方法等。</p><p>它的统计内容与国内流行的教材相比，理论较深，模型较多，案例的涉及面要广，理论的应用面要丰富，统计思想的阐述与算法更为具体。</p><p>《统计推断(翻译版·原书第2版)》可作为工科、管理类学科专业本科生、研究生的教材或参考书，也可供教师、工程技术人员自学之用。</p><a id="more"></a><hr><h3 id="章节简介"><a href="#章节简介" class="headerlink" title="章节简介"></a><strong>章节简介</strong></h3><p><strong>出版说明</strong><br><strong>第2版序</strong><br><strong>第1版序</strong><br><strong>译后序</strong><br><strong>第1章 概率论</strong><br>1.1 集合论<br>1.2 概率论基础<br>1.2.1 公理化基础<br>1.2.2 概率演算<br>1.2.3 计数<br>1.2.4 枚举结果<br>1.3 条件概率与独立性<br>1.4 随机变量<br>1.5 分布函数<br>1.6 概率密度函数和概率质量函数<br>1.7 习题<br>1.8 杂录<br><strong>第2章 变换和期望</strong><br>2.1 随机变量函数的分布<br>2.2 期望<br>2.3 矩和矩母函数<br>2.4 积分号下的求导<br>2.5 习题<br>2.6 杂录<br>2.6.1 矩列的唯一性<br>2.6.2 其他母函数<br>2.6.3 矩母函数能否唯一地确定分布？<br><strong>第3章 常见分布族</strong><br>3.1 引言<br>3.2 离散分布<br>3.3 连续分布<br>3.4 指数族<br>3.5 位置与尺度族<br>3.6 不等式与恒等式<br>3.6.1 概率不等式<br>3.6.2 恒等式<br>3.7 习题<br>3.8 杂录<br>3.8.1 Poisson假设<br>3.8.2 Chebychev不等式及其改进<br>3.8.3 再谈指数族<br><strong>第4章 多维随机变量</strong><br>4.1 联合分布与边缘分布<br>4.2 条件分布与独立性<br>4.3 二维变换<br>4.4 多层模型与混合分布<br>4.5 协方差与相关<br>4.6 多维分布<br>4.7 不等式<br>4.7.1 数值不等式<br>4.7.2 函数不等式<br>4.8 习题<br>4.9 杂录<br>4.9.1 交换悖论<br>4.9.2 算术－几何－调和平均值不等式<br>8.3.1 错误概率与功效函数<br>8.3.2 最大功效检验<br>8.3.3 并－检验与交－并检验的真实水平<br>8.3.4 P-值<br>8.3.5 损失函数最优性<br>8.4 习题<br>8.5 杂录<br>8.5.1 单调功效函数<br>8.5.2 似然比作为证据<br>8.5.3 P-值和后验概率<br>8.5.4 置信集P-值<br><strong>第9章 区间估计</strong><br>9.1 引言<br>9.2 区间估计量的求法<br>9.2.1 反转一个检验统计量<br>9.2.2 枢轴量<br>9.2.3 枢轴化累积分布函数<br>9.2.4 Bayes区间<br>9.3 区间估计量的评价方法<br>9.3.1 尺寸和覆盖概率<br>9.3.2 与检验相关的最优性<br>9.3.3 Bayes最优<br>9.3.4 损失函数最优<br>9.4 习题<br>9.5 杂录<br>9.5.1 置信方法<br>9.5.2 离散分布中的置信区间<br>9.5.3 Fieller定理<br>9.5.4 其他区间如何?<br><strong>第10章 渐近评价</strong><br>10.1 点估计<br>10.1.1 相合性<br>10.1.2 有效性<br>10.1.3 计算与比较<br>10.1.4 自助法标准误差<br>10.2 稳健性<br>10.2.1 均值和中位数<br>10.2.2 M_估计量<br>10.3 假设检验<br>10.3.1 LRT的渐近分布<br>10.3.2 其他大样本检验<br>10.4 区间估计<br>10.4.1 近似极大似然区间<br>10.4.2 其他大样本区间<br>10.5 习题<br>10.6 杂录<br>10.6.1 超有效性<br>10.6.2 适当的正则性条件<br>10.6.3 再谈自助法<br>10.6.4 影响函数<br>10.6.5 自助法区间<br>10.6.6 稳健区间<br><strong>第11章 方差分析和回归分析</strong><br>11.1 引言<br>11.2 一种方式分组的方差分析<br>11.2.1 模型和分布假定<br>11.2.2 经典的ANOVA假设<br>11.2.3 均值的线性组合的推断<br>11.2.4 ANOVAF检验<br>11.2.5 对比的同时估计<br>11.2.6 平方和的分解<br>11.3 简单线性回归<br>11.3.1 最小二乘：数学解<br>11.3.2 最佳线性无偏估计：统计解<br>11.3.3 模型和分布假定<br>11.3.4 正态误差下的估计和检验<br>11.3.5 在给定点x=x0处的估计和预测<br>11.3.6 同时估计和置信带<br>11.4 习题<br>11.5 杂录<br>11.5.1 Cochran定理<br>11.5.2 多重比较<br>11.5.3 随机化完全区组设计<br>11.5.4 其他类型的方差分析<br>11.5.5 置信带的形状<br>11.5.6 Stein悖论<br><strong>第12章 回归模型</strong><br>12.1 引言<br>12.2 变量有误差时的回归<br>12.2.1 函数关系和结构关系<br>12.2.2 最小二乘解<br>12.2.3 极大似然估计<br>12.2.4 置信集<br>12.3 罗吉斯蒂克回归<br>12.3.1 模型<br>12.3.2 估计<br>12.4 稳健回归<br>12.5 习题<br>12.6 杂录<br>12.6.1 函数和结构的意义<br>12.6.2 EIV模型中常规最小乘的相合性<br>12.6.3 EIV模型中的工具变量<br>12.6.4 罗吉斯蒂克似然方程<br>12.6.5 再谈稳健回归<br><strong>附录 计算机代数</strong><br><strong>常用分布表</strong><br><strong>参考文献</strong><br><strong>作者索引</strong><br><strong>名词索引</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《统计推断(翻译版·原书第2版)》从概率论的基础开始，通过例子与习题的旁征博引，引进了大量近代统计处理的新技术和一些国内同类教材中不常见而又广为使用的分布。&lt;/p&gt;
&lt;p&gt;其内容既包括工科概率入门、经典统计和现代统计的基础，又加进了不少近代统计中数据处理的实用方法和思想，例如：Bootstrap再抽样法、刀切(Jackkrlife)估计、EM算法、Logistic回归、稳健(Robest)回归、Markov链、Monte Carlo方法等。&lt;/p&gt;
&lt;p&gt;它的统计内容与国内流行的教材相比，理论较深，模型较多，案例的涉及面要广，理论的应用面要丰富，统计思想的阐述与算法更为具体。&lt;/p&gt;
&lt;p&gt;《统计推断(翻译版·原书第2版)》可作为工科、管理类学科专业本科生、研究生的教材或参考书，也可供教师、工程技术人员自学之用。&lt;/p&gt;
    
    </summary>
    
      <category term="应用统计" scheme="https://paradoxallen.github.io/categories/%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="统计" scheme="https://paradoxallen.github.io/tags/%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>第一个Python程序</title>
    <link href="https://paradoxallen.github.io/10471/"/>
    <id>https://paradoxallen.github.io/10471/</id>
    <published>2017-10-03T16:00:00.000Z</published>
    <updated>2018-06-25T06:35:09.371Z</updated>
    
    <content type="html"><![CDATA[<h3 id="命令行模式"><a href="#命令行模式" class="headerlink" title="命令行模式"></a>命令行模式</h3><p>在Windows开始菜单选择“命令提示符”，就进入到命令行模式，它的提示符类似C:>：</p><h3 id="Python交互模式"><a href="#Python交互模式" class="headerlink" title="Python交互模式"></a>Python交互模式</h3><p>在命令行模式下敲命令python，就看到类似如下的一堆文本输出，然后就进入到Python交互模式，它的提示符是&gt;&gt;&gt;。</p><p>在Python交互模式下输入exit()并回车，就退出了Python交互模式，并回到命令行模式：</p><p>也可以直接通过开始菜单选择Python (command line)菜单项，直接进入Python交互模式，但是输入exit()后窗口会直接关闭，不会回到命令行模式。</p><p>了解了如何启动和退出Python的交互模式，我们就可以正式开始编写Python代码了。</p><p>在写代码之前，请千万不要用“复制”-“粘贴”把代码从页面粘贴到你自己的电脑上。写程序也讲究一个感觉，你需要一个字母一个字母地把代码自己敲进去，在敲代码的过程中，初学者经常会敲错代码：拼写不对，大小写不对，混用中英文标点，混用空格和Tab键，所以，你需要仔细地检查、对照，才能以最快的速度掌握如何写程序。</p><p>在交互模式的提示符&gt;&gt;&gt;下，直接输入代码，按回车，就可以立刻得到代码执行结果。现在，试试输入100+200，看看计算结果是不是300：</p><p>很简单吧，任何有效的数学计算都可以算出来。</p><p>如果要让Python打印出指定的文字，可以用print()函数，然后把希望打印的文字用单引号或者双引号括起来，但不能混用单引号和双引号：</p><p>这种用单引号或者双引号括起来的文本在程序中叫字符串，今后我们还会经常遇到。</p><p>最后，用exit()退出Python，我们的第一个Python程序完成！唯一的缺憾是没有保存下来，下次运行时还要再输入一遍代码。</p><h3 id="命令行模式和Python交互模式"><a href="#命令行模式和Python交互模式" class="headerlink" title="命令行模式和Python交互模式"></a>命令行模式和Python交互模式</h3><p>请注意区分命令行模式和Python交互模式。</p><p>在命令行模式下，可以执行python进入Python交互式环境，也可以执行python hello.py运行一个.py文件。</p><p>执行一个.py文件只能在命令行模式执行。如果敲一个命令python hello.py，看到如下错误：</p><p>错误提示No such file or directory说明这个hello.py在当前目录找不到，必须先把当前目录切换到hello.py所在的目录下，才能正常执行：</p><p>此外，在命令行模式运行.py文件和在Python交互式环境下直接运行Python代码有所不同。Python交互式环境会把每一行Python代码的结果自动打印出来，但是，直接运行Python代码却不会。</p><p>例如，在Python交互式环境下，输入：</p><blockquote><blockquote><blockquote><p>100 + 200 + 300<br>600<br>直接可以看到结果600。</p></blockquote></blockquote></blockquote><p>但是，写一个calc.py的文件，内容如下：</p><p>100 + 200 + 300<br>然后在命令行模式下执行：</p><p>C:\work&gt;python calc.py<br>发现什么输出都没有。</p><p>这是正常的。想要输出结果，必须自己用print()打印出来。把calc.py改造一下：</p><p>print(100 + 200 + 300)<br>再执行，就可以看到结果：</p><p>C:\work&gt;python calc.py<br>600<br>最后，Python交互模式的代码是输入一行，执行一行，而命令行模式下直接运行.py文件是一次性执行该文件内的所有代码。可见，Python交互模式主要是为了调试Python代码用的，也便于初学者学习，它不是正式运行Python代码的环境！</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>在Python交互式模式下，可以直接输入代码，然后执行，并立刻得到结果。</p><p>在命令行模式下，可以直接运行.py文件。</p><hr><h2 id="文本编辑器"><a href="#文本编辑器" class="headerlink" title="文本编辑器"></a>文本编辑器</h2><p>在Python的交互式命令行写程序，好处是一下就能得到结果，坏处是没法保存，下次还想运行的时候，还得再敲一遍。</p><p>所以，实际开发的时候，我们总是使用一个文本编辑器来写代码，写完了，保存为一个文件，这样，程序就可以反复运行了。</p><p>现在，我们就把上次的’hello, world’程序用文本编辑器写出来，保存下来。</p><p>那么问题来了：文本编辑器到底哪家强？</p><p>推荐两款文本编辑器：</p><p>一个是Sublime Text，免费使用，但是不付费会弹出提示框：</p><p><img src="https://i.imgur.com/Hyc7im0.jpg" alt=""></p><p>一个是Notepad++，免费使用，有中文界面：</p><p><img src="https://i.imgur.com/Xhx1QSE.jpg" alt=""></p><p>请注意，用哪个都行，但<strong>是绝对不能用Word和Windows自带的记事本</strong>。Word保存的不是纯文本文件，而记事本会自作聪明地在文件开始的地方加上几个特殊字符（UTF-8 BOM），结果会导致程序运行出现莫名其妙的错误。</p><p>安装好文本编辑器后，输入以下代码：</p><pre><code>print(&apos;hello, world&apos;)</code></pre><p>注意print前面不要有任何空格。然后，选择一个目录，例如C:\work，把文件保存为hello.py，就可以打开命令行窗口，把当前目录切换到hello.py所在目录，就可以运行这个程序了：</p><p><code>C:\work&gt;python hello.pyhello, world</code><br>也可以保存为别的名字，比如first.py，但是必须要以.py结尾，其他的都不行。此外，文件名只能是英文字母、数字和下划线的组合。</p><p>如果当前目录下没有hello.py这个文件，运行python hello.py就会报错：</p><p><code>C:\Users\IEUser&gt;python hello.pypython: can&#39;t open file &#39;hello.py&#39;: [Errno 2] No such file or directory</code><br>报错的意思就是，无法打开hello.py这个文件，因为文件不存在。这个时候，就要检查一下当前目录下是否有这个文件了。如果hello.py存放在另外一个目录下，要首先用cd命令切换当前目录。</p><h3 id="直接运行py文件"><a href="#直接运行py文件" class="headerlink" title="直接运行py文件"></a>直接运行py文件</h3><p>有同学问，能不能像.exe文件那样直接运行.py文件呢？在Windows上是不行的，但是，在Mac和Linux上是可以的，方法是在.py文件的第一行加上一个特殊的注释：</p><pre><code>#!/usr/bin/env python3</code></pre><p>print(‘hello, world’)<br>然后，通过命令给hello.py以执行权限：</p><pre><code>$ chmod a+x hello.py</code></pre><p>就可以直接运行hello.py了，比如在Mac下运行：</p><p><img src="https://i.imgur.com/99hTSjo.png" alt=""></p><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>用文本编辑器写Python程序，然后保存为后缀为.py的文件，就可以用Python直接运行这个程序了。</p><p>Python的交互模式和直接运行.py文件有什么区别呢？</p><p>直接输入python进入交互模式，相当于启动了Python解释器，但是等待你一行一行地输入源代码，每输入一行就执行一行。</p><p>直接运行.py文件相当于启动了Python解释器，然后一次性把.py文件的源代码给执行了，你是没有机会以交互的方式输入源代码的。</p><p>用Python开发程序，完全可以一边在文本编辑器里写代码，一边开一个交互式命令窗口，在写代码的过程中，把部分代码粘到命令行去验证，事半功倍！前提是得有个27’的超大显示器！</p><hr><h2 id="输入和输出"><a href="#输入和输出" class="headerlink" title="输入和输出"></a>输入和输出</h2><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><p>用print()在括号中加上字符串，就可以向屏幕上输出指定的文字。比如输出’hello, world’，用代码实现如下：</p><pre><code>&gt;&gt;&gt; print(&apos;hello, world&apos;)</code></pre><p>print()函数也可以接受多个字符串，用逗号“,”隔开，就可以连成一串输出：</p><pre><code>&gt;&gt;&gt; print(&apos;The quick brown fox&apos;, &apos;jumps over&apos;, &apos;the lazy dog&apos;)The quick brown fox jumps over the lazy dog</code></pre><p>print()会依次打印每个字符串，遇到逗号“,”会输出一个空格</p><p>print()也可以打印整数，或者计算结果：</p><pre><code>&gt;&gt;&gt; print(300)300&gt;&gt;&gt; print(100 + 200)300</code></pre><p>因此，我们可以把计算100 + 200的结果打印得更漂亮一点：</p><pre><code>&gt;&gt;&gt; print(&apos;100 + 200 =&apos;, 100 + 200)100 + 200 = 300</code></pre><p>注意，对于100 + 200，Python解释器自动计算出结果300，但是，’100 + 200 =’是字符串而非数学公式，Python把它视为字符串，请自行解释上述打印结果。</p><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><p>现在，你已经可以用print()输出你想要的结果了。但是，如果要让用户从电脑输入一些字符怎么办？Python提供了一个input()，可以让用户输入字符串，并存放到一个变量里。比如输入用户的名字：</p><pre><code>&gt;&gt;&gt; name = input()Michael</code></pre><p>当你输入name = input()并按下回车后，Python交互式命令行就在等待你的输入了。这时，你可以输入任意字符，然后按回车后完成输入。</p><p>输入完成后，不会有任何提示，Python交互式命令行又回到&gt;&gt;&gt;状态了。那我们刚才输入的内容到哪去了？答案是存放到name变量里了。可以直接输入name查看变量内容：</p><pre><code>&gt;&gt;&gt; name&apos;Michael&apos;</code></pre><p><strong>什么是变量？</strong>请回忆初中数学所学的代数基础知识：</p><p>设正方形的边长为a，则正方形的面积为a x a。把边长a看做一个变量，我们就可以根据a的值计算正方形的面积，比如：</p><p>若a=2，则面积为a x a = 2 x 2 = 4；</p><p>若a=3.5，则面积为a x a = 3.5 x 3.5 = 12.25。</p><p>在计算机程序中，变量不仅可以为整数或浮点数，还可以是字符串，因此，name作为一个变量就是一个字符串。</p><p>要打印出name变量的内容，除了直接写name然后按回车外，还可以用print()函数：</p><pre><code>&gt;&gt;&gt; print(name)Michael</code></pre><p>有了输入和输出，我们就可以把上次打印’hello, world’的程序改成有点意义的程序了：</p><pre><code>name = input()print(&apos;hello,&apos;, name)</code></pre><p>运行上面的程序，第一行代码会让用户输入任意字符作为自己的名字，然后存入name变量中；第二行代码会根据用户的名字向用户说hello，比如输入Michael：</p><pre><code>C:\Workspace&gt; python hello.pyMichaelhello, Michael</code></pre><p>但是程序运行的时候，没有任何提示信息告诉用户：“嘿，赶紧输入你的名字”，这样显得很不友好。幸好，input()可以让你显示一个字符串来提示用户，于是我们把代码改成：</p><pre><code>name = input(&apos;please enter your name: &apos;)print(&apos;hello,&apos;, name)</code></pre><p>再次运行这个程序，你会发现，程序一运行，会首先打印出please enter your name:，这样，用户就可以根据提示，输入名字后，得到hello, xxx的输出：</p><pre><code>C:\Workspace&gt; python hello.pyplease enter your name: Michaelhello, Michael</code></pre><p>每次运行该程序，根据用户输入的不同，输出结果也会不同。</p><p>在命令行下，输入和输出就是这么简单。</p><h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><p>任何计算机程序都是为了执行一个特定的任务，有了输入，用户才能告诉计算机程序所需的信息，有了输出，程序运行后才能告诉用户任务的结果。</p><p>输入是Input，输出是Output，因此，我们把输入输出统称为Input/Output，或者简写为IO。</p><p>input()和print()是在命令行下面最基本的输入和输出，但是，用户也可以通过其他更高级的图形界面完成输入和输出，比如，在网页上的一个文本框输入自己的名字，点击“确定”后在网页上看到输出信息。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;命令行模式&quot;&gt;&lt;a href=&quot;#命令行模式&quot; class=&quot;headerlink&quot; title=&quot;命令行模式&quot;&gt;&lt;/a&gt;命令行模式&lt;/h3&gt;&lt;p&gt;在Windows开始菜单选择“命令提示符”，就进入到命令行模式，它的提示符类似C:&gt;：&lt;/p&gt;
&lt;h3 id=&quot;Py
      
    
    </summary>
    
      <category term="编程相关" scheme="https://paradoxallen.github.io/categories/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="python" scheme="https://paradoxallen.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Python安装编译和运行</title>
    <link href="https://paradoxallen.github.io/25029/"/>
    <id>https://paradoxallen.github.io/25029/</id>
    <published>2017-10-02T16:00:00.000Z</published>
    <updated>2018-06-25T06:32:47.469Z</updated>
    
    <content type="html"><![CDATA[<p>因为Python是跨平台的，它可以运行在Windows、Mac和各种Linux/Unix系统上。在Windows上写Python程序，放到Linux上也是能够运行的。</p><p>要开始学习Python编程，首先就得把Python安装到你的电脑里。安装后，你会得到Python解释器（就是负责运行Python程序的），一个命令行交互环境，还有一个简单的集成开发环境。</p><h3 id="安装Python-3-6"><a href="#安装Python-3-6" class="headerlink" title="安装Python 3.6"></a>安装Python 3.6</h3><p>目前，Python有两个版本，一个是2.x版，一个是3.x版，这两个版本是不兼容的。由于3.x版越来越普及，以最新的Python 3.6版本为基础。请确保你的电脑上安装的Python版本是最新的3.6.x。</p><h3 id="在Mac上安装Python"><a href="#在Mac上安装Python" class="headerlink" title="在Mac上安装Python"></a>在Mac上安装Python</h3><p>如果你正在使用Mac，系统是OS X 10.8~10.10，那么系统自带的Python版本是2.7。要安装最新的Python 3.6，有两个方法：</p><p>方法一：从Python官网下载Python 3.6的安装程序，双击运行并安装；</p><p>方法二：如果安装了Homebrew，直接通过命令<code>brew install python3</code>安装即可。</p><h3 id="在Linux上安装Python"><a href="#在Linux上安装Python" class="headerlink" title="在Linux上安装Python"></a>在Linux上安装Python</h3><p>如果你正在使用Linux，那我可以假定你有Linux系统管理经验，自行安装Python 3应该没有问题，否则，请换回Windows系统。</p><p>对于大量的目前仍在使用Windows的同学，如果短期内没有打算换Mac，就可以继续阅读以下内容。</p><h3 id="在Windows上安装Python"><a href="#在Windows上安装Python" class="headerlink" title="在Windows上安装Python"></a>在Windows上安装Python</h3><p>首先，根据你的Windows版本（64位还是32位）从Python的官方网站下载Python 3.6对应的64位安装程序或32位安装程序，然后，运行下载的EXE安装包：</p><p><img src="https://i.imgur.com/uedLZTE.png" alt=""></p><p>特别要注意勾上<code>Add Python 3.6 to PATH</code>，然后点“Install Now”即可完成安装。</p><h3 id="运行Python"><a href="#运行Python" class="headerlink" title="运行Python"></a>运行Python</h3><p>安装成功后，打开命令提示符窗口，敲入python后，会出现两种情况：</p><p><strong>情况一：</strong></p><p><img src="https://i.imgur.com/vBSk5dP.png" alt=""></p><p>看到上面的画面，就说明Python安装成功！</p><p>你看到提示符<code>&gt;&gt;&gt;</code>就表示我们已经在Python交互式环境中了，可以输入任何Python代码，回车后会立刻得到执行结果。现在，输入<code>exit()</code>并回车，就可以退出Python交互式环境（直接关掉命令行窗口也可以）。</p><p>情况二：得到一个错误：</p><pre><code>‘python’ 不是内部或外部命令，也不是可运行的程序或批处理文件。</code></pre><p><img src="https://i.imgur.com/nwJZ2pJ.png" alt=""></p><p>这是因为Windows会根据一个<code>Path</code>的环境变量设定的路径去查找<code>python.exe</code>，如果没找到，就会报错。如果在安装时漏掉了勾选Add <code>Python 3.6 to PATH</code>，那就要手动把<code>python.exe</code>所在的路径添加到Path中。</p><p>如果你不知道怎么修改环境变量，建议把Python安装程序重新运行一遍，务必记得勾上<code>Add Python 3.6 to PATH</code>。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>学会如何把Python安装到计算机中，并且熟练打开和退出Python交互式环境。</p><p>在Windows上运行Python时，请先启动命令行，然后运行<code>python</code>。</p><p>在Mac和Linux上运行Python时，请打开终端，然后运行<code>python3</code>。</p><p>当我们编写Python代码时，我们得到的是一个包含Python代码的以<code>.py</code>为扩展名的文本文件。要运行代码，就需要Python解释器去执行<code>.py</code>文件。</p><p>由于整个Python语言从规范到解释器都是开源的，所以理论上，只要水平够高，任何人都可以编写Python解释器来执行Python代码（当然难度很大）。事实上，确实存在多种Python解释器。</p><h4 id="CPython"><a href="#CPython" class="headerlink" title="CPython"></a>CPython</h4><p>当我们从Python官方网站下载并安装好Python 3.x后，我们就直接获得了一个官方版本的解释器：CPython。这个解释器是用C语言开发的，所以叫CPython。在命令行下运行python就是启动CPython解释器。</p><p>CPython是使用最广的Python解释器。</p><h4 id="IPython"><a href="#IPython" class="headerlink" title="IPython"></a>IPython</h4><p>IPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强，但是执行Python代码的功能和CPython是完全一样的。好比很多国产浏览器虽然外观不同，但内核其实都是调用了IE。</p><p>CPython用<code>&gt;&gt;&gt;</code>作为提示符，而IPython用<code>In [序号]:</code>作为提示符。</p><h4 id="PyPy"><a href="#PyPy" class="headerlink" title="PyPy"></a>PyPy</h4><p>PyPy是另一个Python解释器，它的目标是执行速度。PyPy采用JIT技术，对Python代码进行动态编译（注意不是解释），所以可以显著提高Python代码的执行速度。</p><p>绝大部分Python代码都可以在PyPy下运行，但是PyPy和CPython有一些是不同的，这就导致相同的Python代码在两种解释器下执行可能会有不同的结果。如果你的代码要放到PyPy下执行，就需要了解PyPy和CPython的不同点。</p><h4 id="Jython"><a href="#Jython" class="headerlink" title="Jython"></a>Jython</h4><p>Jython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行。</p><h4 id="IronPython"><a href="#IronPython" class="headerlink" title="IronPython"></a>IronPython</h4><p>IronPython和Jython类似，只不过IronPython是运行在微软.Net平台上的Python解释器，可以直接把Python代码编译成.Net的字节码。</p><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>Python的解释器很多，但使用最广泛的还是CPython。如果要和Java或.Net平台交互，最好的办法不是用Jython或IronPython，而是通过网络调用来交互，确保各程序之间的独立性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;因为Python是跨平台的，它可以运行在Windows、Mac和各种Linux/Unix系统上。在Windows上写Python程序，放到Linux上也是能够运行的。&lt;/p&gt;
&lt;p&gt;要开始学习Python编程，首先就得把Python安装到你的电脑里。安装后，你会得到Pyth
      
    
    </summary>
    
      <category term="编程相关" scheme="https://paradoxallen.github.io/categories/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="python" scheme="https://paradoxallen.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Python前言与目录</title>
    <link href="https://paradoxallen.github.io/45/"/>
    <id>https://paradoxallen.github.io/45/</id>
    <published>2017-09-30T16:00:00.000Z</published>
    <updated>2018-06-25T06:32:47.465Z</updated>
    
    <content type="html"><![CDATA[<p>Python是一种计算机程序设计语言。你可能已经听说过很多种流行的编程语言，比如非常难学的C语言，非常流行的Java语言，适合初学者的Basic语言，适合网页编程的JavaScript语言等等。</p><p>那Python是一种什么语言？</p><p>首先，我们普及一下编程语言的基础知识。用任何编程语言来开发程序，都是为了让计算机干活，比如下载一个MP3，编写一个文档等等，而计算机干活的CPU只认识机器指令，所以，尽管不同的编程语言差异极大，最后都得“翻译”成CPU可以执行的机器指令。而不同的编程语言，干同一个活，编写的代码量，差距也很大。</p><p>比如，完成同一个任务，C语言要写1000行代码，Java只需要写100行，而Python可能只要20行。</p><p>所以Python是一种相当高级的语言。</p><p>你也许会问，代码少还不好？代码少的代价是运行速度慢，C程序运行1秒钟，Java程序可能需要2秒，而Python程序可能就需要10秒。</p><p>那是不是越低级的程序越难学，越高级的程序越简单？表面上来说，是的，但是，在非常高的抽象计算中，高级的Python程序设计也是非常难学的，所以，高级程序语言不等于简单。</p><p>但是，对于初学者和完成普通任务，Python语言是非常简单易用的。连Google都在大规模使用Python，你就不用担心学了会没用。</p><p>用Python可以做什么？可以做日常任务，比如自动备份你的MP3；可以做网站，很多著名的网站包括YouTube就是Python写的；可以做网络游戏的后台，很多在线游戏的后台都是Python开发的。总之就是能干很多很多事啦。</p><p>Python当然也有不能干的事情，比如写操作系统，这个只能用C语言写；写手机应用，只能用Swift/Objective-C（针对iPhone）和Java（针对Android）；写3D游戏，最好用C或C++。</p><p>Python是著名的“龟叔”Guido van Rossum在1989年圣诞节期间，为了打发无聊的圣诞节而编写的一个编程语言。</p><p>现在，全世界差不多有600多种编程语言，但流行的编程语言也就那么20来种。如果你听说过TIOBE排行榜，你就能知道编程语言的大致流行程度。这是最近10年最常用的10种编程语言的变化图：</p><p><img src="https://i.imgur.com/Y2lJyTv.png" alt=""></p><p>总的来说，这几种编程语言各有千秋。C语言是可以用来编写操作系统的贴近硬件的语言，所以，C语言适合开发那些追求运行速度、充分发挥硬件性能的程序。而Python是用来编写应用程序的高级编程语言。</p><p>当你用一种语言开始作真正的软件开发时，你除了编写代码外，还需要很多基本的已经写好的现成的东西，来帮助你加快开发进度。比如说，要编写一个电子邮件客户端，如果先从最底层开始编写网络协议相关的代码，那估计一年半载也开发不出来。高级编程语言通常都会提供一个比较完善的基础代码库，让你能直接调用，比如，针对电子邮件协议的SMTP库，针对桌面环境的GUI库，在这些已有的代码库的基础上开发，一个电子邮件客户端几天就能开发出来。</p><p>Python就为我们提供了非常完善的基础代码库，覆盖了网络、文件、GUI、数据库、文本等大量内容，被形象地称作“内置电池（batteries included）”。用Python开发，许多功能不必从零编写，直接使用现成的即可。</p><p>除了内置的库外，Python还有大量的第三方库，也就是别人开发的，供你直接使用的东西。当然，如果你开发的代码通过很好的封装，也可以作为第三方库给别人使用。</p><p>许多大型网站就是用Python开发的，例如YouTube、Instagram，还有国内的豆瓣。很多大公司，包括Google、Yahoo等，甚至NASA（美国航空航天局）都大量地使用Python。</p><p>龟叔给Python的定位是“优雅”、“明确”、“简单”，所以Python程序看上去总是简单易懂，初学者学Python，不但入门容易，而且将来深入下去，可以编写那些非常非常复杂的程序。</p><p>总的来说，Python的哲学就是简单优雅，尽量写容易看明白的代码，尽量写少的代码。如果一个资深程序员向你炫耀他写的晦涩难懂、动不动就几万行的代码，你可以尽情地嘲笑他。</p><p><strong>那Python适合开发哪些类型的应用呢？</strong></p><p>首选是网络应用，包括网站、后台服务等等；</p><p>其次是许多日常需要的小工具，包括系统管理员需要的脚本任务等等；</p><p>另外就是把其他语言开发的程序再包装起来，方便使用。</p><p><strong>最后说说Python的缺点。</strong></p><p>任何编程语言都有缺点，Python也不例外。优点说过了，那Python有哪些缺点呢？</p><p>第一个缺点就是运行速度慢，和C程序相比非常慢，因为Python是解释型语言，你的代码在执行时会一行一行地翻译成CPU能理解的机器码，这个翻译过程非常耗时，所以很慢。而C程序是运行前直接编译成CPU能执行的机器码，所以非常快。</p><p>但是大量的应用程序不需要这么快的运行速度，因为用户根本感觉不出来。例如开发一个下载MP3的网络应用程序，C程序的运行时间需要0.001秒，而Python程序的运行时间需要0.1秒，慢了100倍，但由于网络更慢，需要等待1秒，你想，用户能感觉到1.001秒和1.1秒的区别吗？这就好比F1赛车和普通的出租车在北京三环路上行驶的道理一样，虽然F1赛车理论时速高达400公里，但由于三环路堵车的时速只有20公里，因此，作为乘客，你感觉的时速永远是20公里。</p><p><img src="https://i.imgur.com/sSnnJo1.jpg" alt=""></p><p>第二个缺点就是代码不能加密。如果要发布你的Python程序，实际上就是发布源代码，这一点跟C语言不同，C语言不用发布源代码，只需要把编译后的机器码（也就是你在Windows上常见的xxx.exe文件）发布出去。要从机器码反推出C代码是不可能的，所以，凡是编译型的语言，都没有这个问题，而解释型的语言，则必须把源码发布出去。</p><p>这个缺点仅限于你要编写的软件需要卖给别人挣钱的时候。好消息是目前的互联网时代，靠卖软件授权的商业模式越来越少了，靠网站和移动应用卖服务的模式越来越多了，后一种模式不需要把源码给别人。</p><p>再说了，现在如火如荼的开源运动和互联网自由开放的精神是一致的，互联网上有无数非常优秀的像Linux一样的开源代码，我们千万不要高估自己写的代码真的有非常大的“商业价值”。那些大公司的代码不愿意开放的更重要的原因是代码写得太烂了，一旦开源，就没人敢用他们的产品了。</p><p><img src="https://i.imgur.com/NcfLHCr.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Python是一种计算机程序设计语言。你可能已经听说过很多种流行的编程语言，比如非常难学的C语言，非常流行的Java语言，适合初学者的Basic语言，适合网页编程的JavaScript语言等等。&lt;/p&gt;
&lt;p&gt;那Python是一种什么语言？&lt;/p&gt;
&lt;p&gt;首先，我们普及一下
      
    
    </summary>
    
      <category term="编程相关" scheme="https://paradoxallen.github.io/categories/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="python" scheme="https://paradoxallen.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Python中的按位运算</title>
    <link href="https://paradoxallen.github.io/1340/"/>
    <id>https://paradoxallen.github.io/1340/</id>
    <published>2017-09-29T16:00:00.000Z</published>
    <updated>2018-06-24T06:32:35.344Z</updated>
    
    <content type="html"><![CDATA[<p>位操作是程序设计中对位模式或二进制数的一元和二元操作. 在许多古老的微处理器上, 位运算比加减运算略快, 通常位运算比乘除法运算要快很多. 在现代架构中, 情况并非如此:位运算的运算速度通常与加法运算相同(仍然快于乘法运算).</p><p>简单来说，<strong>按位运算就把数字转换为机器语言——二进制的数字来运算的一种运算形式</strong>。在计算机系统中，数值一律用补码来表示(存储)。</p><p>Python中的按位运算符有：<strong>左移运算符（&lt;&lt;），右移运算符（&gt;&gt;）,按位与（&amp;），按位或（|），按位翻转（～）</strong>。这些运算符中只有按位翻转运算符是单目运算符，其他的都是双目运算符。</p><h4 id="按位与-amp"><a href="#按位与-amp" class="headerlink" title="按位与    &amp;"></a>按位与    &amp;</h4><p><strong>举例：</strong><br><strong>3&amp;5</strong><br>解法：3的二进制补码是 11,  5的是101, 3&amp;5也就是011&amp;101,先看百位(其实不是百位,这样做只是便于理解) 一个0一个1,根据(1&amp;1=1，1&amp;0=0，0&amp;0=0，0&amp;1=0)可知百位应该是1,同样十位上的数字1&amp;0=0,个位上的数字1&amp;1=1,因此最后的结果是1.(这之后本来应该还有一步,因为我们现在得到的数值只是所求答案的补码,但是因为正数的补码即是它本身,所以就省略了。不过,下面的例子就不能省略最后这一步了).    </p><p><strong>-1&amp;-2</strong><br>解法:-1的补码是11111111,  -2的补码是11111110, 11111111&amp;11111110得到的结果是:11111110,这个是补码,再转化位原码为100000010 (负数转换位原码的方法是减一取反),最后转换为十进制是 -2.</p><p><strong>-2&amp;6</strong><br>解法:-2的补码是11111110,  6的补码是110,   11111110&amp;110,也就是11111110&amp;00000110(这样写的目的是让初学者能够更好理解按位运算),按照上面的方法得到的结果是:110,转化位十进制就是6.</p><p><strong>小技巧</strong>：利用按位与可以将任意二进制数的最后一位变为0,即就是X&amp;0.</p><h4 id="按位并"><a href="#按位并" class="headerlink" title="按位并    |"></a>按位并    |</h4><p><strong>举例： </strong></p><p><strong>4|7</strong><br>解法：按位并的计算规律和按位与的很相似，只不过换了逻辑运算符，并的规律是： 1|1=1 ,1 |0=1, 0|0=0.   4|7转换位二进制就是:100|111=111.  二进制111即为十进制的7.<br><strong>小技巧</strong>：利用按位并可以将任意二进制数的最后一位变为1,即就是X|1.</p><h4 id="按位异或"><a href="#按位异或" class="headerlink" title="按位异或    ^"></a>按位异或    ^</h4><p><strong>方法</strong>:  对位相加,<strong>特别要注意的是不进位. </strong></p><p><strong>举例：</strong> </p><p><strong>2^5</strong><br>解法:10^101=111,二进制111得到十进制的结果是7.</p><p><strong>1^1</strong><br>解法:1+1=0.(本来二进制1+1=10,但不能进位,所以结果是0) </p><p><strong>-3^4  </strong><br>解法: -3的补码是11111101,4的补码是100 (也即00000100),11111101^00000100=11111101,补码                               11111101转为原码是1000111,即十进制的-7.</p><h4 id="按位翻转"><a href="#按位翻转" class="headerlink" title="按位翻转  ~"></a>按位翻转  ~</h4><p><strong>方法:</strong>   将二进制数+1之后乘以-1,x的按位翻转是-(x+1) . 注意,按位运算符是单目运算符.  -9 ,  1+~4是正确的,5~3就不对了.</p><p><strong>举例:</strong><br><strong>~3</strong><br>解法:3的二进制是11, -(11+1)=-100B=-4D. (注:B和D分别表示二进制和十进制).<br><strong>~-2</strong><br>解法:   -  (-10+1)  =1</p><h4 id="左移运算符-lt-lt"><a href="#左移运算符-lt-lt" class="headerlink" title="左移运算符  &lt;&lt;"></a>左移运算符  &lt;&lt;</h4><p><strong>方法: </strong>   X&lt;&lt;N 将一个数字X所对应的二进制数向左移动N位.</p><p><strong>举例:</strong></p><p><strong>3&lt;&lt;2</strong><br>解法:11向左移动两位变为1100,即12 .</p><h4 id="右移动运算符-gt-gt"><a href="#右移动运算符-gt-gt" class="headerlink" title="右移动运算符  &gt;&gt;"></a>右移动运算符  &gt;&gt;</h4><p><strong>方法:</strong>    X&gt;&gt;N 将一个数字X所对应的二进制数向右移动N位.</p><p><strong>举例: </strong></p><p><strong>3&gt;&gt;2</strong><br>解法:11向右移动两位变为0.</p><p><strong>10&gt;&gt;1</strong><br>解法:10的二进制是1010,向右边移动一位是101,即5.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;位操作是程序设计中对位模式或二进制数的一元和二元操作. 在许多古老的微处理器上, 位运算比加减运算略快, 通常位运算比乘除法运算要快很多. 在现代架构中, 情况并非如此:位运算的运算速度通常与加法运算相同(仍然快于乘法运算).&lt;/p&gt;
&lt;p&gt;简单来说，&lt;strong&gt;按位运
      
    
    </summary>
    
      <category term="编程相关" scheme="https://paradoxallen.github.io/categories/%E7%BC%96%E7%A8%8B%E7%9B%B8%E5%85%B3/"/>
    
    
      <category term="python" scheme="https://paradoxallen.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (11) Large Scale Machine Learning</title>
    <link href="https://paradoxallen.github.io/6245/"/>
    <id>https://paradoxallen.github.io/6245/</id>
    <published>2017-09-28T16:00:00.000Z</published>
    <updated>2018-06-21T16:14:54.319Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Big-Data"><a href="#1-Big-Data" class="headerlink" title="1. Big Data"></a>1. Big Data</h3><p>海量的数据作为训练样本，<strong>“low bias algo + big data”</strong>基本都能让算法精确度更高。但是随之而来的就是极大的计算量。</p><p>正如<a href="https://paradoxallen.github.io/9059/">《Machine Learning Advice》</a>中所提到的，如果算法是 high bias，那么1000个样本与100,000,000个样本效果都不会有变化</p><p>那如何避免一开始就陷入了 high bias 或者因为自己的选择的方法、编程的细节错误所浪费大量时间呢？</p><p>我们应该在海量数据中<strong>选取一部分</strong>尝试。例如，随机选择 100,000,000个样本中的1000个，使用各种学习方法选择在这1000个采样学习中表现最好的一个或几个方法，进行海量数据的学习</p><p>尝试之后，我们即将进行真正的海量数据学习。学习过程中，我们经常会用到梯度下降法。我们以线性回归为例（<strong>任何使用到梯度下降法的算法：逻辑回归、神经网络等，都可以举一反三适用</strong>），参数θ的update函数为：<br><img src="https://i.imgur.com/2PwCkzy.png" alt=""></p><p>公式推导内容见<a href="https://paradoxallen.github.io/58080/">《Linear Regression》</a></p><p>上式中有一处求和<img src="https://i.imgur.com/4HlhA0F.png" alt="">，面对海量数据时会消耗大量时间，而且对每一个样本都会计算一次。这种计算模式称为<strong>“批处理”(batch)</strong>，面对海量数据，显然批处理模式并不适合。以下介绍两种海量数据模型中经常使用的方法</p><h3 id="2-随机梯度下降（Stochastic-Gradient-Descent）"><a href="#2-随机梯度下降（Stochastic-Gradient-Descent）" class="headerlink" title="2. 随机梯度下降（Stochastic Gradient Descent）"></a>2. 随机梯度下降（Stochastic Gradient Descent）</h3><p>介绍随机梯度下降方法之前，我们引入一个新的表达式：<img src="https://i.imgur.com/xdwKU3A.png" alt=""> 这代替了原本的 J(θ) 成为新的代价函数</p><p>根据新的代价函数，我们会得到新的参数θ的update函数：<br><img src="https://i.imgur.com/0qISHJa.png" alt=""></p><p>仔细观察这个update函数，是不是发现求和过程没有了？<br>批处理中，参数的改变需要当前所有样本共同作用，达到整体最优。但是随机方法中，<strong>每次仅关注当前单个样本的最优作用。这种做法会导致最优值的反复，但是却极大提高了效率</strong></p><p>随机梯度下降的具体流程为：<br><img src="https://i.imgur.com/ASzT2Zw.png" alt=""></p><p>因为随机梯度下降会导致最优值的反复，所以有时上图中的 Repeat 会进行1~10次。而且随机梯度下降方法的收敛是随机的，只能保证逐渐向最优值徘徊，而不会真正到达最优值。例如下图：<br><img src="https://i.imgur.com/QNkfNRs.png" alt=""></p><p>如果我们希望达到最优值，应该怎么办呢？可以通过逐渐缩小学习步长α，以求最终的收敛范围逐渐减小到最优值。可以使用：<br><img src="https://i.imgur.com/pGRoyMs.png" alt="">来逐渐缩小学习步长</p><h3 id="3-小规模批处理（Mini-batch）"><a href="#3-小规模批处理（Mini-batch）" class="headerlink" title="3. 小规模批处理（Mini-batch）"></a>3. 小规模批处理（Mini-batch）</h3><p>有没有方法兼顾批处理方法，与随机梯度下降方法的优势？就是Mini-batch Gradient Descent。基本思想是：不是批处理的每次处理所有样本，也不是随机方法每次单个样本，而是每次处理b个样本（1&lt;b≪m）</p><p>假设有1000个样本，b=10。算法的具体执行过程为：<br><img src="https://i.imgur.com/sNBITho.png" alt=""></p><p>实际计算中，小规模批处理方法，可能会比随机梯度下降方法更快！因为<strong>不同组的b个样本可以进行并行计算</strong>（一次并行m个样本计算量太大）。但这种方法多引入了一个参数b，需要更多的调试，算是个缺点</p><h3 id="4-Big-Data-判断收敛"><a href="#4-Big-Data-判断收敛" class="headerlink" title="4. Big Data 判断收敛"></a>4. Big Data 判断收敛</h3><p>批处理方法中，如何判断算法是否收敛？绘制 “Jtrain(θ)−#iterations” 的走势图（见<a href="https://paradoxallen.github.io/58080/">《Linear Regression》</a>，Jtrain(θ)是基于所有样本的和。走势逐渐降低即为收敛</p><p>随机梯度下降方法中，再去计算所有样本的Jtrain(θ)显然不合适，我们转而记录<img src="https://i.imgur.com/BgFkQHn.png" alt="">。例如，每1000个样本各自的<img src="https://i.imgur.com/BgFkQHn.png" alt="">求和后平均，记录这个<strong>平均误差</strong>后再更新θ。</p><p>最后，绘制“平均误差”与迭代次数的关系图。因为收敛是随机的，可能出现噪声，只要<strong>保证整体趋势是逐渐下降的即为收敛</strong></p><p>接下来我们看一下以下几种“平均误差”与迭代次数的关系图，看看有什么问题：<br><img src="https://i.imgur.com/CmhZn1V.png" alt=""></p><p>左上角的图是一种收敛的情况，但是红色曲线的α更小，虽然学习速度较慢，但是却收敛到更接近最优值的区域（更小误差）</p><p>右上角的图是一种收敛的情况，其中红色曲线的取“平均误差”的范围更大，是5000个样本取一次，所以噪声更少更平滑</p><p>左下角的图，因为蓝色曲线噪声过大，看不出是否收敛，建议增大取“平均误差”的范围</p><p>右下角的图，抱歉，完全不收敛。建议使用更小的α再尝试，如果还是不收敛就可能需要选择其他的算法或者增加特征？</p><h3 id="5-Online-Learning"><a href="#5-Online-Learning" class="headerlink" title="5. Online Learning"></a>5. Online Learning</h3><p>试想一种场景：一家新闻网站不断根据不断涌入的新用户的喜好以及老用户的兴趣改变，来定义头条新闻的内容，是不是还要保存这种海量数据呢？</p><p>这需要极大的空间，并且每次都基于所有样本进行学习，那么学习的时间还赶不上产生新样本的时间</p><p>如果使用随机梯度下降方法，每次仅仅关注当前样本的梯度下降，并且<strong>同一个样本仅学习一次</strong>（上文“随机梯度下降”流程图中 Repeat 次数为1），是不是就能解决这个问题。这就是online learning</p><p>online learning 思想可以<strong>适用于无限的数据流</strong>，尤其适用于不断更新中的互联网大数据上的机器学习算法。</p><p>并且，这种方法对于新事物有着极强的适应性，因为每次更新都是基于当前样本，历史样本的作用随着迭代的进行会逐渐失去。因此非常适用于<strong>互联网大数据的实时更新</strong></p><h3 id="6-Data-Parallelism"><a href="#6-Data-Parallelism" class="headerlink" title="6. Data Parallelism"></a>6. Data Parallelism</h3><p>最后一部分是对于并行计算的简介。试想一下，如果我要求4亿个样本某个特征的和，仅在一台机器上求是很慢的。但是，如果我将它分散在四台机器上求解，最后使用一台机器汇总这四台机器的求解结果，是不是快了很多？除去数据传输耗费的时间，是不是快了接近4倍？其中，<strong>分散</strong>的过程我们称为<strong>Map</strong>，而<strong>汇总</strong>的过程我们称为<strong>Reduce</strong></p><p>不仅对于多台机器，同一台机器如果有着多个计算核（CPU），同样可以适用。形象化的示意图如下：<br><img src="https://i.imgur.com/vsnpNZi.png" alt=""><br><img src="https://i.imgur.com/TsFW2LW.png" alt=""></p><p>本节仅仅举了加法求和的例子。但实际情况中，只要是无数据相关性的计算，也就是前一部分的计算结果不作为后一部分的计算输入，两个部分的计算是彼此独立的，都可以并行计算</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (10) Recommender Systems</title>
    <link href="https://paradoxallen.github.io/62349/"/>
    <id>https://paradoxallen.github.io/62349/</id>
    <published>2017-09-24T16:00:00.000Z</published>
    <updated>2018-06-20T16:46:31.977Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-推荐系统"><a href="#1-推荐系统" class="headerlink" title="1. 推荐系统"></a>1. 推荐系统</h3><p><strong>推荐系统</strong>是机器学习的重要应用之一，但是学术界关注较少。但是它的确出现在生活中的方方面面。例如我们上豆瓣、网易云音乐（没错，叫我文青）等，总会有根据你的个人喜好来推荐你可能喜欢的电影、音乐的功能，就是典型的推荐系统。</p><p>我们以外国豆瓣为例，见下图（问号代表尚未评分）：<br><img src="https://i.imgur.com/to4tI0H.png" alt=""></p><p>另外我们需要设计几个变量：</p><p>nu=#users数目；</p><p>nu=#movies数目；</p><p>r(i,j)代表用户j是否已经对电影i评分，取值为0或1；</p><p>y(i,j)代表用户j是否对电影i的具体评分，取值范围0~5</p><p>一般电影评分是<strong>基于内容（content-based）</strong>，取值范围为0~1。例如某部电影的浪漫成分为0.3，动作成分为0.8，这部电影就主要是动作片，掺杂一些浪漫成分。“浪漫”特征可以设定为x1，“动作”特征可以设定为x2，共有n个特征。</p><p>如何寻找相似的电影呢？就是选取“距离”最短的另一部电影，即<img src="https://i.imgur.com/OirBtIk.png" alt=""></p><p>很容易地，我们会想到线性回归的方法。这个方法的前提是<strong>已知每部电影属于每种特征的得分</strong>即x(1),x(2),…,x(m)。</p><p>若每个用户基于特征可以有一个θ(j)，则用户j对电影i的评分为<img src="https://i.imgur.com/6FNr4VE.png" alt="">。</p><p>根据线性回归，我们可以得到优化目标为：<br><img src="https://i.imgur.com/sQY0vHd.png" alt=""></p><p>这里需要注意的是，为了简化系统，我们去除了求和符号之前的1/2m，而是1/2。这样并不影响结果。 </p><p>对上图中的优化目标，利用梯度下降的办法求出 <strong>θ 的最优值</strong>（如果对这一部分不熟悉，建议查看<a href="https://paradoxallen.github.io/58080/">《Linear Regression》</a><br><img src="https://i.imgur.com/NzShBbD.png" alt=""></p><h3 id="2-换个角度思考？"><a href="#2-换个角度思考？" class="headerlink" title="2. 换个角度思考？"></a>2. 换个角度思考？</h3><p>上一部分，我们是假设已知每部电影在每种特征上的得分。如果我们已知有两种特征：<strong>romance和action</strong>，但是不知道每种电影在哪种特征上得分多少。在这里，我们的求解对象变为了x(i).</p><p>但是此时我们已知得知用户对于每种特征的喜欢程度，即已知θ(j)。同时已知y(i,j)，求解x(i)。因此我们的优化目标为：<br><img src="https://i.imgur.com/bczIp7x.png" alt=""></p><p>同理，也可以使用梯度下降法进行最优值求解。</p><p>上一部分的求解过程是<strong>x⇒θ</strong>，这一部分的求解过程是<strong>θ⇒x</strong>。</p><p>如果这两个过程交替进行，是不是就可以达到整体的最优值？答案是肯定的。</p><p>这就是下一部分<strong>“协同过滤”</strong>的基本思想，但是具体实施过程有待改进。</p><h3 id="3-协同过滤（Collaborative-Filtering）"><a href="#3-协同过滤（Collaborative-Filtering）" class="headerlink" title="3. 协同过滤（Collaborative Filtering）"></a>3. 协同过滤（Collaborative Filtering）</h3><p>对于求解过程x⇒θ，与求解过程θ⇒x 交替进行，以求达到整体的最优值。协同过滤基本思想是这样的，但是能不能将一切置于同一个公式下？</p><p>查看下图，红色方框中是两个优化目标的求解部分中相同的部分，即使求和顺序不同。而红框之外的部分分别加到新的优化目标中<br><img src="https://i.imgur.com/MQKWsqs.png" alt=""></p><p>这里，我们去除掉x0,θ0这些恒为1的变量（其实我忍很久了），让学习更加灵活。</p><p>因此，协同过滤的具体算法流程（第一步类似于神经网络，对系统中的参数首先都进行随机化；后两步完全类似于线性回归）。因为我们不再需要k=0的情况，所以不需要单独区分：<br><img src="https://i.imgur.com/c1gFkK7.png" alt=""></p><h3 id="4-均值归一化（Mean-Normalization）"><a href="#4-均值归一化（Mean-Normalization）" class="headerlink" title="4. 均值归一化（Mean Normalization）"></a>4. 均值归一化（Mean Normalization）</h3><p>如果有另一个新用户 Eve，她对于任何电影都尚未评分，我们如何给她推荐呢？很简单，推荐那些大部分都觉得高分的电影呗。</p><p>但是，如果直接利用我们之前的方法求解以下情形：<br><img src="https://i.imgur.com/te6xHCZ.png" alt=""></p><p>为了归一化，肯定最后 Eve 的 θ(5) 一定是全0向量。因此，我们需要对原先的方法采取一些<strong>预操作</strong>，来避免出现全 0 的预测向量，而是让它变成每部电影的平均得分。</p><p>方法也十分简单，就是对每部电影求出一个平均值（只统计那些对该部电影打过分数的样本），然后将每个人对每部电影的得分减去这部电影的平均值，然后进行<strong>协同过滤计算</strong>。但是，最后的预测得分公式也需要小小的改变：<img src="https://i.imgur.com/QfYFAWe.png" alt="">（其中μi表示电影i的平均得分）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (9) Anomaly Detection</title>
    <link href="https://paradoxallen.github.io/39500/"/>
    <id>https://paradoxallen.github.io/39500/</id>
    <published>2017-09-18T16:00:00.000Z</published>
    <updated>2018-06-19T17:00:26.256Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-异常检测-amp-高斯分布"><a href="#1-异常检测-amp-高斯分布" class="headerlink" title="1. 异常检测 &amp; 高斯分布"></a>1. 异常检测 &amp; 高斯分布</h3><p><strong>异常检测是一种介于监督学习与非监督学习之间的机器学习方式</strong>。一般用于检查大规模正品中的<strong>小规模</strong>次品。</p><p>根据单个特征量的概率分布，从而求出某个样本正常的概率，若正常的概率小于阈值，即 p(x)&lt;ϵ 视其为异常（次品）。正品与次品的 label 值 y 定义为：<br><img src="https://i.imgur.com/o9Rt4yM.png" alt=""></p><p>如果某个样本由x1,x2两个变量决定，如下图红色叉所示：<br><img src="https://i.imgur.com/ylEk8oe.png" alt=""></p><p>同一个圆圈内部，表示的是成为正品的概率相同。越中心的圆圈内部正品率越高。越外层的圆圈内正品率越低。</p><p>异常检测一般<strong>将每个特征量的分布假设为正态分布</strong>（如果特征量与正态分布差距很大，之后我们会提到方法对其进行修正）。</p><p>为什么是正态分布？因为在生产与科学实验中发现，很多随机变量的概率分布都可以近似地用正态分布来描述（猜测正确的概率更大）。因</p><p>此，以下稍微介绍一下正态分布的基础知识，如果很熟悉的同学可以略过这部分。</p><p><strong>正态分布（高斯分布）</strong>，包含两个参数：均值μ（分布函数取峰值时所对应横坐标轴的值），与方差σ^2（标准差为σ，控制分布函数的“胖瘦”）。</p><p>如果变量 x 满足于正态分布，将其记为 x∼N(μ,σ^2)。而取某个 x 的对应正品概率为：<img src="https://i.imgur.com/RhxicSW.png" alt=""></p><p>均值 <img src="https://i.imgur.com/bzn9fbc.png" alt="">，方差<img src="https://i.imgur.com/88vXqR7.png" alt=""></p><p>正态分布曲线与坐标轴之间的面积（即函数积分）恒定为 1，因此<strong>“高”曲线必然“瘦”，“矮”曲线必然“胖”</strong>：<br><img src="https://i.imgur.com/QUEyjr8.png" alt=""></p><p>由图可知，标准差σ控制着分布函数的“胖瘦”。原因是因为<strong>标准差有关的取值范围，有着固定的分布概率（积分）</strong>：<br><img src="https://i.imgur.com/GPZPvbN.png" alt=""></p><h3 id="2-异常检测算法流程："><a href="#2-异常检测算法流程：" class="headerlink" title="2. 异常检测算法流程："></a>2. 异常检测算法流程：</h3><p>我们拥有一组训练数据：x(1),x(2),…,x(m)，每个样本有着 m 个特征量 x1,x2,…,xn </p><p>将每个样本投影到不同的特征的坐标轴上，基于样本得到各个特征的概率正态分布曲线</p><p>假设各个特征的概率是独立的，因此单个样本的异常概率为<br><img src="https://i.imgur.com/B4JPDaw.png" alt=""></p><p>各个特征的均值为<img src="https://i.imgur.com/v3TxnSr.png" alt="">，方差为 <img src="https://i.imgur.com/oO4VpDF.png" alt=""></p><p>如果我们有着 10000 个正品样本，以及 20 个次品样本，我们应该这样区分<strong>训练集、交叉验证集，与测试集</strong>： </p><p><strong>训练集</strong>：6000个正品作为训练集（不包括次品样本）</p><p><strong>交叉验证集</strong>：2000 个正品样本 + 10 个次品样本。用以确定次品概率的阈值 ϵ</p><p><strong>测试集</strong>：2000 个正品样本 + 10 个次品样本。用以判断算法的检测效果</p><p>特别注意，因为使用异常检测的样本集合一般都是偏斜严重的（正品样本远远多于次品样本）。</p><p>因此，需要在<a href="https://paradoxallen.github.io/9059/">Machine Learning Advice</a>中提到的 <strong>precision/recall/F-score</strong> 来进行判断算法的检测效果。</p><h3 id="3-异常检测-VS-监督学习"><a href="#3-异常检测-VS-监督学习" class="headerlink" title="3. 异常检测 VS. 监督学习"></a>3. 异常检测 VS. 监督学习</h3><p>监督学习方法与异常检测类似，处理对象都是一堆有 label 的样本，并且目标都是预测新样本的类别。那么什么时候使用监督学习的方法？什么时候使用异常检测的方法？</p><p>大体上，区别如下：<br>样本比例：异常检测适用于<strong>正样本（y=1，即次品）个数远远小于负样本</strong>的个数的情况；监督学习适用于<strong>正负样本个数都非常多</strong>的情况</p><p>异常规律：如果<strong>正样本（y=1，即次品）有着难以预测的模式，引起正样本的原因有很多很多</strong>，适用于异常检测；但是如果<strong>正样本有着固定的规律</strong>，比如感冒（病因已被研究透彻），可以尝试基于大量的样本使用监督学习的方法建立模式进行判断</p><h3 id="4-特征选择"><a href="#4-特征选择" class="headerlink" title="4. 特征选择"></a>4. 特征选择</h3><p>绝大多数情况下，特征量符合正态分布的分布情况。但如果特征的分布极端不符合，我们只能对其进行一些处理，以产生全新的特征来适用于异常检测算法。例如：<br><img src="https://i.imgur.com/LuwgqTb.png" alt=""></p><p>此时，我们有着变换后的特征变量：<strong>xnew=log(x1)</strong></p><p>或者，一般情况下我们希望<strong>正品的 p(x) 很大，次品的 p(x) 很小</strong>。也就是说，<strong>在异常情况下某些特征应该变得极大或极小</strong>（正态分布中对于极大值或者极小值的对应概率都是极小的，所以整个样本的正品概率相乘会很容易满足 p(x)&lt;ϵ）：例如创建新变量<img src="https://i.imgur.com/qP5XSx6.png" alt="">，进一步放大了值增大或减小的程度。</p><h3 id="5-Multivariate-Gaussion"><a href="#5-Multivariate-Gaussion" class="headerlink" title="5. Multivariate Gaussion"></a>5. Multivariate Gaussion</h3><p>如果一个样本有着多种特征，那么整体的正品概率可以按照以上提到的，视每个变量为互相独立然后各自概率相乘进行求解（我们称之为 <strong>original model</strong>）。</p><p>但是，如果出现了下图这种正相关（负相关）极强的特征量，同心圆内部的正品概率必然不同，显然不合适了：<br><img src="https://i.imgur.com/PQ9GGVM.png" alt=""></p><p>我们希望原本的同心圆可以更扁，可以变换方向，例如上图的蓝色椭圆。</p><p>此时，我们可以利用协方差矩阵，构造全新的多变量正态分布公式。此时我们用到的不再是方差 σ2，而是<strong>协方差矩阵</strong><img src="https://i.imgur.com/TOlEvpS.png" alt="">。<strong>多变量正态分布的概率公式</strong>为：<br><img src="https://i.imgur.com/qyUGsLg.png" alt="">，其中|Σ|表示协方差矩阵的行列式。</p><p>协方差矩阵与均值，对概率分布图的影响如下：<br><img src="https://i.imgur.com/QQAdk48.png" alt=""><br><img src="https://i.imgur.com/edL5bda.png" alt=""></p><h3 id="6-original-model-VS-multivariate-Gaussian"><a href="#6-original-model-VS-multivariate-Gaussian" class="headerlink" title="6. original model VS. multivariate Gaussian"></a>6. original model VS. multivariate Gaussian</h3><p>如果一个样本有着多种特征，那我们究竟是应该使用 original model，还是 multivariate Gaussian？</p><p>大体上，区别如下：<br><strong>特征选择：</strong>original model 中的各个单个特征（或创造出的新特征），应该尽量满足在异常情况下产生概率极小的特性；而如果特征之间，发现了正相关或负相关的关系，应该用 multivariate Gaussian</p><p><strong>计算效率：</strong>original model 仅仅乘法，效率较高；multivariate Gaussian 需要计算协方差的逆矩阵，效率较低</p><p><strong>样本数目：</strong>original model 在训练集极小的情况下也可以计算；</p><p>multivariate Gaussian 至少需要训练集样本数目大于特征数目，否则协方差矩阵无法求逆</p><p>协方差矩阵无法求逆（奇异矩阵）的情况极少发生，但是一旦发生，可能有以下几种原因：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">特征的数量大于训练集的样本个数</span><br><span class="line">冗余的特征变量（x1≈x2 或者 x3=x4+x5 这样的高度冗余情况）</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (8) PCA</title>
    <link href="https://paradoxallen.github.io/22819/"/>
    <id>https://paradoxallen.github.io/22819/</id>
    <published>2017-09-14T16:00:00.000Z</published>
    <updated>2018-06-19T14:30:00.421Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Motivation-for-Dimensionality-Reduction"><a href="#1-Motivation-for-Dimensionality-Reduction" class="headerlink" title="1. Motivation for Dimensionality Reduction"></a>1. Motivation for Dimensionality Reduction</h3><p>为什么要数据降维？目的性一般来说有三个：</p><p><strong>（1）加速计算，要计算的维度更少；</strong></p><p><strong>（2）节省空间；</strong></p><p><strong>（3）可视化</strong></p><p>因为现在的可视化只能对于2D或者3D有较好的处理。</p><p>那么，数据为什么可以降维呢？举个例子，我用一幅图表示厘米与英尺的关系，这时特征有（厘米，英尺）。但其实我存储的时候，<strong>只需要其中任意一个，另外一个是极强相关度</strong>。此时只有一维特征。</p><h3 id="2-Principal-Component-Analysis-PCA-简介"><a href="#2-Principal-Component-Analysis-PCA-简介" class="headerlink" title="2. Principal Component Analysis (PCA) 简介"></a>2. Principal Component Analysis (PCA) 简介</h3><p><strong>主成分分析（Principal Component Analysis）</strong>是数据降维的一种极为有效的方法。</p><p>通过将高维数据投影到低维空间，以求通过<strong>牺牲极少的数据精度来换取更低维度的数据</strong>。</p><p>先来举个例子，例如下图中的5个点：<br><img src="https://i.imgur.com/bAi9DAY.png" alt=""></p><p>如果精确描述这5个点，那么需要2D的特征(x1,x2)。但是，如果我们以图中的直线作为唯一的1D特征，将每个点投影到直线上，是不是基本可以区分出这5个点？这肯定丧失了一部分精度。</p><p>注意：此时的特征正方向（1D空间的基向量），方向沿着直线的任意两方都正确，我们<strong>关心的只是直线而不是方向</strong>。</p><p>上图的例子中，PCA的任务就是找到这条直线，然后投影到直线上去。而这条直线，必须满足<strong>每个点到直线欧氏距离的平方和最小，即丧失的精度最少</strong>：<img src="https://i.imgur.com/KPVsso6.png" alt=""></p><p>既然都是找直线，那肯定有人自然而然想到了线性回归。但是两者之间有着极大不同。首先就是优化目标，见下图：<br><img src="https://i.imgur.com/lySevWa.png" alt=""></p><p>左图是线性回归所需要最小化的距离（蓝色线段和），右图是2D空间PCA所需要最小化的距离（蓝色线段和）。除此之外，线性回归是为了拟合特殊的变量y，而PCA是一种无监督学习，目的性并不明确，每个变量都会降维。</p><h3 id="3-Principal-Component-Analysis-PCA-流程"><a href="#3-Principal-Component-Analysis-PCA-流程" class="headerlink" title="3. Principal Component Analysis (PCA) 流程"></a>3. Principal Component Analysis (PCA) 流程</h3><p>述说流程之前，希望大家可以看看这篇分析（本课程缺少理论分析）：<a href="http://sebastianraschka.com/Articles/2014_kernel_pca.html" target="_blank" rel="noopener">Kernel tricks and nonlinear dimensionality reduction via RBF kernel PCA</a></p><p>老规矩，首先要 <strong>mean normalization / feature scaling</strong>。方法就跟之前课程提到的一样，对每个特征 j 求均值<img src="https://i.imgur.com/tfWfqOH.png" alt=""></p><p>然后更新特征的值<img src="https://i.imgur.com/nXB3It5.png" alt="">其中Sj是特征值的范围，可以是最大值减去最小值。</p><p>计算协方差矩阵：<img src="https://i.imgur.com/BTVkiWi.png" alt="">，协方差矩阵一般是<strong>对称正定矩阵</strong>，一定会求出特征值与特征向量。</p><p>计算特征值与特征向量，matlab中写作：<code>[U, S, V] = svd(simga)</code>. </p><p>U 是特征向量集合，<img src="https://i.imgur.com/BpkU4CG.png" alt=""></p><p>S是特征值集合<img src="https://i.imgur.com/USdq9tK.png" alt=""></p><p>U∈Rn×n，降维之后取U的前k个特征向量u(1),u(2)…u(k)作为子空间的基向量，构造<img src="https://i.imgur.com/pfb6Kwe.png" alt="">。</p><p>此时x∈Rn→z∈Rk <img src="https://i.imgur.com/auNcI4d.png" alt=""><br>其中z是k×1的列向量，x是n×1的列向量，UTreduce 是k×n的矩阵</p><p>降维之后，进行计算获得结果之后，总是需要升维到原空间中。这个时候的升维精确度已经有所下降，但已经是尽量减少损耗。具体的降维转换公式为<img src="https://i.imgur.com/Wp14DBC.png" alt=""></p><p>也就是下图中的绿色交叉点返回到2D坐标系中表示。<br><img src="https://i.imgur.com/RBzzKVQ.png" alt=""></p><h3 id="4-Principal-Component-Analysis-PCA-其他问题"><a href="#4-Principal-Component-Analysis-PCA-其他问题" class="headerlink" title="4. Principal Component Analysis (PCA) 其他问题"></a>4. Principal Component Analysis (PCA) 其他问题</h3><p>OK，知道怎么使用PCA，那么我们应该选择降维k=？我们应该有评判标准。我们大体目标是<img src="https://i.imgur.com/HuLOWKM.png" alt=""></p><p>因此我们选择标准为：<br><img src="https://i.imgur.com/YjJRuyL.png" alt="">，表示意义为“<strong>保留99%的差异性</strong>”。 </p><p>如果使用特征值来表示就是，<img src="https://i.imgur.com/DTImTvI.png" alt=""></p><p>PCA 中，无监督学习的唯一学习参数是 Ureduce。<strong>Only Training Data！</strong>仅仅是由训练集训练出来的参数。</p><p>可以用<strong>Cross-Validation data 和 test Data进行检验</strong>，但是选择主分量的时候<strong>只应用training data</strong>。</p><p>有时候 PCA 被用于解决 overfitting ，但这样做其实非常不好。PCA 降低了数据维度，但同时损失了数据原有的标记信息。而之前的课程中提到的<strong>regularization 技术</strong>才是解决过拟合的正途。</p><p>使用 PCA 之前，一定要利用原维度的数据进行计算。仅仅当计算过程出现了<strong>（1）计算过慢；（2）占用内存过多</strong>；这两个问题之后，才应该考虑 PCA。盲目的使用，是不可取的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (7) K-Means</title>
    <link href="https://paradoxallen.github.io/48762/"/>
    <id>https://paradoxallen.github.io/48762/</id>
    <published>2017-09-10T16:00:00.000Z</published>
    <updated>2018-06-18T15:56:13.732Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Unsupervised-Learning"><a href="#1-Unsupervised-Learning" class="headerlink" title="1. Unsupervised Learning"></a>1. Unsupervised Learning</h3><p>之前课程中说到的学习，都是监督学习，即有一个label，明确告诉你这个样本，属于哪个类型，或者导致的值是多少。但是，如果我碰到没有label，或者我也不知道label是怎样的情况，但是我还是想要分成若干类。这样的问题，就是一种<strong>无监督</strong>问题。</p><p><strong>聚类（clustering） </strong></p><p>聚类是一种典型的无监督学习例子，但是<strong>聚类不等同于无监督学习</strong>，密度估计同样是一个典型的无监督学习例子。回到聚类，例如有下图：<br><img src="https://i.imgur.com/RWM0hdQ.png" alt=""></p><p>每种样本（蓝色圆圈）都没有label指定类别，但是人眼一看就知道分成两类比较合适。如何让机器也知道如何分类呢？这就是聚类问题。</p><h3 id="2-K-Means-Algorithm"><a href="#2-K-Means-Algorithm" class="headerlink" title="2. K-Means Algorithm"></a>2. K-Means Algorithm</h3><p><strong>K-Means 算法是解决无监督学习的有效算法之一</strong>。K（大写）表示将样本分为K个类型。算法具体的过程通俗易懂，如下图所示：<br><img src="https://i.imgur.com/XXqO0uo.png" alt=""><br><img src="https://i.imgur.com/mTXlNHO.png" alt=""></p><p>配合上图，再作一些简单的解释：<br><img src="https://i.imgur.com/CJB9nyT.png" alt=""></p><p>K-Means 算法进行一段时间后。<strong>可能会出现某个中心点没有分配到任何一个样本点的情况，这个时候可以直接去掉这个分类变为 (K-1) 簇</strong>。</p><p>但是在某些情况我们很执着的要分为 K 簇，这种情况下需要随机确定一个新的聚类中心替代当前中心点，之后再次迭代。</p><h4 id="Optimization-Objective"><a href="#Optimization-Objective" class="headerlink" title="Optimization Objective"></a>Optimization Objective</h4><p>K-Means 算法同样有着 cost function，目标同样是为了最小化 cost。cost 记为：<img src="https://i.imgur.com/Uvbvz66.png" alt=""></p><p>分析K-Means的两个主要步骤，簇分配是以μ为常量，c为变量；移动聚类中心是以c为常量，μ为变量。通过不断修改，减小 cost。</p><p>不同于回归问题，因为学习率过大导致随着迭代次数增加竟然还会增大 cost。聚类问题中不会出现这种问题（就没有学习率），聚类中随着迭代次数增加 <strong>cost 一定是逐渐下降的</strong>。</p><h4 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h4><p>之前只是说，随机化取得K个聚类中心。具体究竟如何执行呢？ </p><p><strong>（1）在m个样本中，随机取得K个样本 (K&lt;m) </strong></p><p><strong>（2）令μ1,…,μK等同于所取到的K个样本</strong> </p><p>但是，有的时候因为初始化的随机性，会陷入局部最大值的情况。例如下图所示：<br><img src="https://i.imgur.com/xKhfaFg.png" alt=""></p><p>这样即使 K-Means 算法停止了，所取得的依然不是较好的分类方法，cost 依然十分大。怎么解决？<strong>多随机初始化几次！一般是运行50~1000次 K-Means 算法，取其中 cost 最小一次的结果</strong>。</p><h4 id="Choosing-the-numbers-of-clusters"><a href="#Choosing-the-numbers-of-clusters" class="headerlink" title="Choosing the numbers of clusters"></a>Choosing the numbers of clusters</h4><p>好吧，最后一个问题来了，究竟应该分成几类？K=？ </p><p>这个问题没有标准解，依照可视化图来观察是个不错的主意。有一种特殊情况，如果K−J(cost)图如下图左边所示，圆圈所指位置为<strong>“肘区”</strong>，一般我们取“肘区”所对应的 K 值。这种方法也称为 Elbow method.</p><p>但是绝大多数时候，我们看到的是如下图右边的情况，这种时候……好吧看你的心情以及实际需求（最多只能承受多少类的负担）取值吧。<br><img src="https://i.imgur.com/mUOx6Rc.png" alt=""></p><p>如果出现K−J(cost)图随着 K 值的增加上下起伏，那说明出现了“局部最大值”的问题。这个时候就应该使用上一小节提到的，多进行几次 K-Means 算法，求出一个J(cost)最小时的 cost 作为当前 K 值的 cost.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (6) Machine Learning Advice</title>
    <link href="https://paradoxallen.github.io/9059/"/>
    <id>https://paradoxallen.github.io/9059/</id>
    <published>2017-09-04T16:00:00.000Z</published>
    <updated>2018-06-15T16:45:25.622Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Evaluating-a-Learning-Algorithm"><a href="#1-Evaluating-a-Learning-Algorithm" class="headerlink" title="1. Evaluating a Learning Algorithm"></a>1. Evaluating a Learning Algorithm</h3><p>如果一个机器学习方法的结果不令人满意，可能有各种方法来解决。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">获取更多的训练样本（最为耗时，不作为优先考虑）</span><br><span class="line">尝试更少 / 更多的特征</span><br><span class="line">尝试引入多项式特征</span><br><span class="line">增加 / 减少正则化系数 λ</span><br></pre></td></tr></table></figure></p><p>究竟应该使用哪种方法来解决问题，需要一个诊断过程，称为 <strong>Maching Learning Diagnostic</strong>。为了更好的阐述，我们先引入两个名词：<strong>欠拟合、过拟合 </strong></p><p><strong>欠拟合：</strong>对于训练集，hypothesis 得到的结果，与真实的结果差距较大，并不能对样本集有效拟合；</p><p><strong>过拟合：</strong>对于训练集，hypothesis 得到的结果，与真实的结果差距较小；但是对于测试集，hypothesis 得到的结果，与真实的结果差距较大。这说明 hypothesis 的泛化能力较差，只是在训练集上得到的效果较好</p><p>通常情况下，我们会得到一组数据而不是区分好的训练集与测试集。这时就需要我们做一些<strong>处理</strong>：</p><p>首先打乱数据的次序，然后将其之前大约70%的部分来作为训练集，训练样本总数记为m，训练样本记为<img src="https://i.imgur.com/S04RXiX.png" alt=""></p><p>剩下的30%部分作为测试集，测试样本总数记为mtest，测试样本记为<img src="https://i.imgur.com/rluZ3Dt.png" alt=""></p><p>在测试集上，我们会计算 hypothesis 与真实数据的偏差。对于线性回归与逻辑回归，有一些不同。这些在之前的系列中都有提及： </p><p><strong>线性回归</strong>：<img src="https://i.imgur.com/BmXcDfD.png" alt=""><br><strong>逻辑回归</strong>：<img src="https://i.imgur.com/g2yXaRe.png" alt=""></p><p>我们在训练集、测试集的基础上，再次添加一种数据集：<strong>交叉验证集（cross validation set）</strong>。</p><p>比例大约是训练集60%，测试集20%，交叉验证集20%，交叉验证样本总数记为mcv，交叉验证样本记为<img src="https://i.imgur.com/B6gENOi.png" alt="">。而Jcv(θ)的计算方式与Jtest(θ)相同，只是适用的数据集范围不同。 </p><p>那么问题来了，为什么要区分这三个集合？以及它们的区别是什么呢？</p><p>这三个名词在机器学习领域的文章中极其常见，但很多人对他们的概念并不是特别清楚，尤其是后两个经常被人混用。Ripley, B.D（1996）在他的经典专著《Pattern Recognition and Neural Networks》中给出了这三个词的定义。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Training set: A set of examples used for learning, which is to fit the parameters [i.e., weights] of the classifier.</span><br><span class="line">Validation set: A set of examples used to tune the parameters [i.e., architecture, not weights] of a classifier, for example to choose the number of hidden units in a neural network.</span><br><span class="line">Test set: A set of examples used only to assess the performance [generalization] of a fully specified classifier.</span><br></pre></td></tr></table></figure></p><p>显然，<strong>training set是用来训练模型或确定模型参数的</strong>，如ANN中权值等； <strong>validation set是用来做模型选择（model selection），即做模型的最终优化及确定的</strong>，如ANN的结构；而<strong> test set则纯粹是为了测试已经训练好的模型的推广能力</strong>。</p><p>当然，test set这并不能保证模型的正确性，他只是说相似的数据用此模型会得出相似的结果。但实际应用中，一般只将数据集分成两类，即training set 和test set，大多数文章并不涉及validation set。<br>Ripley还谈到了<strong>Why separate test and validation sets?</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">The error rate estimate of the final model on validation data will be biased (smaller than the true error rate) since the validation set is used to select the final model.</span><br><span class="line">After assessing the final model with the test set, YOU MUST NOT tune the model any further.</span><br></pre></td></tr></table></figure></p><p>对于过拟合，如何选择 model？使用多高次方的多项式拟合？答案是<strong>遍历尝试</strong>，如下图所示。分别在训练集上计算得到各个θ的值之后，再在交叉验证集（而不是测试集）上计算Jcv(θ)，选择Jcv(θ)最小的一个model作为hypothesis.<br><img src="https://i.imgur.com/mOk3wc8.png" alt=""></p><p>如此一来，测试集上的Jtest(θ)就可以用于其他评估方向，而依靠交叉验证集来决定多项式拟合的次数。</p><h3 id="2-Bias-vs-Variance"><a href="#2-Bias-vs-Variance" class="headerlink" title="2. Bias vs. Variance"></a>2. Bias vs. Variance</h3><p><strong>Bias 与 Variance </strong>并非是通常意义上的名词偏差、方差。而是用于描述两种机器学习中出现的问题。</p><p><strong>high bias意味着欠拟合，high variance意味着过拟合。</strong>下图可以直观看出来：<br><img src="https://i.imgur.com/CzSF40I.png" alt=""></p><p>接下来，我们从另一些角度来看看 Bias 与 Variance 的表现<br>以 hypothesis 的多项式次数 d 为横轴，error 为纵轴，我们可以得到在训练集与交叉验证集上的误差曲线为：<br><img src="https://i.imgur.com/o40hQlt.png" alt=""></p><p>对于 Bias 与 Variance 也有了各自的概念： </p><p><strong>Bias：</strong>Jtrain(θ)较大，Jcv(θ)较大，Jtrain(θ)≈Jcv(θ)。一般是 d（维度）较小的时候，才会产生 bias，欠拟合阶段；</p><p><strong>Variance：</strong>Jtrain(θ)较小，Jcv(θ)较大，Jtrain(θ)≪Jcv(θ)。一般是 d（维度）较高的时候，才会产生 variance，过拟合阶段</p><p>以 hypothesis 的正则化系数 λ 为横轴，error 为纵轴，我们可以得到在训练集与交叉验证集上的误差曲线与上图左右反转：<br><img src="https://i.imgur.com/tT6dRPD.png" alt=""></p><p><strong>Bias：</strong>Jtrain(θ)较大，Jcv(θ)较大，Jtrain(θ)≈Jcv(θ)。一般是 λ 较大的时候，才会产生 bias，欠拟合阶段；</p><p><strong>Variance：</strong>Jtrain(θ)较小，Jcv(θ)较大，Jtrain(θ)≪Jcv(θ)。一般是 λ 较小的时候，才会产生 variance，过拟合阶段</p><p>那么如何选择 λ 呢？其实类似于选择多项式的维度 d，依次尝试。在确定了多项式维度之后，依次增大 λ 的值后再次计算 Jcv(θ) 的值，找到使得 Jcv(θ) 最小的 λ 值。增大的幅度可以较大，例如 λ=0,0.01,0.02,0.04,0.08,…</p><h3 id="3-Learning-Curve"><a href="#3-Learning-Curve" class="headerlink" title="3. Learning Curve"></a>3. Learning Curve</h3><p><strong>learning curve </strong>同样是以 error 为纵轴，只不过这次的横轴换成了训练集的样本数目，但同样是 Jcv(θ) 与 Jtest(θ) 的曲线。一般形状是这样：<br><img src="https://i.imgur.com/jBnE12f.png" alt=""></p><p>在样本较少的时候，训练集很容易被拟合，因此误差较小，但是因为训练样本少所以很难保证对于交叉验证集也是有效的，因此误差较大。</p><p>在样本较多的时候，训练集较难被拟合，因此误差升高，但是因为训练样本更多所以有更大可能保证对于交叉验证集也是有效的，因此 Jcv(θ) 会下降。</p><p>那么对于 high bias 与 high variance 而言，如果我们使用增加样本个数（最为耗时的方法）m，可否起到什么作用？</p><p><img src="https://i.imgur.com/1GHLkWf.png" alt=""></p><p>由图可以看出，在 high bias 情况下，一般都是由于本身选择的 hypothesis 的模型错误，而与训练集样本个数无关。错的模型训练再多次还是错的。</p><p><img src="https://i.imgur.com/iWW3jJS.png" alt=""></p><p>由图可以看出，在 high variance 情况下，很可能是过拟合问题，而增加训练集样本个数会使得模型的泛化能力进一步增强，从而消除过拟合问题，使得误差降低。</p><h3 id="4-Deciding-What-to-Do-Next-Revisited"><a href="#4-Deciding-What-to-Do-Next-Revisited" class="headerlink" title="4. Deciding What to Do Next Revisited"></a>4. Deciding What to Do Next Revisited</h3><p>回想一下最开始提出的若干种优化方案，各自适用情况有：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">获取更多的训练样本 ⇒ high variance</span><br><span class="line">尝试更少的特征 ⇒ high bias</span><br><span class="line">尝试更多的特征 ⇒ high variance</span><br><span class="line">尝试引入多项式特征 ⇒ high bias</span><br><span class="line">增加正则化系数 λ ⇒ high variance</span><br><span class="line">减少正则化系数 λ ⇒ high bias</span><br></pre></td></tr></table></figure></p><p>将以上所有内容，如果引用到神经网络中，同样适用。 </p><p>小规模（层数少、各层神经元数目少）的神经网络容易出现欠拟合问题；大规模的神经网络容易出现过拟合问题。 </p><p>那么使用多少个隐藏层？可以使用依次递增层数，交叉验证集上误差来确定。 </p><p>λ 该如何确定？根据上文，利用交叉验证集，依次递增选择误差最小的确定即可。</p><h3 id="5-Building-a-Spam-Classifier"><a href="#5-Building-a-Spam-Classifier" class="headerlink" title="5. Building a Spam Classifier"></a>5. Building a Spam Classifier</h3><p>让我们以<strong>“垃圾邮件分类（Spam-Classifier）”</strong>问题作为例子：如何判断一封邮件是垃圾邮件，还是正常邮件？</p><p>很容易我们会想到，依据某些单词作为特征，这些单词的出现与否决定这封邮件的性质。</p><p>所以，我们会想到一个0，1组成的特征向量，1代表邮件具备这个特征（出现这个单词），0代表邮件不具备这个特征（没出现这个单词）。</p><p>有了基本大方向之后，就要开始一场头脑风暴了，以求提高机器学习的效果。这种行为，不存在标准答案，任何结果都是有可能的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">增加数据</span><br><span class="line">采用更为复杂的特征（邮件路径）</span><br><span class="line">基于正文寻找更为精确复杂的特征（discount, discounts 视为同一单词；deal, dealer 视为同一单词）</span><br><span class="line">算法改进（故意拼错隐瞒检查的单词？deeler = dealer）</span><br></pre></td></tr></table></figure></p><p>头脑风暴之后，谁说的才更有道理呢？自然需要各种方法对于误差的分析（error analysis）。这里需要注意：<strong>所有的 error analysis 都是在交叉验证集上完成的。</strong></p><p>一般来说，机器学习算法的设计会经历以下几个过程：<br>以最快的速度（一天之内？）用最简单的方法，尝试去解决眼下的问题，并在交叉验证集上验证；</p><p>画出 learning curve，去观察，发现现在的问题是 high bias，还是 high variance，需不需要更多的特征？更多的数据集？</p><p>error analysis：上面已经在cross-validation数据集上测试了系统性能，现在我们人工去看哪些数据造成了大error的产生？是否可以通过改变systematic trend减少error？</p><p>Spam-Classifier 举例，我们看一下进行Error Analysis的步骤：<br>所有 spam 分为四类：pharma，replica/fake，Steal password 和 其他</p><p>如下图，寻找一些可能有助于改善分类效果的features<br><img src="https://i.imgur.com/lZ2lLvZ.png" alt=""></p><p>然后，在是否引入特征的问题上，一定要做实验：例如，可以比较引入此特征前后，预测的准确率<img src="https://i.imgur.com/CJLBwew.png" alt="">是否提高？</p><p>error analysis 的核心在于：一定要找到一种<strong>数值化的</strong>评定方法，以求判断 error 的大小。</p><h3 id="6-Handling-Skewed-Data"><a href="#6-Handling-Skewed-Data" class="headerlink" title="6. Handling Skewed Data"></a>6. Handling Skewed Data</h3><p>首先，我们需要介绍，什么样的数据称为<strong>偏斜数据（skewed data）</strong>。这次，我们举的例子是预测癌症。</p><p>预测癌症例子：实际生活中，癌症的发病率极低，可能在人群中只有0.5%的发病率。假设我们手头有一种预测方法，预测出来有1%的人会发病。这样，按照之前的评价标准，有0.5%的误诊率。但是，如果我不做任何的检查，直接判断病人是没病的，是不是同样有着0.5%的误诊率？这是不应该的，我们化验检查一番力气之后的答案竟然和瞎猜的误诊率相同。</p><p>当分类问题中，某一类所占比例极小<strong>（一般将较小比例的类别置为1）</strong>，就会有偏斜数据的问题。</p><p>这时候，直接用 error 来描述在这种数据集上的问题不合适，我们需要寻求更新的数值化评价标准。</p><p>引入新的标准 precision 和 recall 之前，我们需要介绍几个新名字：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">True positive：预测结果为真，并且实际分类同样为真</span><br><span class="line">True negative：预测结果为假，并且实际分类同样为假</span><br><span class="line">False positive：预测结果为真，但是实际分类为假</span><br><span class="line">False negative：预测结果为假，但是实际分类为真</span><br></pre></td></tr></table></figure></p><p>说起来有点复杂，来看下面这张图一目了然：<br><img src="https://i.imgur.com/bVYkrZU.png" alt=""></p><p>接下来，我们在此基础上，介绍两个评价标准：</p><p><strong>查准率（precision）</strong>：在查出有癌症的患者中，实际患病的概率，公式见上图。<strong>precision 越高算法越好</strong>；</p><p><strong>召回率（recall）：</strong>所有身患癌症的病人中，有多少概率被查出，公式见上图。 <strong>recall越高算法越好</strong>；</p><p>再回到一开始的例子，如果我全部将病人视为健康，准确率是99.5%，但是我的查准率和召回率极低。可以判定不是一个好算法。</p><p>回到预测癌症例子</p><p>如果为了保证确诊率，可以将逻辑回归的分类值由0.5改为0.7，这样一来 precision 必然会升高，但是也会导致 recall 的下降</p><p>如果为了引起大家的更多重视，可以将逻辑回归的分类值由0.5改为0.3，这样一来 precision 必然会下降，但是也会导致 recall 的升高</p><p>貌似 precision（记为P） 和 recall（记为R） 总是背道而驰，偏偏两者还都是重要的偏斜类上的算法评价标准，如何将这两个标准合为一个？平均值显然是不行的，P=1且R=0时平均值为0.5，看不出端倪。</p><p><strong>F值（F-score）</strong>：F=2PR/(P+R)，完美地解决了这个问题。F值的中心思想就是：赋予P、R中更低的值更大的权值，因为P、R总是此消彼长，这样就制约两者必须同时保持在较大值才使得F值较高。可以用于<strong>所有有着此消彼长关系的标准的综合评价</strong>。</p><h3 id="7-Using-Large-Data-Sets"><a href="#7-Using-Large-Data-Sets" class="headerlink" title="7. Using Large Data Sets"></a>7. Using Large Data Sets</h3><p>一般来说，增大数据集，可以提高算法的 accuracy，但也不全是这样。比如房价预测，如果我仅仅给你房子的面积，而没有房子在市中心还是偏远地区？房龄多少？等信息，我们是无法进行良好预测的。所以，我们需要知道的前提条件是： </p><p><strong>如果当前的特征已经足够预测，增大数据集的确可以提高准确性 </strong><br>总结： </p><ol><li><p>想要保证bias小，就要保证有足够多的feature，即linear/logistics regression中有很多parameters，neuron networks中应该有很多hidden layer neurons</p></li><li><p>想要保证variance小，就要保证不产生overfit，那么就需要很多data set（只要样本数远远大于特征数，是无法过拟合每个点）。这里需要Jcv和Jtrain都很小，才能使Jtest相对小</p></li></ol><p>综上所述，对数据及进行理性分析的结果是两条： </p><p><strong>首先，x中有足够多的feature，以得到low bias;</strong><br><strong>其次，有足够大的training set，以得到low variance</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (5) Neural Network Part2</title>
    <link href="https://paradoxallen.github.io/53070/"/>
    <id>https://paradoxallen.github.io/53070/</id>
    <published>2017-08-29T16:00:00.000Z</published>
    <updated>2018-06-15T04:54:42.481Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><p>上一篇文章，介绍了<strong>前向传播(forward propagation)</strong>的过程，以及神经网络计算非线性问题的例子(XOR问题)</p><p>这一篇文章，开始介绍，如何来计算神经网络中各种参数的方法：<strong>后向传播(backward propagation)</strong></p><h3 id="1-Cost-Function"><a href="#1-Cost-Function" class="headerlink" title="1. Cost Function"></a>1. Cost Function</h3><p>为了拟合神经网络的各个参数，我们首先需要规定一些变量：<br><img src="https://i.imgur.com/GxSynvL.png" alt=""></p><p>不同分类问题：</p><p>若分为2类，其实用一个神经元作为输出层就可以了，用y=0和y=1区分；</p><p>多类问题，利用以前说过的，分为K类则最终的y∈R^K，例如分为三类的问题输出可选择为<img src="https://i.imgur.com/ariPfuP.png" alt=""></p><p>因为同样是分类问题，我们回想一下逻辑回归的cost function，其实神经网络的相同，只不过是对于分为K类问题的版本而已：<br><img src="https://i.imgur.com/9XJbSNE.png" alt=""></p><p>是不是……太复杂了，如果用 cost function 计算梯度之后梯度直接梯度下降法来计算神经网络参数，明显不如接下来介绍的这种方法简单快捷。</p><h3 id="2-Backpropagation"><a href="#2-Backpropagation" class="headerlink" title="2. Backpropagation"></a>2. Backpropagation</h3><p><strong>后向传播</strong>，是神经网络中用于替代直接计算梯度下降法，来设定各层参数的方法。方法的精髓在于，<strong>训练网络时先根据初始化的参数（一般是随机设定），计算得到最后一层（输出层）的输出，计算与实际网络输出之间的差，再根据当前层的差，反推出上一层的差，逐渐反推到第一层。每一层根据自身层的差，来逼近真实参数</strong>。</p><p>首先我们设定某l层的某j个神经元，与真实的神经元的值，两者的差距为<img src="https://i.imgur.com/Nc3lrrH.png" alt="">，the error of node j in layer l.</p><p>对于输出层的每个神经元来说，可以直接计算：<img src="https://i.imgur.com/3Ds9vuD.png" alt="">，向量化表示为<img src="https://i.imgur.com/cgaxaK5.png" alt=""></p><p>对于其他层的每个神经元而言，就需要依靠上一层神经元的计算结果来反推，<strong>反推的过程可以视为原本 Forwardpropagation 过程中分散到各个下一层神经元的水流沿着同样的道路再次汇聚到上一层的神经元中</strong>：<img src="https://i.imgur.com/9XmWAoL.png" alt=""></p><p>对于g′(t)的计算，这里需要一点点小技巧：<br><img src="https://i.imgur.com/CqMnQoj.png" alt=""></p><p>推导过程见下：<br><img src="https://i.imgur.com/Or2ogvB.png" alt=""></p><p>不存在δ(1)，因为<strong>输入层没有误差</strong></p><p>我们首先不考虑正则化项（最小化各个系数<img src="https://i.imgur.com/AbSeYkS.png" alt="">，得到计算各层各个单元的误差项之和</p><p>因此，我们得到求出神经网络各层参数的方法：<br><img src="https://i.imgur.com/SKUhVtK.png" alt=""></p><h3 id="3-Gradient-Checking"><a href="#3-Gradient-Checking" class="headerlink" title="3. Gradient Checking"></a>3. Gradient Checking</h3><p>当我们需要验证求出的神经网络工作的对不对呢？可以使用 gradient checking，通过check梯度判断我们的code有没有问题</p><p>对于下面这个[θ−J(θ)]图，取θ点左右各一点(θ−ϵ)与(θ+ϵ)，则点θ的梯度近似等于<img src="https://i.imgur.com/xliVzIX.png" alt=""></p><p><img src="https://i.imgur.com/XvkBIeh.png" alt=""></p><p>对于神经网络中的情况，则有：<br><img src="https://i.imgur.com/SPV9RtR.png" alt=""></p><p>由于在backpropagation算法中我们一直能得到J(θ)的导数D（derivative），那么就可以将这个近似值与D进行比较</p><p><strong>Summary: </strong><br>(1) 在backpropagation中计算出J(θ)对θ的导数D并组成vector（Dvec） </p><p>(2) 用numerical gradient checking方法计算大概的梯度<img src="https://i.imgur.com/xa4LZR9.png" alt=""></p><p>(3) 看是否得到相同（or相近）的结果 </p><p>(4) <strong>（非常重要）</strong>停止checking，只用 backpropagation 进行神经网络学习，否则会非常非常慢</p><h3 id="4-Backpropagation-in-Practice"><a href="#4-Backpropagation-in-Practice" class="headerlink" title="4. Backpropagation in Practice"></a>4. Backpropagation in Practice</h3><p>这一节我们来看看实际 octave/MATLAB 编程中的一些技巧，例如对于神经网络如下：<br><img src="https://i.imgur.com/e2uWpzX.png" alt=""><br>当s1=10,s2=10时，则有θ(1)∈R10×11,θ(2)∈R10×11，一般来说，为了方便变形与传递参数，我们是将所有θ展开成一个完整的变量：<br>    thetaVec=[Theta1(:);Theta2(:);Theta3(:)] </p><p>再次展开的时候，例如重组为θ(1)时，取出其中前110个重组就好：<br>    Theta1=reshape(thetaVec(1:110),10,11)</p><p>如何初始化各层的参数呢？这同样是一个需要注意的地方：<strong>不能将各层的各个神经元的参数赋值为相同的数</strong>。</p><p>有兴趣的同学可以计算一下，这样神经网络的某一层内的所有神经元计算都变得相同，这样神经网络的非线性程度就降低了。<strong>一般来说，都是在(+ϵ,−ϵ)之间随机赋值</strong></p><p>我的神经网络需要有多少层呢？同样是个有趣的问题，一般来说，<strong>三层结构（仅仅一个隐藏层）</strong>已经足够来处理大部分非线性情况。如果分类效果不好，可以尝试使用更多的隐藏层，但需要保证每个隐藏层的神经元个数相同，层数越多就越慢，当然一般来说分类效果就更好</p><p>每层神经网络需要多少个神经元？能确定的只有输入层与输出层：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">No. of input units: Dimension of features</span><br><span class="line">No. output units: Number of classes</span><br><span class="line">Other units: who knows… usually the more the better</span><br></pre></td></tr></table></figure></p><h3 id="5-Put-it-together"><a href="#5-Put-it-together" class="headerlink" title="5. Put it together"></a>5. Put it together</h3><p>最终我们回顾一下神经网络的主要步骤： </p><p><strong>randomly initialize weights</strong></p><p><strong>(for 1 to m) forward-propagation</strong></p><p><strong>(for 1 to m) cost function</strong></p><p><strong>(for 1 to m) backward-propogation</strong></p><p><strong>gradient checking, then stop</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (4) Neural Network Part1</title>
    <link href="https://paradoxallen.github.io/21187/"/>
    <id>https://paradoxallen.github.io/21187/</id>
    <published>2017-08-22T16:00:00.000Z</published>
    <updated>2018-06-14T00:26:45.751Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-由XOR-Problem想到"><a href="#1-由XOR-Problem想到" class="headerlink" title="1. 由XOR Problem想到"></a>1. 由XOR Problem想到</h3><p>有一种经典的非线性问题：<strong>XOR，也就是异或</strong>。真值表如下： </p><p>0 0 | 0<br>1 0 | 1<br>1 1 | 0<br>0 1 | 1，| 左侧是输入，| 右侧是输出 </p><p>如果在二维坐标系上绘图，可以看出来仅利用一条直线是无法对这个问题分类的，也就是<strong>线性不可分</strong>问题。</p><p>如果利用逻辑回归的方法，可以枚举各种特征的出现可能，即<br>继续想下去，如果基础特征量更多的话？就会出现过拟合的问题，同时带来极大的计算量。</p><p>例如，计算机视觉中处理一张照片，每个像素都需要一个数值表示。对于100*100像素值的图片，仅仅考虑二次项等级，就有特征数量为5000个（与相同，故除以2）。</p><p>于是，这章介绍的非线性分类器，<strong>神经网络（Neural Network，NN）</strong>就发挥了作用。</p><h3 id="2-人工神经网络"><a href="#2-人工神经网络" class="headerlink" title="2. 人工神经网络"></a>2. 人工神经网络</h3><p>神经网络最初提出的初衷，是因为要模拟人类大脑的结构（很初级的模拟，因为人类对于自己大脑究竟是怎样都没有弄清楚）。<strong>通过多个感知机之间的输入输出，从而完成整体的智能行为</strong>。</p><p>在人工神经网络中，“感知机”就是一个有着输入与输出功能的小单元，接收上一层的输入，将输出传给下一层。</p><p>人工神经网络是层级结构，某一层上的单元之间互相不会有输入输出关系，只和上一层或者下一层的单元产生数据传输关系。至少会有两层：<strong>输入层（input layer）与输出层（output layer）</strong>，但是两层的神经网络可以解决的问题很少，一般都是三层或者三层以上，中间的这些层就称为<strong>“隐藏层（hidden layer）”</strong>，我们来看一个最简单的例子：<br><img src="https://i.imgur.com/U02TDKa.jpg" alt=""></p><p>由<strong>输入层，到隐藏层，最终到输出层</strong>。这是一次 <strong>forward propogation</strong> 过程。类似于逻辑回归，但是神经网络的输入是某个样本的所有基础特征，不需要考虑 这一类新加入的特征。</p><h3 id="3-回到XOR-Problem"><a href="#3-回到XOR-Problem" class="headerlink" title="3. 回到XOR Problem"></a>3. 回到XOR Problem</h3><p>先讲几个基础的利用神经网络进行二进制运算分类的问题：</p><ol><li><p>二进制 AND<br><img src="https://i.imgur.com/5pgAvdw.jpg" alt=""><br>即为，只有1,1时返回值才为1，符合 AND 的操作结果。</p></li><li><p>二进制 OR<br><img src="https://i.imgur.com/vNpszeX.jpg" alt=""><br>即为，只有0,0时返回值才为0，符合 OR 的操作结果。</p></li><li><p>二进制 NOT<br><img src="https://i.imgur.com/8ODYZRw.jpg" alt=""></p></li></ol><p>XOR 问题复杂一些，但是如果我们做了如下转换：</p><p><code>XNOR = NOT XOR = AND OR NOT AND NOT</code></p><p>变换的正确性，很容易通过真值表来验证。大家可以分别计算各个括号中的内容，然后通过 OR 连接起来。<br>我们将 AND 的内容视为 ，NOT AND NOT 的内容视为<br><img src="https://i.imgur.com/tcDGEAE.jpg" alt=""></p><h3 id="4-神经网络多类分类"><a href="#4-神经网络多类分类" class="headerlink" title="4. 神经网络多类分类"></a>4. 神经网络多类分类</h3><p>神经网络处理多类的分类问题是很方便的。举个例子，区分手写数字时，有10个类别：0，1，2，……，9。<br>对于某一个训练样本来说，有着特征组合,这个神经网络的输出层有10个单元。当输出层的10个单元全部取 0，意味着输入不是任何一种数字。</p><p>对于手写数字的识别，一直是业界的研究重点之一。视频中举了一篇经典的利用神经网络处理该问题的Paper，有兴趣的同学可以访问作者的个人主页查看Demo与Paper：<a href="http://yann.lecun.com/exdb/lenet/" target="_blank" rel="noopener">Yann LeCun</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (3) Logistic Regression &amp; Regularization</title>
    <link href="https://paradoxallen.github.io/26112/"/>
    <id>https://paradoxallen.github.io/26112/</id>
    <published>2017-08-15T16:00:00.000Z</published>
    <updated>2018-06-13T04:32:05.451Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Logistic-Regression"><a href="#1-Logistic-Regression" class="headerlink" title="1. Logistic Regression"></a>1. Logistic Regression</h3><p>对于分类问题而言，很容易想到利用<strong>线性回归</strong>方法，拟合之后的<strong>hθ(x)&gt;0.5则为True，其余为False</strong>.</p><p>但是线性回归有一个问题，拟合出的值都是离散的，范围不确定。</p><p>为了方便分析，我们希望将拟合出的值限制在0~1之间。因此，出现了<strong>逻辑回归</strong>。</p><p>逻辑回归的模型是一个<strong>非线性模型</strong>：<strong>sigmoid函数，又称逻辑回归函数</strong>。但它本质上又是一个线性回归模型，因为除去sigmoid映射函数关系，其他的步骤，算法都是线性回归的。</p><p>sigmoid函数（或，逻辑回归函数）：<br><img src="https://i.imgur.com/QLwtN5k.png" alt=""><br>其函数图像为：<br><img src="https://i.imgur.com/wC6xW9Z.png" alt=""></p><p>这个函数的特征非常明显<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">函数值一直在0~1范围内；</span><br><span class="line">经过(0,0.5)点。这个很容易作为区分0，1类的分界线。</span><br></pre></td></tr></table></figure></p><p>逻辑回归中，对于原本线性回归中拟合而成的hypothesis函数，需要经过sigmoid函数的修饰：<br><img src="https://i.imgur.com/ElJujkC.png" alt=""></p><p>此时，hθ(x)的含义发生了变化，<br><img src="https://i.imgur.com/6DX46yY.png" alt=""><br>。成为<br>        <strong>‘’the probability that y=1, given x, parameterized by θ’’</strong></p><p>因此有<br><img src="https://i.imgur.com/SbZ7wQN.png" alt=""></p><p><strong>Decision Boundary</strong>。表示的是 hypothesis 函数确定之后，划分数据分类的界限，并不一定可以百分百区分数据集，只是函数的属性之一。下图蓝色曲线即为某个 Desicision Boundary。<br><img src="https://i.imgur.com/nM4tJjr.jpg" alt=""></p><h3 id="2-Cost-Function"><a href="#2-Cost-Function" class="headerlink" title="2. Cost Function"></a>2. Cost Function</h3><p>回忆线性回归的 cost function，我们在其中插入 cost 函数的概念：<br><img src="https://i.imgur.com/2fhUuge.png" alt=""></p><p>完全照搬线性回归的 cost function 到逻辑回归中，因为sigmoid函数的非线性，会造成J(θ)取值的不断震荡，导致其是一个非凸形函数（non-convex）。表示在“J(θ)—θ”二维图中如下：<br><img src="https://i.imgur.com/YxpdAK6.png" alt=""></p><p>我们需要构造一种新的 cost 函数。出发点为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">当y=1时，若hypothesis函数拟合结果为0，即为“重大失误”，cost 趋于无穷大；</span><br><span class="line">当y=0时，若hypothesis函数拟合结果为1，即为“重大失误”，cost 趋于无穷大；</span><br></pre></td></tr></table></figure></p><p>构造的新 cost 函数：<br><img src="https://i.imgur.com/1AZPras.png" alt=""></p><p>如果进一步合并，可以得到最终逻辑回归的cost函数。并且值得指出的是，代入这个cost函数通过梯度下降法得到的 θ 更新函数依然成立：<br><img src="https://i.imgur.com/F16aq5V.png" alt=""></p><h3 id="3-梯度下降法的优化"><a href="#3-梯度下降法的优化" class="headerlink" title="3. 梯度下降法的优化"></a>3. 梯度下降法的优化</h3><p>对于梯度下降法的优化有很多，但是都需要J(θ)与∂J(θ)/∂θj的代码。</p><p>以此为基础的对于梯度下降法的优化（视频中都没有具体介绍，有兴趣的同学可以点击链接）有：<br><a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method" target="_blank" rel="noopener">共轭梯度法</a><br><a href="https://en.wikipedia.org/w/index.php?title=Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm&amp;redirect=no" target="_blank" rel="noopener">BFGS</a><br><a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS" target="_blank" rel="noopener">L-BFGS</a></p><p>这些优化方法的特点也很一致：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">不需要人为选择 α，自适应性</span><br><span class="line">更复杂，更慢</span><br></pre></td></tr></table></figure></p><p>这里提到了两个MATLAB的非线性优化函数：<br><strong>optimset：创建或编辑一个最优化参数选项。</strong>具体调用在MATLAB中 <code>help optimset</code> 命令查看；<br><strong>fminunc：最小值优化。</strong>具体调用在MATLAB中 <code>help fminunc</code>命令查看；</p><p><strong>建议</strong>：Ng在优化这一部分讲的过于简略，基本等于什么都没说……还是要根据这几个方法名称在使用时搜索更多。</p><h3 id="4-one-vs-all-one-vs-rest"><a href="#4-one-vs-all-one-vs-rest" class="headerlink" title="4.one vs. all (one vs. rest)"></a>4.one vs. all (one vs. rest)</h3><p>如果需要进行多类的分类，需要一种精妙的修改，使得两类的分类问题得以适用于多类的分类。 </p><ol><li><p>现已知有n类样本需要区分开（1，2，3，……）；</p></li><li><p>以原1类为新1类，剩余的原2，3，……作为新2类。原本的多类问题变成了二类问题:<br><img src="https://i.imgur.com/TKq60Ef.png" alt=""></p></li><li>以原2类为新1类，剩余的原1，3，……作为新2类。再分类:<br><img src="https://i.imgur.com/EioDwff.png" alt=""></li><li><p><img src="https://i.imgur.com/wK9pOb4.png" alt=""></p></li><li><p>对于任意一个 x 而言，如何分辨是哪一类呢？于是，求出所有的<img src="https://i.imgur.com/7U91HVE.png" alt="">值最大对应的i（<strong>表示y=i的概率最大</strong>）即为x的所属分类</p></li></ol><h3 id="5-Regularization（正则化）"><a href="#5-Regularization（正则化）" class="headerlink" title="5. Regularization（正则化）"></a>5. Regularization（正则化）</h3><p>拟合会产生三种情况： </p><ol><li><p><strong>underfitting（欠拟合）=high bias</strong>，大部分训练样本无法拟合</p></li><li><p><strong>overfitting（过拟合）=high variance</strong>，为了拟合几乎每一个训练样本。导致拟合函数极为复杂，易产生波动，泛化（generalize）能力差，虽然训练样本几乎百分百拟合，但是测试样本很可能因为极大波动而极少拟合成功</p></li><li><p><strong>just right</strong>，对于训练样本，拟合得不多不少刚刚好，并且泛化到测试样本拟合效果同样较好</p></li></ol><p>欠拟合，比较好解决，创造并引入更多的特征即可。例如：对于x,y而言，可以引入x2,y2,xy等等新的特征</p><p>过拟合，则比较复杂。可用的方法有两个： </p><ol><li><p><strong>Reduce number of features, 减少特征量</strong></p></li><li><p><strong>Regularization，正则化</strong>。保持所有的特征数量不变，而去改变特征前的度量单位 θj（若 θj 趋于0，则此特征可视为无影响）</p></li></ol><p>解决过拟合的正则化方法，因此需要引入全新的优化目标到 cost function 中。原先的 cost function 只是希望适合拟合更为接近，现在还需要使得特征前的度量单位 θj 的最小。因此有：<br><img src="https://i.imgur.com/Msp5tw4.png" alt=""></p><p>正则化方法处理之后，∂J(θ)/∂θj发生对应变化，因此我们有：<br><img src="https://i.imgur.com/dmhGxHY.png" alt=""></p><p>若λ非常大（例如10^10），则正则化方法会导致结果 underfitting。这也很好理解，因为优化目标中有使得 <img src="https://i.imgur.com/ctA4muM.png" alt=""> 尽可能小，这样会导致 θ 全部趋于 0。</p><p>一般来说，<strong>α,λ,m&gt;0，所以(1−αλ/m)&lt;1，常见使其取值0.99 左右</strong></p><h3 id="6-Regularization-for-Normal-Equation"><a href="#6-Regularization-for-Normal-Equation" class="headerlink" title="6. Regularization for Normal Equation"></a>6. Regularization for Normal Equation</h3><p>课程视频中缺少证明，因此我们仅需掌握结论使用即可<br>对于 Week 2 中的<strong>Normal Equation</strong>方法，原本需要求解的方程<br><img src="https://i.imgur.com/ZiPg749.png" alt=""><br>做一个小小的改动：<br><img src="https://i.imgur.com/cZwlXun.png" alt=""></p><p>若样本拥有n个特征，则<img src="https://i.imgur.com/Gu1x2Z9.png" alt="">表示的是(n+1) * (n+1)维的对角矩阵，除了(0, 0)取值为 0，其余对角位置取 1。</p><p><strong>non-invertibility</strong>：非不可逆性……好拗口，意思就是对于原本的(xTx)矩阵可能会出现不可逆的情况。但是，对于正则化之后的矩阵<img src="https://i.imgur.com/KB79QV2.png" alt="">一定是可逆的（未提供证明）。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;br&gt;&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (2) Linear Regression</title>
    <link href="https://paradoxallen.github.io/58080/"/>
    <id>https://paradoxallen.github.io/58080/</id>
    <published>2017-08-09T16:00:00.000Z</published>
    <updated>2018-06-13T04:32:05.448Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Multivariate-Linear-Regression"><a href="#1-Multivariate-Linear-Regression" class="headerlink" title="1. Multivariate Linear Regression"></a>1. Multivariate Linear Regression</h3><p>Week 1 讨论仅一个特征，即仅有一个未知量x影响了目标y的取值。如果现在有很多特征？现在我们有x1,x2…xn影响了目标y的取值。</p><p>此时需要区分的是变量标记规则：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xi表示的是第i个特征</span><br><span class="line">x(i)表示的是第i个样本，一个样本是由多个特征组成的列向量</span><br></pre></td></tr></table></figure></p><p>例如：<br><img src="https://i.imgur.com/1VQTtI7.png" alt=""><br>综上，我们有<br><img src="https://i.imgur.com/IN8QuHt.png" alt=""><br>可以视为，每个样本都多出一个特征：x0=1，这样表示有利于之后的矩阵表示</p><h3 id="2-多变量梯度下降法："><a href="#2-多变量梯度下降法：" class="headerlink" title="2. 多变量梯度下降法："></a>2. 多变量梯度下降法：</h3><p><img src="https://i.imgur.com/jqtDLIq.png" alt=""></p><h3 id="3-Feature-Scaling（特征缩放）"><a href="#3-Feature-Scaling（特征缩放）" class="headerlink" title="3. Feature Scaling（特征缩放）"></a>3. Feature Scaling（特征缩放）</h3><p>很简单，就是将每种特征的数据范围限定在同一个数量级。例如<br><img src="https://i.imgur.com/cauqMeB.png" alt=""><br>这样会导致迭代次数过多。这时候，如果我们找到一种mapping方式，使得两者属于同一个数量级的范围内，可以有效减少迭代次数</p><p><strong>注意</strong>：无法降低单次的迭代时间，但是却能有效地降低迭代次数</p><p>其实方法很多，这有一种：<br><img src="https://i.imgur.com/egjFhiQ.png" alt=""><br>其中，mean(x)表示向量每个元素的平均值，max(x)表示向量中最大元素，min(x)表示向量中最小元素</p><h3 id="4-Learning-Rate"><a href="#4-Learning-Rate" class="headerlink" title="4. Learning Rate"></a>4. Learning Rate</h3><p>learning rate 是机器学习中的一个不稳定因素，如何判断选取的 learning rate 是合适的？我们可以看看以下这幅图：<br><img src="https://i.imgur.com/81mq8t6.jpg" alt=""></p><p>如果以迭代次数为横坐标，cost function 结果为纵坐标，绘制的图像是递减的，说明 learning rate 选择的是恰当的。如果碰到下图所显示的三种情况，那就只有一条路：<strong>减小 learning rate </strong><br><img src="https://i.imgur.com/DmExZo6.jpg" alt=""></p><p>但是 learning rate 太小同样会导致一个问题：<strong>学习过慢</strong>。所以，只能靠试：0.001，0.003，0.01，0.03，0.1，0.3……</p><h3 id="5-Polynomial-Regression（多项式回归，不同于多变量线性回归）"><a href="#5-Polynomial-Regression（多项式回归，不同于多变量线性回归）" class="headerlink" title="5. Polynomial Regression（多项式回归，不同于多变量线性回归）"></a>5. Polynomial Regression（多项式回归，不同于多变量线性回归）</h3><p>有时候，我们需要自己创造一些“特征”，来拟合一些非线性分布情况<br>例如：<br><img src="https://i.imgur.com/5F2PbWA.png" alt=""><br>看上去只有一个特征x，但我们完全可以理解为x^2和√x都是单独的新特征</p><p>以后的课程会具体讲述如何选择这些特征</p><h3 id="6-Normal-Equation"><a href="#6-Normal-Equation" class="headerlink" title="6. Normal Equation"></a>6. Normal Equation</h3><p>梯度下降法可以用于寻找函数（cost function）的最小值，想一想，初高中的时候我们使用的是什么方法？最小值点的导数为零，然后解方程</p><p>将导数置为零这种方法即<strong> Normal Equation</strong>。<br><img src="https://i.imgur.com/aqLezpl.png" alt=""></p><p>上文提过，增加一个全1分量x0后得到<br><img src="https://i.imgur.com/N6qtlas.png" alt=""></p><p>可以得到：<br><img src="https://i.imgur.com/LU4vw47.png" alt=""></p><p>matlab编程十分简单：<br><img src="https://i.imgur.com/OVUbKD9.png" alt=""></p><p>Normal Equation 有以下<strong>优缺点</strong>：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">不需要 learning rate，也就不需要选择；</span><br><span class="line">不需要迭代，不需要考虑收敛的问题；</span><br><span class="line">当特征非常多的时候，因为涉及求逆操作，会非常慢（注：方阵才有逆矩阵）</span><br></pre></td></tr></table></figure></p><h3 id="7-Octave-Tutorial"><a href="#7-Octave-Tutorial" class="headerlink" title="7. Octave Tutorial"></a>7. Octave Tutorial</h3><p>这一部分十分简单，其实就是MATLAB的使用方法。建议不论是否初学者都去看看，会有收获。 </p><p>谈到一个问题：<strong>如果现有的样本数，小于每个样本所有的特征数怎么办？去除多余的特征（PCA？）</strong>。特征过多，也可能会导致矩阵不可逆的情况。 </p><p>下面记录一些觉得挺有趣的命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">~=：不等于号</span><br><span class="line">xor(0, 1)：异或操作</span><br><span class="line">rand(m, n)：0~1之间的大小为m*n的随机数矩阵；randn：产生均值为0，方差为1的符合正态分布的随机数（有负数）</span><br><span class="line">length(A)：返回A中行、列中更大值</span><br><span class="line">A(:)：将矩阵A变为列向量形式，不论A是向量还是矩阵</span><br><span class="line">sum(A,1)：每列求和得到一个行向量；sum(A,2)：每行求和得到一个列向量</span><br><span class="line">pinv：伪求逆；inv：求逆</span><br><span class="line">imagesc(A)：帅爆！根据矩阵中每个值绘制各种颜色的方块</span><br><span class="line">A.^2 ~= A^2，后者是两个矩阵相乘</span><br></pre></td></tr></table></figure></p><h3 id="8-Submitting-Programming-Assignments"><a href="#8-Submitting-Programming-Assignments" class="headerlink" title="8. Submitting Programming Assignments"></a>8. Submitting Programming Assignments</h3><p>其实看看视频就行了，主要要注意，submit() 时输入的Token，不是Coursera 的密码，而是作业的密码：<br><img src="https://i.imgur.com/a8OsCRt.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Andrew Ng Machine Learning (1) Introduction</title>
    <link href="https://paradoxallen.github.io/36971/"/>
    <id>https://paradoxallen.github.io/36971/</id>
    <published>2017-08-04T16:00:00.000Z</published>
    <updated>2018-06-13T04:32:05.450Z</updated>
    
    <content type="html"><![CDATA[<p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p><a id="more"></a><hr><h3 id="1-Environment-Setup-Instructions"><a href="#1-Environment-Setup-Instructions" class="headerlink" title="1. Environment Setup Instructions"></a>1. Environment Setup Instructions</h3><p>这一章介绍课程一般使用的工具。octave或者matlab即可，这两者本质上没有什么区别，都有着丰富的数学库计算库。</p><h3 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2. Introduction"></a>2. Introduction</h3><ol><li><strong>机器学习定义：</strong>简单来说，让计算机执行一些行为，但是without explicit programmed。</li><li><p><strong>监督学习（supervised learning）：</strong>通过<strong>已知</strong>的正确信息，得到未知的信息。一般用于解决的问题有两类： </p><ol><li><p><strong>回归问题（regression）</strong><br> 例如：房价预测。已知一百平房子售价，两百平房子售价，三百平房子售价。那么，一百五十平房子的售价呢？</p><p> 很容易想到，<strong>拟合</strong>。那么接下来，是直线拟合？二次曲线拟合？</p><p> 这就是回归问题要解决的，通过已知离散信息，<strong>预测</strong>未知的连续信息。</p></li><li><p><strong>分类问题（classification）</strong><br> 例如：肿瘤分类。已知肿瘤size与肿瘤良性（label: 0）还是恶性（label: 1）分类的关系如下：<br><img src="https://i.imgur.com/MlGVT51.jpg" alt=""></p><p> 更进一步，完全可以用一维坐标图来表示：<br><img src="https://i.imgur.com/xI6gTxQ.jpg" alt=""></p><p> 现在需要对于根据size的对某个肿瘤判断它是良性还是恶性，就是分类问题。</p><p> 再进一步，现在我仅仅把size作为feature之一，如果我们加入考虑肿瘤积水（二维坐标图）？肿瘤硬度（三维坐标图）？原则上可以考虑无限种feature，如何处理？这里卖了个关子，可以使用支持向量机（SVM）。</p></li><li><p><strong>无监督学习（unsupervised learning）/ 聚类算法（clustering）：</strong><br> 对于有监督学习而言，我们已知的信息很多：每个data的label是什么，label可以分为多少类，等等。<strong>但是无监督学习中，这些都是未知的。</strong><br> 最常见的应用：浏览新闻时经常会有的相关新闻，或者搜索时的相关搜索结果，又或是weibo中的推荐分组，等等。</p><p> 可以理解为计算机自己理解后将信息分为了若干类。</p><p> 经典案例：<a href="http://soma.mcmaster.ca/papers/Paper_7.pdf" target="_blank" rel="noopener"><strong>cocktail party problem</strong></a>。简而言之，就是在聚会上摆俩麦克风分别记录声音，需要通过这些信息将不同声源的声音分隔开来。</p><p> 如果考虑声音本身的特征与背景知识，解法会比较复杂，需要大量的先验知识。但站在机器学习的角度来思考，就是一类无监督学习的应用场景。matlab实现仅需几行代码而已。</p></li></ol></li></ol><h3 id="3-Bonus-Course-Wiki-Lecture-Notes"><a href="#3-Bonus-Course-Wiki-Lecture-Notes" class="headerlink" title="3. Bonus: Course Wiki Lecture Notes"></a>3. Bonus: Course Wiki Lecture Notes</h3><p><strong>单一变量线性回归</strong>：即通过线性方程来拟合已知data的feature，与label之间的关系。表达为<br><img src="https://i.imgur.com/zMe7YGr.png" alt=""></p><p>不同的线性方程之间，需要挑出来一个拟合更适合的，这就需要一个判断标准。常用的基于<strong>Mean squared error (MSE)</strong>：<br><img src="https://i.imgur.com/adK8AA2.png" alt=""></p><p>如果让计算机来实现这个过程，怎么办呢？<strong>梯度下降法（Gradient Descent）</strong>是一种常见手段。<br>该方法的核心是：<br><img src="https://i.imgur.com/i2kZYPl.png" alt=""></p><p>其中α是下降步长，人为确定。可以看出来，梯度下降法的目的是使J(θi,θj)越来越小，对于其是否线性，参数的数量，都没有限制。</p><p>最后利用梯度下降法代入导数（不论是链式法则，还是单个变量代入）得到最优的拟合函数参数：<br><img src="https://i.imgur.com/VBHlmsF.jpg" alt=""></p><h3 id="4-Model-and-Cost-Function"><a href="#4-Model-and-Cost-Function" class="headerlink" title="4. Model and Cost Function"></a>4. Model and Cost Function</h3><p>再次回顾一下监督学习，两类：<strong>regression和classification</strong>。<br>简单来说，<strong>regression是predict real-valued output，classification是predict discrete-valued output。</strong><br>注明了以下符号，方便以后沟通：<br><img src="https://i.imgur.com/0Lr7GzM.png" alt=""></p><p>我们设计拟合，目的就是为了使得表示拟合数据与已知真实数据直接差距的cost function最小，这个cost function是啥？就是我们之前提到的判断标准，常见的为：<br><img src="https://i.imgur.com/GiYdN6C.png" alt=""><br>当有两个未知量时，cost function就需要表示为三维图：<br><img src="https://i.imgur.com/DJLSXXG.jpg" alt=""></p><p>为了方便表示，也可以把上图表示为类似地理上的“等高线”。术语称为 <strong>contour plot（轮廓图）</strong>：<br><img src="https://i.imgur.com/ZbnTLxm.jpg" alt=""></p><h3 id="5-Parameter-Learning"><a href="#5-Parameter-Learning" class="headerlink" title="5. Parameter Learning"></a>5. Parameter Learning</h3><p>现在，我们开始具体来看看，第一个机器学习算法：<strong>梯度下降法（gradient descent）</strong></p><p><img src="https://i.imgur.com/SAMHpSX.png" alt=""><br><strong>特别注意</strong>，这里的temp是为了保证更新是同一时间发生的，这才是最正宗的梯度下降法。如果先更新了θ0，再用更新之后的θ0去更新θ1，就违背了梯度下降法的初衷。</p><p>可能你已经发现，<strong>这里的最优值，仅仅是局部的（local）而不是全局的（global）</strong>。因此选取的初值不同，可能会导致算法停留在不同的最优值。因此，这是梯度下降法的一个缺陷。但是对于线性回归而言，cost function 是一个convex function（形状为弓形），仅有一个最优值，局部最优值就是全局最优值。</p><p>到此为止，我们说的其实都是：<strong>batch</strong> gradient descent。也就是我的cost function是所有样本的MSE之和。如果不是计算所有样本，仅仅是计算某个重要子集的MSE之和，这个方法在以后的课程中会提到。</p><p>最后，提到线性代数中的寻找极值的方法：<strong>normal equations</strong>。但是在大规模的数据计算中，还是梯度下降法更为适用。</p><h3 id="6-Linear-Algebra-Review"><a href="#6-Linear-Algebra-Review" class="headerlink" title="6. Linear Algebra Review"></a>6. Linear Algebra Review</h3><p>矩阵：<a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Matrix_(mathematics)</a></p><p>向量：特殊的矩阵，注意是 列数 = 1</p><p>1-index：下标从1开始。0-index：下标从0开始。</p><p>矩阵乘法没有交换律，但是有结合律。</p><p>只有方阵（#col=#row）才有逆矩阵。但仅仅满足方阵这一个条件并不足够，同时需要满足行列式不等于0。没有逆矩阵的矩阵，称之为“奇异（singular）矩阵”或“退化（degenerate）矩阵”。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。&lt;br&gt;课程网址：&lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.coursera.org/learn/machine-learning/home/welcome&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning 课程笔记" scheme="https://paradoxallen.github.io/tags/Machine-Learning-%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(5) 常见算法优缺点</title>
    <link href="https://paradoxallen.github.io/65434/"/>
    <id>https://paradoxallen.github.io/65434/</id>
    <published>2017-06-09T16:00:00.000Z</published>
    <updated>2018-06-13T05:09:21.235Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p><a href="https://paradoxallen.github.io/62602/">机器学习(三) 模型结果应用</a></p><p><a href="https://paradoxallen.github.io/21484/">机器学习(四) 常见算法优缺点</a></p><p>文章结构：</p><ul><li><p><strong>什么是感知器分类算法</strong></p></li><li><p><strong>在Python中实现感知器学习算法</strong></p></li></ul><p><em>在iris（鸢尾花）数据集上训练一个感知器模型</em></p><ul><li><strong>自适应线性神经元和融合学习</strong></li></ul><p><em>使用梯度下降方法来最小化损失函数</em></p><p><em>在Python中实现一个自适应的线性神经元</em></p><a id="more"></a><hr><h3 id="什么是感知器分类算法"><a href="#什么是感知器分类算法" class="headerlink" title="什么是感知器分类算法"></a><strong>什么是感知器分类算法</strong></h3><p>设想我们改变逻辑回归算法，“迫使”它只能输出-1或1抑或其他定值。在这种情况下，之前的逻辑函数‍‍g就会变成阈值函数sign：</p><p><img src="https://i.imgur.com/TwrMWwh.png" alt=""></p><p><img src="https://i.imgur.com/pDzWDxS.png" alt=""></p><p>如果我们令假设为hθ(x)=g(θTx)hθ(x)=g(θTx)，将其带入之前的迭代法中：</p><p><img src="https://i.imgur.com/r4P3819.png" alt=""></p><p>至此我们就得出了感知器学习算法。简单地来说，感知器学习算法是神经网络中的一个概念，单层感知器是最简单的神经网络，输入层和输出层直接相连。</p><p><img src="https://i.imgur.com/Nb3JtYy.png" alt=""></p><p>每一个输入端和其上的权值相乘，然后将这些乘积相加得到乘积和，这个结果与阈值相比较（一般为0），若大于阈值输出端就取1，反之，输出端取-1。</p><p>初始权重向量W=[0,0,0]，更新公式W(i)=W(i)+ΔW(i)；ΔW(i)=η<em>(y-y’)</em>X(i)； </p><p>η：学习率，介于[0,1]之间 </p><p>y：输入样本的正确分类 </p><p>y’：感知器计算出来的分类 </p><p>通过上面公式不断更新权值，直到达到分类要求。</p><p><img src="https://i.imgur.com/RlHERhT.jpg" alt=""></p><p>初始化权重向量W，与输入向量做点乘，将结果与阈值作比较，得到分类结果1或-1。</p><hr><h3 id="在Python中实现感知器学习算法"><a href="#在Python中实现感知器学习算法" class="headerlink" title="在Python中实现感知器学习算法"></a><strong>在Python中实现感知器学习算法</strong></h3><p>下面直接贴上实现代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Perceptron(object):</span><br><span class="line">    &quot;&quot;&quot;Perceptron classifier.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ------------</span><br><span class="line">    eta : float</span><br><span class="line">        Learning rate (between 0.0 and 1.0)</span><br><span class="line">    n_iter : int</span><br><span class="line">        Passes over the training dataset.</span><br><span class="line"></span><br><span class="line">    Attributes</span><br><span class="line">    -----------</span><br><span class="line">    w_ : 1d-array</span><br><span class="line">        Weights after fitting.</span><br><span class="line">    errors_ : list</span><br><span class="line">        Number of misclassifications (updates) in each epoch.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, eta=0.01, n_iter=10):</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y):</span><br><span class="line">        &quot;&quot;&quot;Fit training data.</span><br><span class="line"></span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span><br><span class="line">            Training vectors, where n_samples is the number of samples and</span><br><span class="line">            n_features is the number of features.</span><br><span class="line">        y : array-like, shape = [n_samples]</span><br><span class="line">            Target values.</span><br><span class="line"></span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        self : object</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.w_ = np.zeros(1 + X.shape[1])</span><br><span class="line">        self.errors_ = []</span><br><span class="line"></span><br><span class="line">        for _ in range(self.n_iter):</span><br><span class="line">            errors = 0</span><br><span class="line">            for xi, target in zip(X, y):</span><br><span class="line">                update = self.eta * (target - self.predict(xi))</span><br><span class="line">                self.w_[1:] += update * xi</span><br><span class="line">                self.w_[0] += update</span><br><span class="line">                errors += int(update != 0.0)</span><br><span class="line">            self.errors_.append(errors)</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def net_input(self, X):</span><br><span class="line">        &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span><br><span class="line">        return np.dot(X, self.w_[1:]) + self.w_[0]</span><br><span class="line"></span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span><br><span class="line">        return np.where(self.net_input(X) &gt;= 0.0, 1, -1)</span><br></pre></td></tr></table></figure><p><strong>特别说明：</strong></p><p>学习速率η(eta)只有在权重（一般取值0或者很小的数）为非零值的时候，才会对分类结果产生作用。如果所有的权重都初始化为0，学习速率参数eta只影响权重向量的大小，而不影响其方向，为了使学习速率影响分类结果，权重需要初始化为非零值。需要更改的代码中的相应行在下面突出显示:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, eta=0.01, n_iter=50, random_seed=1): # add random_seed=1</span><br><span class="line">    ...</span><br><span class="line">    self.random_seed = random_seed # add this line</span><br><span class="line">def fit(self, X, y):</span><br><span class="line">    ...</span><br><span class="line">    # self.w_ = np.zeros(1 + X.shape[1]) ## remove this line</span><br><span class="line">    rgen = np.random.RandomState(self.random_seed) # add this line</span><br><span class="line">    self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1]) # add this line</span><br></pre></td></tr></table></figure></p><p><strong>在iris（鸢尾）数据集上训练一个感知器模型</strong></p><p><strong>读取iris数据集</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import collections</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(&apos;https://archive.ics.uci.edu/ml/&apos;</span><br><span class="line">        &apos;machine-learning-databases/iris/iris.data&apos;, header=None)</span><br><span class="line">print (df.head())</span><br><span class="line">print (&quot;\n&quot;)</span><br><span class="line">print (df.describe())</span><br><span class="line">print (&quot;\n&quot;)</span><br><span class="line">print (collections.Counter(df[4]))</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/tRDUNXi.jpg" alt=""></p><p><strong>可视化iris数据</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 为了显示中文(这里是Mac的解决方法，其他的大家可以去百度一下)</span><br><span class="line">from matplotlib.font_manager import FontProperties</span><br><span class="line">font = FontProperties(fname=&apos;/System/Library/Fonts/STHeiti Light.ttc&apos;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 选择 setosa and versicolor类型的花</span><br><span class="line">y = df.iloc[0:100, 4].values</span><br><span class="line">y = np.where(y == &apos;Iris-setosa&apos;, -1, 1)</span><br><span class="line"></span><br><span class="line"># 提取它们的特征 （sepal length and petal length）</span><br><span class="line">X = df.iloc[0:100, [0, 2]].values</span><br><span class="line"></span><br><span class="line"># 可视化数据，因为数据有经过处理，总共150行数据，1-50行是setosa花，51-100是versicolor花，101-150是virginica花</span><br><span class="line">plt.scatter(X[:50, 0], X[:50, 1],</span><br><span class="line">            color=&apos;red&apos;, marker=&apos;o&apos;, label=&apos;setosa&apos;)</span><br><span class="line">plt.scatter(X[50:100, 0], X[50:100, 1],</span><br><span class="line">            color=&apos;blue&apos;, marker=&apos;x&apos;, label=&apos;versicolor&apos;)</span><br><span class="line"></span><br><span class="line">plt.xlabel(&apos;sepal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;petal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/wR17A5s.png" alt=""></p><p><strong>训练感知器模型</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Perceptron是我们前面定义的感知器算法函数，这里就直接调用就好</span><br><span class="line">ppn = Perceptron(eta=0.1, n_iter=10)</span><br><span class="line"></span><br><span class="line">ppn.fit(X, y)</span><br><span class="line"></span><br><span class="line">plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=&apos;o&apos;)</span><br><span class="line">plt.xlabel(&apos;迭代次数&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;权重更新次数（错误次数）&apos;,FontProperties=font,fontsize=14)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/e6o2LBT.png" alt=""></p><p><strong>绘制函数决策区域</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib.colors import ListedColormap</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot_decision_regions(X, y, classifier, resolution=0.02):</span><br><span class="line"></span><br><span class="line">    # setup marker generator and color map</span><br><span class="line">    markers = (&apos;s&apos;, &apos;x&apos;, &apos;o&apos;, &apos;^&apos;, &apos;v&apos;)</span><br><span class="line">    colors = (&apos;red&apos;, &apos;blue&apos;, &apos;lightgreen&apos;, &apos;gray&apos;, &apos;cyan&apos;)</span><br><span class="line">    cmap = ListedColormap(colors[:len(np.unique(y))])</span><br><span class="line"></span><br><span class="line">    # plot the decision surface</span><br><span class="line">    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),</span><br><span class="line">                           np.arange(x2_min, x2_max, resolution))</span><br><span class="line">    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)</span><br><span class="line">    Z = Z.reshape(xx1.shape)</span><br><span class="line">    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)</span><br><span class="line">    plt.xlim(xx1.min(), xx1.max())</span><br><span class="line">    plt.ylim(xx2.min(), xx2.max())</span><br><span class="line"></span><br><span class="line">    # plot class samples</span><br><span class="line">    for idx, cl in enumerate(np.unique(y)):</span><br><span class="line">        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],</span><br><span class="line">                    alpha=0.8, c=cmap(idx),</span><br><span class="line">                    edgecolor=&apos;black&apos;,</span><br><span class="line">                    marker=markers[idx], </span><br><span class="line">                    label=cl)</span><br><span class="line">plot_decision_regions(X, y, classifier=ppn)</span><br><span class="line">plt.xlabel(&apos;sepal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.ylabel(&apos;petal 长度 [cm]&apos;,FontProperties=font,fontsize=14)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/AFltDVw.png" alt=""></p><hr><h3 id="自适应线性神经元和融合学习"><a href="#自适应线性神经元和融合学习" class="headerlink" title="自适应线性神经元和融合学习"></a><strong>自适应线性神经元和融合学习</strong></h3><p><strong>使用梯度下降方法来最小化损失函数</strong></p><p>梯度下降的方法十分常见，具体的了解可以参考附录的文章[2]，如今，梯度下降主要用于在神经网络模型中进行权重更新，即在一个方向上更新和调整模型的参数，来最小化损失函数。</p><p><img src="https://i.imgur.com/pYoV9cF.jpg" alt=""><br>图：梯度下降原理过程演示</p><p><strong>在Python中实现一个自适应的线性神经元</strong></p><p>先贴上定义的python函数，<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"># 定义神经元函数</span><br><span class="line">class AdalineGD(object):</span><br><span class="line">    &quot;&quot;&quot;ADAptive LInear NEuron classifier.</span><br><span class="line"></span><br><span class="line">    Parameters</span><br><span class="line">    ------------</span><br><span class="line">    eta : float</span><br><span class="line">        Learning rate (between 0.0 and 1.0)</span><br><span class="line">    n_iter : int</span><br><span class="line">        Passes over the training dataset.</span><br><span class="line"></span><br><span class="line">    Attributes</span><br><span class="line">    -----------</span><br><span class="line">    w_ : 1d-array</span><br><span class="line">        Weights after fitting.</span><br><span class="line">    cost_ : list</span><br><span class="line">        Sum-of-squares cost function value in each epoch.</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, eta=0.01, n_iter=50):</span><br><span class="line">        self.eta = eta</span><br><span class="line">        self.n_iter = n_iter</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y):</span><br><span class="line">        &quot;&quot;&quot; Fit training data.</span><br><span class="line"></span><br><span class="line">        Parameters</span><br><span class="line">        ----------</span><br><span class="line">        X : &#123;array-like&#125;, shape = [n_samples, n_features]</span><br><span class="line">            Training vectors, where n_samples is the number of samples and</span><br><span class="line">            n_features is the number of features.</span><br><span class="line">        y : array-like, shape = [n_samples]</span><br><span class="line">            Target values.</span><br><span class="line"></span><br><span class="line">        Returns</span><br><span class="line">        -------</span><br><span class="line">        self : object</span><br><span class="line"></span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.w_ = np.zeros(1 + X.shape[1])</span><br><span class="line">        self.cost_ = []</span><br><span class="line"></span><br><span class="line">        for i in range(self.n_iter):</span><br><span class="line">            net_input = self.net_input(X)</span><br><span class="line">            # Please note that the &quot;activation&quot; method has no effect</span><br><span class="line">            # in the code since it is simply an identity function. We</span><br><span class="line">            # could write `output = self.net_input(X)` directly instead.</span><br><span class="line">            # The purpose of the activation is more conceptual, i.e.,  </span><br><span class="line">            # in the case of logistic regression, we could change it to</span><br><span class="line">            # a sigmoid function to implement a logistic regression classifier.</span><br><span class="line">            output = self.activation(X)</span><br><span class="line">            errors = (y - output)</span><br><span class="line">            self.w_[1:] += self.eta * X.T.dot(errors)</span><br><span class="line">            self.w_[0] += self.eta * errors.sum()</span><br><span class="line">            cost = (errors**2).sum() / 2.0</span><br><span class="line">            self.cost_.append(cost)</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def net_input(self, X):</span><br><span class="line">        &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;</span><br><span class="line">        return np.dot(X, self.w_[1:]) + self.w_[0]</span><br><span class="line"></span><br><span class="line">    def activation(self, X):</span><br><span class="line">        &quot;&quot;&quot;Compute linear activation&quot;&quot;&quot;</span><br><span class="line">        return self.net_input(X)</span><br><span class="line"></span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span><br><span class="line">        return np.where(self.activation(X) &gt;= 0.0, 1, -1)</span><br></pre></td></tr></table></figure></p><p><strong>查看不同学习率下的错误率随迭代次数的变化情况：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))</span><br><span class="line"></span><br><span class="line"># 可视化W调整的过程中，错误率随迭代次数的变化</span><br><span class="line">ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)</span><br><span class="line">ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker=&apos;o&apos;)</span><br><span class="line">ax[0].set_xlabel(&apos;Epochs&apos;)</span><br><span class="line">ax[0].set_ylabel(&apos;log(Sum-squared-error)&apos;)</span><br><span class="line">ax[0].set_title(&apos;Adaline - Learning rate 0.01&apos;)</span><br><span class="line"></span><br><span class="line">ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)</span><br><span class="line">ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker=&apos;o&apos;)</span><br><span class="line">ax[1].set_xlabel(&apos;Epochs&apos;)</span><br><span class="line">ax[1].set_ylabel(&apos;Sum-squared-error&apos;)</span><br><span class="line">ax[1].set_title(&apos;Adaline - Learning rate 0.0001&apos;)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/g6mKKU3.png" alt=""></p><p><strong>iris数据的应用情况：</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 标准化特征</span><br><span class="line">X_std = np.copy(X)</span><br><span class="line">X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()</span><br><span class="line">X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()</span><br><span class="line"># 调用函数开始训练</span><br><span class="line">ada = AdalineGD(n_iter=15, eta=0.01)</span><br><span class="line">ada.fit(X_std, y)</span><br><span class="line"># 绘制效果</span><br><span class="line">plot_decision_regions(X_std, y, classifier=ada)</span><br><span class="line">plt.title(&apos;Adaline - Gradient Descent&apos;)</span><br><span class="line">plt.xlabel(&apos;sepal length [standardized]&apos;)</span><br><span class="line">plt.ylabel(&apos;petal length [standardized]&apos;)</span><br><span class="line">plt.legend(loc=&apos;upper left&apos;)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"># 可视化W调整的过程中，错误率随迭代次数的变化</span><br><span class="line">plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=&apos;o&apos;)</span><br><span class="line">plt.xlabel(&apos;Epochs&apos;)</span><br><span class="line">plt.ylabel(&apos;Sum-squared-error&apos;)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>output：</p><p><img src="https://i.imgur.com/kRorVXJ.png" alt=""></p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>1）<a href="https://blog.csdn.net/u013719780/article/details/51755409" target="_blank" rel="noopener">机器学习系列：感知器</a><br>2）<a href="https://blog.csdn.net/zyq522376829/article/details/66632699" target="_blank" rel="noopener">机器学习入门系列04，Gradient Descent（梯度下降法）</a><br>3）<a href="https://zhuanlan.zhihu.com/p/27449596?utm_source=weibo&amp;utm_medium=social" target="_blank" rel="noopener">一文看懂各种神经网络优化算法：从梯度下降到Adam方法</a><br>4）<a href="https://blog.csdn.net/huakai16/article/details/77701020" target="_blank" rel="noopener">机器学习与神经网络（三）：自适应线性神经元的介绍和Python代码实现</a><br>5）<a href="http://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/ch02/ch02.ipynb" target="_blank" rel="noopener">《Training Machine Learning Algorithms for Classification》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/62602/&quot;&gt;机器学习(三) 模型结果应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/21484/&quot;&gt;机器学习(四) 常见算法优缺点&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;文章结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;什么是感知器分类算法&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;在Python中实现感知器学习算法&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;在iris（鸢尾花）数据集上训练一个感知器模型&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;自适应线性神经元和融合学习&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;使用梯度下降方法来最小化损失函数&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;在Python中实现一个自适应的线性神经元&lt;/em&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="算法" scheme="https://paradoxallen.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(4) 常见算法优缺点</title>
    <link href="https://paradoxallen.github.io/21484/"/>
    <id>https://paradoxallen.github.io/21484/</id>
    <published>2017-06-04T16:00:00.000Z</published>
    <updated>2018-06-13T05:09:08.547Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p><a href="https://paradoxallen.github.io/62602/">机器学习(三) 模型结果应用</a></p><p>机器学习算法我们了解了很多，但是放在一起来比较优缺点是缺少的，本篇文章就一些常见的算法来进行一次优缺点梳理。</p><a id="more"></a><hr><h3 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a><strong>决策树算法</strong></h3><h4 id="一、决策树优点"><a href="#一、决策树优点" class="headerlink" title="一、决策树优点"></a><strong>一、决策树优点</strong></h4><p>1、决策树易于理解和解释，可以可视化分析，容易提取出规则。</p><p>2、可以同时处理标称型和数值型数据。</p><p>3、测试数据集时，运行速度比较快。</p><p>4、决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。</p><h4 id="二、决策树缺点"><a href="#二、决策树缺点" class="headerlink" title="二、决策树缺点"></a><strong>二、决策树缺点</strong></h4><p>1、对缺失数据处理比较困难。</p><p>2、容易出现过拟合问题。</p><p>3、忽略数据集中属性的相互关联。</p><p>4、ID3算法计算信息增益时结果偏向数值比较多的特征。</p><h4 id="三、改进措施"><a href="#三、改进措施" class="headerlink" title="三、改进措施"></a><strong>三、改进措施</strong></h4><p>1、对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。</p><p>2、使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题</p><h4 id="四、常见算法"><a href="#四、常见算法" class="headerlink" title="四、常见算法"></a><strong>四、常见算法</strong></h4><h5 id="一）C4-5算法"><a href="#一）C4-5算法" class="headerlink" title="一）C4.5算法"></a><strong>一）C4.5算法</strong></h5><p>ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。ID3算法计算每个属性的信息增益，并选取具有最高增益的属性作为给定的测试属性。</p><p>C4.5算法核心思想是ID3算法，是ID3算法的改进，改进方面有：</p><ul><li><p>用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；</p></li><li><p>在树构造过程中进行剪枝；</p></li><li><p>能处理非离散的数据；</p></li><li><p>能处理不完整的数据。</p></li></ul><p><strong>优点</strong>：产生的分类规则易于理解，准确率较高。</p><p><strong>缺点</strong>：</p><p>1）在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效；</p><p>2）C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</p><h5 id="二）CART分类与回归树"><a href="#二）CART分类与回归树" class="headerlink" title="二）CART分类与回归树"></a><strong>二）CART分类与回归树</strong></h5><p>是一种决策树分类方法，采用基于最小距离的基尼指数估计函数，用来决定由该子数<br>据集生成的决策树的拓展形。如果目标变量是标称的，称为分类树；如果目标变量是连续的，称为回归树。分类树是使用树结构算法将数据分成离散类的方法。</p><p><strong>优点</strong></p><p>1）非常灵活，可以允许有部分错分成本，还可指定先验概率分布，可使用自动的成本复杂性剪枝来得到归纳性更强的树。</p><p>2）在面对诸如存在缺失值、变量数多等问题时CART 显得非常稳健。</p><hr><h3 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a><strong>分类算法</strong></h3><h4 id="一、KNN算法"><a href="#一、KNN算法" class="headerlink" title="一、KNN算法"></a><strong>一、KNN算法</strong></h4><p><strong>KNN算法的优点</strong> </p><p>1、KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练</p><p>2、KNN理论简单，容易实现</p><p><strong>KNN算法的缺点</strong></p><p>1、对于样本容量大的数据集计算量比较大。</p><p>2、样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。</p><p>3、KNN每一次分类都会重新进行一次全局运算。</p><p>4、k值大小的选择。</p><p><strong>KNN算法应用领域</strong></p><p>文本分类、模式识别、聚类分析，多分类领域</p><h4 id="二、支持向量机（SVM）"><a href="#二、支持向量机（SVM）" class="headerlink" title="二、支持向量机（SVM）"></a><strong>二、支持向量机（SVM）</strong></h4><p>支持向量机是一种基于分类边界的方法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。基于分类边界的分类算法的目标是，通过训练，找到这些分类之间的边界（直线的――称为线性划分，曲线的――称为非线性划分）。对于多维数据（如N维），可以将它们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面（超面比N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。</p><p>支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。</p><p><strong>SVM优点</strong></p><p>1、解决小样本下机器学习问题。<br>2、解决非线性问题。<br>3、无局部极小值问题。（相对于神经网络等算法）<br>4、可以很好的处理高维数据集。<br>5、泛化能力比较强。</p><p><strong>SVM缺点</strong></p><p>1、对于核函数的高维映射解释力不强，尤其是径向基函数。<br>2、对缺失数据敏感。</p><p><strong>SVM应用领域</strong></p><p>文本分类、图像识别、主要二分类领域</p><h4 id="三、朴素贝叶斯算法"><a href="#三、朴素贝叶斯算法" class="headerlink" title="三、朴素贝叶斯算法"></a><strong>三、朴素贝叶斯算法</strong></h4><p><strong>朴素贝叶斯算法优点</strong></p><p>1、对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已。<br>2、支持增量式运算。即可以实时的对新增的样本进行训练。<br>3、朴素贝叶斯对结果解释容易理解。</p><p><strong>朴素贝叶斯缺点</strong></p><p>1、由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。</p><p><strong>朴素贝叶斯应用领域</strong></p><p>文本分类、欺诈检测中使用较多</p><h4 id="四、Logistic回归算法"><a href="#四、Logistic回归算法" class="headerlink" title="四、Logistic回归算法"></a><strong>四、Logistic回归算法</strong></h4><p><strong>logistic回归优点</strong></p><p>1、计算代价不高，易于理解和实现</p><p><strong>logistic回归缺点</strong></p><p>1、容易产生欠拟合。</p><p>2、分类精度不高。</p><p><strong>logistic回归应用领域</strong></p><p>用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等。</p><p>Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等。</p><hr><h3 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a><strong>聚类算法</strong></h3><h4 id="一、K-means-算法"><a href="#一、K-means-算法" class="headerlink" title="一、K means 算法"></a><strong>一、K means 算法</strong></h4><p>是一个简单的聚类算法，把n的对象根据他们的属性分为k个分割，k&lt; n。 算法的核心就是要优化失真函数J,使其收敛到局部最小值但不是全局最小值。<br>其中N为样本数，K是簇数，rnk b表示n属于第k个簇，uk 是第k个中心点的值。然后求出最优的uk</p><p><strong>优点</strong>：算法速度很快</p><p><strong>缺点</strong>：分组的数目k是一个输入参数，不合适的k可能返回较差的结果。</p><h4 id="二、EM最大期望算法"><a href="#二、EM最大期望算法" class="headerlink" title="二、EM最大期望算法"></a><strong>二、EM最大期望算法</strong></h4><p>EM算法是基于模型的聚类方法，是在概率模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量。E步估计隐含变量，M步估计其他参数，交替将极值推向最大。</p><p>EM算法比K-means算法计算复杂，收敛也较慢，不适于大规模数据集和高维数据，但比K-means算法计算结果稳定、准确。EM经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。</p><hr><h3 id="集成算法（AdaBoost算法）"><a href="#集成算法（AdaBoost算法）" class="headerlink" title="集成算法（AdaBoost算法）"></a><strong>集成算法（AdaBoost算法）</strong></h3><h4 id="一、-AdaBoost算法优点"><a href="#一、-AdaBoost算法优点" class="headerlink" title="一、  AdaBoost算法优点"></a><strong>一、  AdaBoost算法优点</strong></h4><p>1、很好的利用了弱分类器进行级联。</p><p>2、可以将不同的分类算法作为弱分类器。</p><p>3、AdaBoost具有很高的精度。</p><p>4、相对于bagging算法和Random Forest算法，AdaBoost充分考虑的每个分类器的权重。</p><h4 id="二、Adaboost算法缺点"><a href="#二、Adaboost算法缺点" class="headerlink" title="二、Adaboost算法缺点"></a><strong>二、Adaboost算法缺点</strong></h4><p>1、AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。</p><p>2、数据不平衡导致分类精度下降。</p><p>3、训练比较耗时，每次重新选择当前分类器最好切分点。</p><h4 id="三、AdaBoost应用领域"><a href="#三、AdaBoost应用领域" class="headerlink" title="三、AdaBoost应用领域"></a><strong>三、AdaBoost应用领域</strong></h4><p>模式识别、计算机视觉领域，用于二分类和多分类场景</p><hr><h3 id="人工神经网络算法"><a href="#人工神经网络算法" class="headerlink" title="人工神经网络算法"></a><strong>人工神经网络算法</strong></h3><h4 id="一、神经网络优点"><a href="#一、神经网络优点" class="headerlink" title="一、神经网络优点"></a><strong>一、神经网络优点</strong></h4><p>1、分类准确度高，学习能力极强。</p><p>2、对噪声数据鲁棒性和容错性较强。</p><p>3、有联想能力，能逼近任意非线性关系。</p><h4 id="二、神经网络缺点"><a href="#二、神经网络缺点" class="headerlink" title="二、神经网络缺点"></a><strong>二、神经网络缺点</strong></h4><p>1、神经网络参数较多，权值和阈值。</p><p>2、黑盒过程，不能观察中间结果。</p><p>3、学习过程比较长，有可能陷入局部极小值。</p><h4 id="三、人工神经网络应用领域"><a href="#三、人工神经网络应用领域" class="headerlink" title="三、人工神经网络应用领域"></a><strong>三、人工神经网络应用领域</strong></h4><p>目前深度神经网络已经应用与计算机视觉，自然语言处理，语音识别等领域并取得很好的效果。</p><hr><h3 id="排序算法（PageRank）"><a href="#排序算法（PageRank）" class="headerlink" title="排序算法（PageRank）"></a><strong>排序算法（PageRank）</strong></h3><p>PageRank是google的页面排序算法，是基于从许多优质的网页链接过来的网页，必定还是优质网页的回归关系，来判定所有网页的重要性。（也就是说，一个人有着越多牛X朋友的人，他是牛X的概率就越大。）</p><h4 id="一、PageRank优点"><a href="#一、PageRank优点" class="headerlink" title="一、PageRank优点"></a><strong>一、PageRank优点</strong></h4><p>完全独立于查询，只依赖于网页链接结构，可以离线计算。</p><h4 id="二、PageRank缺点"><a href="#二、PageRank缺点" class="headerlink" title="二、PageRank缺点"></a><strong>二、PageRank缺点</strong></h4><p>1）PageRank算法忽略了网页搜索的时效性。</p><p>2）旧网页排序很高，存在时间长，积累了大量的in-links，拥有最新资讯的新网页排名却很低，因为它们几乎没有in-links。</p><hr><h3 id="关联规则算法（Apriori算法）"><a href="#关联规则算法（Apriori算法）" class="headerlink" title="关联规则算法（Apriori算法）"></a><strong>关联规则算法（Apriori算法）</strong></h3><p>Apriori算法是一种挖掘关联规则的算法，用于挖掘其内含的、未知的却又实际存在的数据关系，其核心是基于两阶段频集思想的递推算法 。</p><p><strong>Apriori算法分为两个阶段：</strong></p><p>1）寻找频繁项集</p><p>2）由频繁项集找关联规则</p><p><strong>算法缺点：</strong></p><p>1）在每一步产生侯选项目集时循环产生的组合过多，没有排除不应该参与组合的元素；</p><p>2） 每次计算项集的支持度时，都对数据库中    的全部记录进行了一遍扫描比较，需要很大的I/O负载。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1）Jason Brownlee  《How To Use Machine Learning Results》</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/62602/&quot;&gt;机器学习(三) 模型结果应用&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;机器学习算法我们了解了很多，但是放在一起来比较优缺点是缺少的，本篇文章就一些常见的算法来进行一次优缺点梳理。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="算法" scheme="https://paradoxallen.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Tour of Machine Learning Algorithms(3) 模型结果应用</title>
    <link href="https://paradoxallen.github.io/62602/"/>
    <id>https://paradoxallen.github.io/62602/</id>
    <published>2017-05-31T16:00:00.000Z</published>
    <updated>2018-06-13T05:08:51.908Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前文传送"><a href="#前文传送" class="headerlink" title="前文传送"></a>前文传送</h4><p><a href="https://paradoxallen.github.io/9731/">机器学习(一) 算法介绍</a></p><p><a href="https://paradoxallen.github.io/9731/">机器学习(二) 模型调优</a></p><p>当你有了一个相当不错的模型结果了，这个时间就需要上线应用了，但实际上这个过程也是需要注意很多东西的呢，比如汇报你的项目结果、上线计划沟通、上线后的监控等等，这都是相当重要的。</p><a id="more"></a><hr><p>永远要记得，建立模型只是为了解决业务问题，<strong>模型只是一个工具而已</strong>，所以，脱离具体业务场景的模型都是假的。所以，在一开始，就要对自己的目标进行明确，在做完了模型后再度审视自己目标，看下自己做出来的模型是否仍是解决这个目标。</p><p>根据你试图解决的问题类型，我们可以大致分为两种呈现方式：</p><ul><li><p><strong>报告汇报式</strong></p></li><li><p><strong>部署上线式</strong></p></li></ul><p>当然了，实际上更多的是两种方式的融合，即两个都需要做，那么下面我们就分别来说一下这两种方式在实际操作上都需要做些什么呗。</p><h2 id="报告汇报式"><a href="#报告汇报式" class="headerlink" title="报告汇报式"></a>报告汇报式</h2><p>一旦你发现了一个很不错的模型，并且训练的结果也很不错，你此时就需要总结这一切内容，并很好的展示给你的观众（可以是老板、客户或者是同事等），而此时如何完美地展示显得格外重要。</p><p>展示的最好方式我个人觉得是ppt，但有些地方更偏好于单页报告，不过也不影响，下面罗列的内容，都是可以在这两种方式内容上展示的，不管你现在做的模型是什么，比赛的、教程的，还是工作的，都可以试着去总结这些关键点，完成一次报告的撰写。</p><ul><li><p><strong><em>Context</em></strong> (Why): 定义问题存在的大背景，并且说明研究的动机或目的。</p></li><li><p><strong><em>Problem</em></strong> (Question): 简单扼要地把问题描述一个具体需要解决的问题并回答它。</p></li><li><p><strong><em>Solution</em></strong> (Answer): 简单扼要地描述关于上一个环节提出的问题的解决方案，而且要详细具体。</p></li><li><p><strong><em>Findings</em></strong>: 罗列一下你在建模过程中发现的一些有价值的点，比如在数据上的发现，又或者是原先方式的缺点及现有方式的优点，也可以是模型在性能方面的优势等等。</p></li><li><p><strong><em>Limitations</em></strong>: 考虑模型能力所不能覆盖的点，或者是方案不能解决的点。不要回避这些问题，你不说别人也会问，而且，只有你重新认识模型的短处，才能知道模型的优点。</p></li><li><p><strong><em>Conclusions</em></strong> (Why+Question+Answer): 回顾目的、研究的问题及解决方案，并将它们尽可能压缩在几句话，让人能够记住。</p></li></ul><p>如果是自己平时做练习的项目，我觉得可以多按照上面的点来描述自己的项目结果，并且将报告上传到社区网络，让更多的人来评价，你从中也可以得到更多的反馈，这对你下一次的报告有很大的帮助。</p><h2 id="部署上线式"><a href="#部署上线式" class="headerlink" title="部署上线式"></a>部署上线式</h2><p>同样的，你有一个训练得很不错的模型，这时候需要将它部署到生产系统中，你需要确定很多东西，比如调用的环节、入参出参以及各种接口开发，下面有3个方面的内容需要在做这些事情之前进行考虑，分别是：<strong>算法实现、模型自动化测试、模型效果追踪</strong>。</p><p><strong>1）算法实现</strong></p><p>其实python里有很多算法都是可以直接通过库来调用的，但这对于一般情况下是很好用的，但是如果涉及到要具体部署应用，这要考虑的东西就多了。</p><p>在你考虑部署一个新模型在现有的生产系统上，你需要非常仔细地研究这可能需要产生的依赖项和“技术负债”（这里可以理解为一些所需的技术，包括硬软件）。所以，在建模前，需要考虑去查找能匹配你的方法的公司生产级别的库，要不然，等到要上线的时候，你就需要重复模型调优的过程了哦。</p><p><strong>2）模型自动化测试</strong></p><p>编写自动化测试代码，对模型的应用进行验证，监控在实际的使用过程中，并且能够重复实现模型效果的最低水平，尽可能是可以对不同的数据都可以随机性地测试。</p><p><strong>3）模型效果追踪</strong></p><p>增添一些基础设施来监控模型的性能，并且可以在精度低于最低水平的时候发出警报，追踪模型实时或者离线的数据样本的效果，包括入参。当你发现不仅仅是模型效果发生了很大的变化，就连入参也有很大的变化，那这个时候就需要考虑模型的更新或者重构了。</p><p>当然，有一些模型是可以实现在线自我学习并且更新自己的，但并不是所有的生产系统可以支持这种操作，毕竟这种还只是一个比较先进的办法，仍存在很多不太完善的地方。比较传统的方式还是对现有的模型进行人工管理，人工更新与切换，这样子显得更加明智而且稳健。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1）Jason Brownlee  《How To Use Machine Learning Results》</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前文传送&quot;&gt;&lt;a href=&quot;#前文传送&quot; class=&quot;headerlink&quot; title=&quot;前文传送&quot;&gt;&lt;/a&gt;前文传送&lt;/h4&gt;&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(一) 算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://paradoxallen.github.io/9731/&quot;&gt;机器学习(二) 模型调优&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;当你有了一个相当不错的模型结果了，这个时间就需要上线应用了，但实际上这个过程也是需要注意很多东西的呢，比如汇报你的项目结果、上线计划沟通、上线后的监控等等，这都是相当重要的。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习" scheme="https://paradoxallen.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="https://paradoxallen.github.io/tags/Machine-Learning/"/>
    
      <category term="模型" scheme="https://paradoxallen.github.io/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
</feed>
