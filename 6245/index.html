<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Andrew Ng Machine Learning (11) Large Scale Machine Learning | RP&#39;s Blog</title>
  <meta name="author" content="LRP">
  
  <meta name="description" content="此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。课程网址：https://www.coursera.org/learn/machine-learning/home/welcome">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Andrew Ng Machine Learning (11) Large Scale Machine Learning"/>
  <meta property="og:site_name" content="RP&#39;s Blog"/>

  
    <meta property="og:image" content=""/>
  

  <link rel="alternate" href="/atom.xml" title="RP&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <!-- wumiiVerification -->
  <meta name="wumiiVerification" content="fb50a101-84fe-4ca2-91a7-ae8cf792978b" />
  <meta name="wumiiVerification" content="d73b5866-c390-4156-a4dd-51b526b5335e" />
  <!-- favicon -->
  <link rel="icon" type="image/x-icon" href="/favicon.ico">
  <!-- Font-Awesome -->
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">

</head>

<body>
  <header id="header"><div class= "header-content inner">
	<div class = "alignleft col-one">
		
			<div class='avatar'>
				<img src = "/img/default/avatar.jpg">
              </div>
		
		<div class="header-div">
		    <h1><a href="/">RP&#39;s Blog</a></h1>
		    <h2><a href="/">学习总结  思考感悟</a></h2>
		</div>
	</div>
	<div class = "alignright col-two">
		
	</div>
	<div class="clearfix"></div>
</div>

<div class= "header-nav">
	<div class='header-nav-content inner'>
		<div id="main-nav" class="alignleft">
		    		
		    		  <a href="/"><i class="fa fa-home"></i>首页</a>
		    		
		    		  <a href="/archives"><i class="fa fa-archive"></i>归档</a>
		    		
		</div>
		<div id="sub-nav" class="alignright">
		    
		      <a href="/atom.xml"><i class="fa fa-rss"></i>订阅</a>
		    
		      <a href="/about"><i class="fa fa-user"></i>关于</a>
		    
		</div>
	</div>
	<div class="clearfix"></div>
</div>
</header>
    <div id="content" class="inner">
      <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
        <div class="icon"></div>
        
        <time datetime="2017-09-28T16:00:00.000Z"><a href="/6245/">2017-09-29</a></time>
        
  
    <h1 class="title">Andrew Ng Machine Learning (11) Large Scale Machine Learning</h1>
  

    </header>

    <div class="entry">
      
        
    <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Big-Data"><span class="toc-text">1. Big Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-随机梯度下降（Stochastic-Gradient-Descent）"><span class="toc-text">2. 随机梯度下降（Stochastic Gradient Descent）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-小规模批处理（Mini-batch）"><span class="toc-text">3. 小规模批处理（Mini-batch）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Big-Data-判断收敛"><span class="toc-text">4. Big Data 判断收敛</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Online-Learning"><span class="toc-text">5. Online Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Data-Parallelism"><span class="toc-text">6. Data Parallelism</span></a></li></ol>
    </div>

        <p>此文是斯坦福大学 Andrew Ng 所开设的 Coursera 课程：Machine Learning 的课程笔记。<br>课程网址：<br><a href="https://www.coursera.org/learn/machine-learning/home/welcome" target="_blank" rel="noopener">https://www.coursera.org/learn/machine-learning/home/welcome</a></p>
<a id="more"></a>
<hr>
<h3 id="1-Big-Data"><a href="#1-Big-Data" class="headerlink" title="1. Big Data"></a>1. Big Data</h3><p>海量的数据作为训练样本，<strong>“low bias algo + big data”</strong>基本都能让算法精确度更高。但是随之而来的就是极大的计算量。</p>
<p>正如<a href="https://paradoxallen.github.io/9059/">《Machine Learning Advice》</a>中所提到的，如果算法是 high bias，那么1000个样本与100,000,000个样本效果都不会有变化</p>
<p>那如何避免一开始就陷入了 high bias 或者因为自己的选择的方法、编程的细节错误所浪费大量时间呢？</p>
<p>我们应该在海量数据中<strong>选取一部分</strong>尝试。例如，随机选择 100,000,000个样本中的1000个，使用各种学习方法选择在这1000个采样学习中表现最好的一个或几个方法，进行海量数据的学习</p>
<p>尝试之后，我们即将进行真正的海量数据学习。学习过程中，我们经常会用到梯度下降法。我们以线性回归为例（<strong>任何使用到梯度下降法的算法：逻辑回归、神经网络等，都可以举一反三适用</strong>），参数θ的update函数为：<br><img src="https://i.imgur.com/2PwCkzy.png" alt=""></p>
<p>公式推导内容见<a href="https://paradoxallen.github.io/58080/">《Linear Regression》</a></p>
<p>上式中有一处求和<img src="https://i.imgur.com/4HlhA0F.png" alt="">，面对海量数据时会消耗大量时间，而且对每一个样本都会计算一次。这种计算模式称为<strong>“批处理”(batch)</strong>，面对海量数据，显然批处理模式并不适合。以下介绍两种海量数据模型中经常使用的方法</p>
<h3 id="2-随机梯度下降（Stochastic-Gradient-Descent）"><a href="#2-随机梯度下降（Stochastic-Gradient-Descent）" class="headerlink" title="2. 随机梯度下降（Stochastic Gradient Descent）"></a>2. 随机梯度下降（Stochastic Gradient Descent）</h3><p>介绍随机梯度下降方法之前，我们引入一个新的表达式：<img src="https://i.imgur.com/xdwKU3A.png" alt=""> 这代替了原本的 J(θ) 成为新的代价函数</p>
<p>根据新的代价函数，我们会得到新的参数θ的update函数：<br><img src="https://i.imgur.com/0qISHJa.png" alt=""></p>
<p>仔细观察这个update函数，是不是发现求和过程没有了？<br>批处理中，参数的改变需要当前所有样本共同作用，达到整体最优。但是随机方法中，<strong>每次仅关注当前单个样本的最优作用。这种做法会导致最优值的反复，但是却极大提高了效率</strong></p>
<p>随机梯度下降的具体流程为：<br><img src="https://i.imgur.com/ASzT2Zw.png" alt=""></p>
<p>因为随机梯度下降会导致最优值的反复，所以有时上图中的 Repeat 会进行1~10次。而且随机梯度下降方法的收敛是随机的，只能保证逐渐向最优值徘徊，而不会真正到达最优值。例如下图：<br><img src="https://i.imgur.com/QNkfNRs.png" alt=""></p>
<p>如果我们希望达到最优值，应该怎么办呢？可以通过逐渐缩小学习步长α，以求最终的收敛范围逐渐减小到最优值。可以使用：<br><img src="https://i.imgur.com/pGRoyMs.png" alt="">来逐渐缩小学习步长</p>
<h3 id="3-小规模批处理（Mini-batch）"><a href="#3-小规模批处理（Mini-batch）" class="headerlink" title="3. 小规模批处理（Mini-batch）"></a>3. 小规模批处理（Mini-batch）</h3><p>有没有方法兼顾批处理方法，与随机梯度下降方法的优势？就是Mini-batch Gradient Descent。基本思想是：不是批处理的每次处理所有样本，也不是随机方法每次单个样本，而是每次处理b个样本（1&lt;b≪m）</p>
<p>假设有1000个样本，b=10。算法的具体执行过程为：<br><img src="https://i.imgur.com/sNBITho.png" alt=""></p>
<p>实际计算中，小规模批处理方法，可能会比随机梯度下降方法更快！因为<strong>不同组的b个样本可以进行并行计算</strong>（一次并行m个样本计算量太大）。但这种方法多引入了一个参数b，需要更多的调试，算是个缺点</p>
<h3 id="4-Big-Data-判断收敛"><a href="#4-Big-Data-判断收敛" class="headerlink" title="4. Big Data 判断收敛"></a>4. Big Data 判断收敛</h3><p>批处理方法中，如何判断算法是否收敛？绘制 “Jtrain(θ)−#iterations” 的走势图（见<a href="https://paradoxallen.github.io/58080/">《Linear Regression》</a>，Jtrain(θ)是基于所有样本的和。走势逐渐降低即为收敛</p>
<p>随机梯度下降方法中，再去计算所有样本的Jtrain(θ)显然不合适，我们转而记录<img src="https://i.imgur.com/BgFkQHn.png" alt="">。例如，每1000个样本各自的<img src="https://i.imgur.com/BgFkQHn.png" alt="">求和后平均，记录这个<strong>平均误差</strong>后再更新θ。</p>
<p>最后，绘制“平均误差”与迭代次数的关系图。因为收敛是随机的，可能出现噪声，只要<strong>保证整体趋势是逐渐下降的即为收敛</strong></p>
<p>接下来我们看一下以下几种“平均误差”与迭代次数的关系图，看看有什么问题：<br><img src="https://i.imgur.com/CmhZn1V.png" alt=""></p>
<p>左上角的图是一种收敛的情况，但是红色曲线的α更小，虽然学习速度较慢，但是却收敛到更接近最优值的区域（更小误差）</p>
<p>右上角的图是一种收敛的情况，其中红色曲线的取“平均误差”的范围更大，是5000个样本取一次，所以噪声更少更平滑</p>
<p>左下角的图，因为蓝色曲线噪声过大，看不出是否收敛，建议增大取“平均误差”的范围</p>
<p>右下角的图，抱歉，完全不收敛。建议使用更小的α再尝试，如果还是不收敛就可能需要选择其他的算法或者增加特征？</p>
<h3 id="5-Online-Learning"><a href="#5-Online-Learning" class="headerlink" title="5. Online Learning"></a>5. Online Learning</h3><p>试想一种场景：一家新闻网站不断根据不断涌入的新用户的喜好以及老用户的兴趣改变，来定义头条新闻的内容，是不是还要保存这种海量数据呢？</p>
<p>这需要极大的空间，并且每次都基于所有样本进行学习，那么学习的时间还赶不上产生新样本的时间</p>
<p>如果使用随机梯度下降方法，每次仅仅关注当前样本的梯度下降，并且<strong>同一个样本仅学习一次</strong>（上文“随机梯度下降”流程图中 Repeat 次数为1），是不是就能解决这个问题。这就是online learning</p>
<p>online learning 思想可以<strong>适用于无限的数据流</strong>，尤其适用于不断更新中的互联网大数据上的机器学习算法。</p>
<p>并且，这种方法对于新事物有着极强的适应性，因为每次更新都是基于当前样本，历史样本的作用随着迭代的进行会逐渐失去。因此非常适用于<strong>互联网大数据的实时更新</strong></p>
<h3 id="6-Data-Parallelism"><a href="#6-Data-Parallelism" class="headerlink" title="6. Data Parallelism"></a>6. Data Parallelism</h3><p>最后一部分是对于并行计算的简介。试想一下，如果我要求4亿个样本某个特征的和，仅在一台机器上求是很慢的。但是，如果我将它分散在四台机器上求解，最后使用一台机器汇总这四台机器的求解结果，是不是快了很多？除去数据传输耗费的时间，是不是快了接近4倍？其中，<strong>分散</strong>的过程我们称为<strong>Map</strong>，而<strong>汇总</strong>的过程我们称为<strong>Reduce</strong></p>
<p>不仅对于多台机器，同一台机器如果有着多个计算核（CPU），同样可以适用。形象化的示意图如下：<br><img src="https://i.imgur.com/vsnpNZi.png" alt=""><br><img src="https://i.imgur.com/TsFW2LW.png" alt=""></p>
<p>本节仅仅举了加法求和的例子。但实际情况中，只要是无数据相关性的计算，也就是前一部分的计算结果不作为后一部分的计算输入，两个部分的计算是彼此独立的，都可以并行计算</p>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/机器学习/">机器学习</a>
  </div>

        
  <div class="tags">
    <a href="/tags/Machine-Learning-课程笔记/">Machine Learning 课程笔记</a>
  </div>

        
  <div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a title="分享到QQ空间" href="#" class="bds_qzone" data-cmd="qzone"></a><a title="分享到新浪微博" href="#" class="bds_tsina" data-cmd="tsina"></a><a title="分享到腾讯微博" href="#" class="bds_tqq" data-cmd="tqq"></a><a title="分享到人人网" href="#" class="bds_renren" data-cmd="renren"></a><a title="分享到微信" href="#" class="bds_weixin" data-cmd="weixin"></a></div>
  <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["qzone","tsina","tqq","renren","weixin"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["qzone","tsina","tqq","renren","weixin"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>
<!-- 返回顶部 -->
<div id="toTop">
	<a href="#">▲</a>
	<a href="#footer">▼</a>
</div></div></div>
      <aside id="sidebar" class="alignright">
        
           <div class="search">
  <form action="/search/index.html" method="get" accept-charset="utf-8">
<!--     <input type="search" name="wd"results="0" placeholder="搜索">
    <input type="hidden" name="wd" value="site:paradoxallen.github.io"> -->
     <input type="text" id="search" class="st-default-search-input" placeholder="搜索" style="height: 100%" />
  </form>
</div> 
        
          
<div class="widget tag">
  <h3 class="title" id="categories">分类</h3>
     <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/个人随笔/">个人随笔</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客开发/">博客开发</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/应用统计/">应用统计</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数学基础/">数学基础</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据分析/">数据分析</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据挖掘/">数据挖掘</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程相关/">编程相关</a><span class="category-list-count">25</span></li></ul> 
</div>
 

        
          
<div class="widget tagcloud">
  <h3 class="title">标签</h3>
  <div class="entry">
    <a href="/tags/Excel/" style="font-size: 12.86px;">Excel</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/Machine-Learning/" style="font-size: 15.71px;">Machine Learning</a> <a href="/tags/Machine-Learning-课程笔记/" style="font-size: 18.57px;">Machine Learning 课程笔记</a> <a href="/tags/SQL/" style="font-size: 14.29px;">SQL</a> <a href="/tags/hexo/" style="font-size: 11.43px;">hexo</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/博客/" style="font-size: 11.43px;">博客</a> <a href="/tags/可视化/" style="font-size: 10px;">可视化</a> <a href="/tags/大气科学/" style="font-size: 10px;">大气科学</a> <a href="/tags/数据类型/" style="font-size: 10px;">数据类型</a> <a href="/tags/机器学习/" style="font-size: 17.14px;">机器学习</a> <a href="/tags/概率论与数理统计/" style="font-size: 10px;">概率论与数理统计</a> <a href="/tags/模型/" style="font-size: 12.86px;">模型</a> <a href="/tags/爬虫/" style="font-size: 12.86px;">爬虫</a> <a href="/tags/目录/" style="font-size: 10px;">目录</a> <a href="/tags/算法/" style="font-size: 11.43px;">算法</a> <a href="/tags/线性代数/" style="font-size: 10px;">线性代数</a> <a href="/tags/统计/" style="font-size: 10px;">统计</a> <a href="/tags/编码/" style="font-size: 10px;">编码</a> <a href="/tags/高等数学/" style="font-size: 10px;">高等数学</a>
  </div>
</div>

        
          
  <div class="widget tag">
    <h3 class="title">归档</h3>
	<ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">五月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">十一月 2017</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">十月 2017</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">九月 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a><span class="archive-list-count">4</span></li></ul>
  </div>

        
      </aside>
      <div class="clearfix"></div>
    </div>
  <footer id="footer"><div class="footer-content inner">
  <div class="alignleft">
  
    &copy; 2018 LRP
    
  </div>

  <!--
  <div class="alignright">
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, Theme
    <a href="https://github.com/pengloo53/Hexo-theme-light_cn">light_cn</a>
  </div>
  -->

  <!--
  <div>
    Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a>
  </div>
  -->
  
  <div class="clearfix"></div>
</div></footer>
  <script src="http://libs.baidu.com/jquery/2.1.1/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>

<!-- calendar widget -->


<!-- 百度统计 -->

	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?9acf0cedd48dc53be256ede5a98c2aaa";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>


<!-- fancybox -->

<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


</body>
</html>