<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Python爬虫学习———前言 | RP&#39;s Blog</title>
  <meta name="author" content="LRP">
  
  <meta name="description" content="爬虫的技术可以应用到很多生活场景中，例如，自动投票，批量下载感兴趣的文章、小说、视频，微信机器人，爬取重要的数据进行数据分析等等">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Python爬虫学习———前言"/>
  <meta property="og:site_name" content="RP&#39;s Blog"/>

  
    <meta property="og:image" content=""/>
  

  <link rel="alternate" href="/atom.xml" title="RP&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <!-- wumiiVerification -->
  <meta name="wumiiVerification" content="fb50a101-84fe-4ca2-91a7-ae8cf792978b" />
  <meta name="wumiiVerification" content="d73b5866-c390-4156-a4dd-51b526b5335e" />
  <!-- favicon -->
  <link rel="icon" type="image/x-icon" href="/favicon.ico">
  <!-- Font-Awesome -->
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">

</head>

<body>
  <header id="header"><div class= "header-content inner">
	<div class = "alignleft col-one">
		
			<div class='avatar'>
				<img src = "/img/default/avatar.jpg">
              </div>
		
		<div class="header-div">
		    <h1><a href="/">RP&#39;s Blog</a></h1>
		    <h2><a href="/">学习总结  思考感悟</a></h2>
		</div>
	</div>
	<div class = "alignright col-two">
		
	</div>
	<div class="clearfix"></div>
</div>

<div class= "header-nav">
	<div class='header-nav-content inner'>
		<div id="main-nav" class="alignleft">
		    		
		    		  <a href="/"><i class="fa fa-home"></i>首页</a>
		    		
		    		  <a href="/archives"><i class="fa fa-archive"></i>归档</a>
		    		
		</div>
		<div id="sub-nav" class="alignright">
		    
		      <a href="/atom.xml"><i class="fa fa-rss"></i>订阅</a>
		    
		      <a href="/about"><i class="fa fa-user"></i>关于</a>
		    
		</div>
	</div>
	<div class="clearfix"></div>
</div>
</header>
    <div id="content" class="inner">
      <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
        <div class="icon"></div>
        
        <time datetime="2017-07-31T16:00:00.000Z"><a href="/44178/">2017-08-01</a></time>
        
  
    <h1 class="title">Python爬虫学习———前言</h1>
  

    </header>

    <div class="entry">
      
        
    <div id="toc">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-什么是爬虫？"><span class="toc-text">1. 什么是爬虫？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-爬虫学习路线"><span class="toc-text">2. 爬虫学习路线</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-从第一个爬虫开始"><span class="toc-text">3. 从第一个爬虫开始</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#爬虫的过程"><span class="toc-text">爬虫的过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#urllib库"><span class="toc-text">urllib库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#request的使用"><span class="toc-text">request的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#urlopen方法"><span class="toc-text">urlopen方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Request方法"><span class="toc-text">Request方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#error的使用"><span class="toc-text">error的使用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-URLError类"><span class="toc-text">1. URLError类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-HTTPError类"><span class="toc-text">2. HTTPError类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-总结"><span class="toc-text">4. 总结</span></a></li></ol>
    </div>

        <p>爬虫的技术可以应用到很多生活场景中，例如，自动投票，批量下载感兴趣的文章、小说、视频，微信机器人，爬取重要的数据进行数据分析等等</p>
<a id="more"></a>
<hr>
<h2 id="1-什么是爬虫？"><a href="#1-什么是爬虫？" class="headerlink" title="1. 什么是爬虫？"></a>1. 什么是爬虫？</h2><p>首先应该弄明白一件事，就是什么是爬虫，为什么要爬虫，百度了一下，是这样解释的：</p>
<blockquote>
<p>网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。</p>
</blockquote>
<p>其实，说白了就是爬虫可以模拟浏览器的行为做你想做的事，订制化自己搜索和下载的内容，并实现自动化的操作。比如浏览器可以下载小说，但是有时候并不能批量下载，那么爬虫的功能就有用武之地了。</p>
<p>实现爬虫技术的编程环境有很多种，Java，Python，C++等都可以用来爬虫。但是选择了Python，相信很多人也一样选择Python，因为Python确实很适合做爬虫，丰富的第三方库十分强大，简单几行代码便可实现你想要的功能，更重要的，Python也是数据挖掘和分析的好能手。这样爬取数据和分析数据一条龙的服务都用Python真的感觉很棒啊！</p>
<h2 id="2-爬虫学习路线"><a href="#2-爬虫学习路线" class="headerlink" title="2. 爬虫学习路线"></a>2. 爬虫学习路线</h2><p>知道了什么是爬虫，给大家说说总结出的学习爬虫的基本路线吧，只供大家参考，因为每个人都有适合自己的方法，在这里只是提供一些思路。</p>
<p>学习Python爬虫的大致步骤如下：</p>
<ul>
<li>首先学会基本的Python语法知识</li>
<li>学习Python爬虫常用到的几个重要内置库urllib, http等，用于下载网页</li>
<li>学习正则表达式re、BeautifulSoup（bs4）、Xpath（lxml）等网页解析工具</li>
<li>开始一些简单的网站爬取（从百度开始的，哈哈），了解爬取数据过程</li>
<li>了解爬虫的一些反爬机制，header，robot，时间间隔，代理ip，隐含字段等</li>
<li>学习一些特殊网站的爬取，解决登录、Cookie、动态网页等问题</li>
<li>了解爬虫与数据库的结合，如何将爬取数据进行储存</li>
<li>学习应用Python的多线程、多进程进行爬取，提高爬虫效率</li>
<li>学习爬虫的框架，Scrapy、PySpider等</li>
<li>学习分布式爬虫（数据量庞大的需求）<br>以上便是一个整体的学习概况，好多内容也需要继续学习，关于提到的每个步骤的细节，会在后续内容中以实战的例子逐步与大家分享，当然中间也会穿插一些关于爬虫的好玩内容。</li>
</ul>
<h2 id="3-从第一个爬虫开始"><a href="#3-从第一个爬虫开始" class="headerlink" title="3. 从第一个爬虫开始"></a>3. 从第一个爬虫开始</h2><p>第一个爬虫代码的实现我想应该是从urllib开始吧，开始学习的时候就是使用urllib库敲了几行代码就实现了简单的爬数据功能，我想大多伙伴们也都是这么过来的。当时的感觉就是：哇，好厉害，短短几行竟然就可以搞定一个看似很复杂的任务，于是就在想这短短的几行代码到底是怎么实现的呢，如何进行更高级复杂的爬取呢？带着这个问题我也就开始了urllib库的学习。</p>
<p>首先不得不提一下爬取数据的过程，弄清楚这到底是怎样一个过程，学习urllib的时候会更方便理解。</p>
<h3 id="爬虫的过程"><a href="#爬虫的过程" class="headerlink" title="爬虫的过程"></a>爬虫的过程</h3><p>其实，爬虫的过程和浏览器浏览网页的过程是一样的。道理大家应该都明白，就是当我们在键盘上输入网址点击搜索之后，通过网络首先会经过DNS服务器，分析网址的域名，找到了真正的服务器。然后我们通过HTTP协议对服务器发出GET或POST请求，若请求成功，我们就得到了我们想看到的网页，一般都是用HTML, CSS, JS等前端技术来构建的，若请求不成功，服务器会返回给我们请求失败的状态码，常见到的503，403等。</p>
<p>爬虫的过程亦是如此，通过对服务器发出请求得到HTML网页，然后对下载的网页进行解析，得到我们想要的内容。当然，这是一个爬虫过程的一个概况，其中还有很多细节的东西需要我们处理的，这些在后续会继续与大家分享。</p>
<p>了解了爬虫的基本过程后，就可以开始我们真正的爬虫之旅了。</p>
<h3 id="urllib库"><a href="#urllib库" class="headerlink" title="urllib库"></a>urllib库</h3><p>Python有一个内置的urllib库，可谓是爬虫过程非常重要的一部分了。这个内置库的使用就可以完成向服务器发出请求并获得网页的功能，所以也是学习爬虫的第一步了。</p>
<p>用的是Python3.x，urllib库的结构相对于Python2.x有一些出入，Python2.x中使用的urllib2和urllib库，而Python3.x中合并成一个唯一的urllib库。</p>
<p>首先，我们来看看Python3.x的urllib库都有什么吧。</p>
<p>用的IDE是Pycharm，编辑调试非常方便，很赞。<br>在控制台下输入如下代码：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;import urllib</span><br><span class="line">&gt;&gt;dir(urllib)</span><br><span class="line"></span><br><span class="line">[&apos;__builtins__&apos;,&apos;__cached__&apos;, &apos;__doc__&apos;, &apos;__file__&apos;, &apos;__loader__&apos;, &apos;__name__&apos;, &apos;__package__&apos;,&apos;__path__&apos;, &apos;__spec__&apos;, &apos;error&apos;, &apos;parse&apos;, &apos;request&apos;, &apos;response&apos;]</span><br></pre></td></tr></table></figure></p>
<p>可以看到urllib除了以双下划线开头结尾的内置属性外，还有4个重要的属性，分别是error，parse，request，response。</p>
<p>在Python的urllib库中doc开头是这样简短描述的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Error：“Exception classesraised by urllib.”----就是由urllib举出的exception类</span><br><span class="line">Parse：“Parse (absolute and relative) URLs.”----解析绝对和相对的URLs</span><br><span class="line">Request：“An extensiblelibrary for opening URLs using a variety of protocols”</span><br><span class="line">----用各种协议打开URLs的一个扩展库</span><br><span class="line">Response：“Response classesused by urllib.”----被urllib使用的response类</span><br></pre></td></tr></table></figure></p>
<p>这4个属性中最重要的当属request了，它完成了爬虫大部分的功能，我们先来看看request是怎么用的。</p>
<h3 id="request的使用"><a href="#request的使用" class="headerlink" title="request的使用"></a>request的使用</h3><p>request请求最简单的操作是用urlopen方法，代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">response = urllib.request.urlopen(&apos;http://python.org/&apos;)</span><br><span class="line">result = response.read()</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b&apos;&lt;!doctype html&gt;\n&lt;!--[if lt IE 7]&gt;...&lt;/body&gt;\n&lt;/html&gt;\n&apos;</span><br></pre></td></tr></table></figure></p>
<p>发现得到的运行结果竟然是乱码！！别着急，这是因为编码的问题，我们只需要将请求的类文件读取再解码就可以了。</p>
<p>修改代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">response = urllib.request.urlopen(&apos;http://python.org/&apos;)</span><br><span class="line">result = response.read().decode(&apos;utf-8&apos;)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure></p>
<p>运行结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;!doctype html&gt;</span><br><span class="line">&lt;!--[if lt IE 7]&gt;   &lt;html class=&quot;no-js ie6 lt-ie7 lt-ie8&gt;..</span><br><span class="line">&lt;!--[if IE 7]&gt;      &lt;html class=&quot;no-js ie7 lt-ie8 lt-ie9&quot;&gt;.. </span><br><span class="line">&lt;!--[if IE 8]&gt;      &lt;html class=&quot;no-js ie8 lt-ie9&quot;&gt;                 &lt;![endif]--&gt;</span><br><span class="line">&lt;!--[if gt IE 8]&gt;&lt;!--&gt;&lt;html class=&quot;no-js&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;</span><br><span class="line"></span><br><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;meta charset=&quot;utf-8&quot;&gt;</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>得到的就是我们想要的html的网页了，怎么样，简单吧。</p>
<p>下面来介绍一下这个urlopen方法和其中应用的参数。</p>
<h3 id="urlopen方法"><a href="#urlopen方法" class="headerlink" title="urlopen方法"></a>urlopen方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">def urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TI</span><br><span class="line">            MEOUT,*, cafile=None, capath=None, </span><br><span class="line">            cadefault=False, context=None):</span><br></pre></td></tr></table></figure>
<p>urlopen是request的其中一个方法，功能是打开一个URL，URL参数可以是一串字符串（如上例子中一样），也可以是Request对象（后面会提到）。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">url：即是我们输入的url网址，（如：http://www.xxxx.com/）；</span><br><span class="line">data：是我们要发给服务器请求的额外信息（比如登录网页需要主动填写的用户信息）。如果需要添加data参数，那么是POST请求，默认无data参数时，就是GET请求；</span><br><span class="line"></span><br><span class="line">一般来讲，data参数只有在http协议下请求才有意义</span><br><span class="line">data参数被规定为byte object，也就是字节对象</span><br><span class="line">data参数应该使用标准的结构，这个需要使用urllib.parse.urlencode()将data进行 转换，而一般我们把data设置成字典格式再进行转换即可；data在以后实战中会介绍如何使用</span><br><span class="line">timeout：是选填的内容，定义超时时间，单位是秒，防止请求时间过长，不填就是默认的时间；</span><br><span class="line">cafile：是指向单独文件的，包含了一系列的CA认证 （很少使用，默认即可）;</span><br><span class="line">capath：是指向文档目标，也是用于CA认证（很少使用，默认即可）；</span><br><span class="line">cafile：可以忽略</span><br><span class="line">context：设置SSL加密传输（很少使用，默认即可）；</span><br><span class="line">它会返回一个类文件对象，并可以针对这个对象进行各种操作（如上例中的read操作，将html全部读出来），其它常用方法还有：</span><br><span class="line"></span><br><span class="line">geturl(): 返回URL，用于看是否有重定向。</span><br><span class="line">result = response.geturl()</span><br><span class="line"></span><br><span class="line">结果： https://www.python.org/</span><br><span class="line"></span><br><span class="line">info()：返回元信息，例如HTTP的headers。</span><br><span class="line">result = response.info()</span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">    x-xss-protection: 1; mode=block</span><br><span class="line">    X-Clacks-Overhead: GNU Terry Pratchett</span><br><span class="line">    ...</span><br><span class="line">    Vary: Cookie   </span><br><span class="line">    Strict-Transport-Security: max-age=63072000;includeSubDomains</span><br><span class="line">getcode()：返回回复的HTTP状态码，成功是200，失败可能是503等，可以用来检查代理IP的可使用性。</span><br><span class="line">result = response.getcode()</span><br><span class="line"></span><br><span class="line">结果：200</span><br></pre></td></tr></table></figure></p>
<h3 id="Request方法"><a href="#Request方法" class="headerlink" title="Request方法"></a>Request方法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class Request:</span><br><span class="line">    def __init__(self, url, data=None, headers=&#123;&#125;,</span><br><span class="line">                 origin_req_host=None, unverifiable=False,</span><br><span class="line">                 method=None):</span><br></pre></td></tr></table></figure>
<p>如上定义，Request是一个类，初始化中包括请求需要的各种参数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">url，data和上面urlopen中的提到的一样。</span><br><span class="line">headers是HTTP请求的报文信息，如User_Agent参数等，它可以让爬虫伪装成浏览器而不被服务器发现你正在使用爬虫。</span><br><span class="line">origin_reg_host, unverifiable, method等不太常用</span><br><span class="line">headers很有用，有些网站设有反爬虫机制，检查请求若没有headers就会报错，因此为保证爬虫的稳定性，基本每次都会将headers信息加入进去，这是反爬的简单策略之一。</span><br></pre></td></tr></table></figure></p>
<p>那么如何找到你所在浏览器的headers呢？</p>
<p>可以通过进入浏览器F12查看到<br>比如，用的Chrome浏览器，按F12-&gt;network就可以查看request的headers，可以把这个浏览器的headers信息复制下来使用。</p>
<p>下面来看看Request如何使用吧，代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">headers = &#123;&apos;User_Agent&apos;: &apos;&apos;&#125;</span><br><span class="line">response = urllib.request.Request(&apos;http://python.org/&apos;, headers=headers)</span><br><span class="line">html = urllib.request.urlopen(response)</span><br><span class="line">result = html.read().decode(&apos;utf-8&apos;)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure></p>
<p>结果和前面urlopen是一样的，前面提到urlopen除了可以接受指定参数，也可以接受Request类的对象。’ ‘里面填写自己浏览器的信息即可。</p>
<p>urllib库的requset属性里面还有很多其它方法，代理、超时、认证、HTTP的POST模式下请求等内容将在下次进行分享，这次主要介绍基本功能。</p>
<p>下面来说说异常，urllib库的error方法。</p>
<h3 id="error的使用"><a href="#error的使用" class="headerlink" title="error的使用"></a>error的使用</h3><p>error属性里面主要包括了两个重要的exception类，URLError类和HTTPError类。</p>
<h4 id="1-URLError类"><a href="#1-URLError类" class="headerlink" title="1. URLError类"></a>1. URLError类</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, reason, filename=None):</span><br><span class="line">    self.args = reason,</span><br><span class="line">    self.reason = reason</span><br><span class="line">    if filename is not None:</span><br><span class="line">        self.filename = filename</span><br></pre></td></tr></table></figure>
<p>URLError类是OSError的子类，继承OSError，没有自己的任何行为特点，但是将作为error里面所有其它类型的基类使用。<br>URLError类初始化定义了reason参数，意味着当使用URLError类的对象时，可以查看错误的reason。</p>
<h4 id="2-HTTPError类"><a href="#2-HTTPError类" class="headerlink" title="2. HTTPError类"></a>2. HTTPError类</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self, url, code, msg, hdrs, fp):</span><br><span class="line">    self.code = code</span><br><span class="line">    self.msg = msg</span><br><span class="line">    self.hdrs = hdrs</span><br><span class="line">    self.fp = fp</span><br><span class="line">    self.filename = url</span><br></pre></td></tr></table></figure>
<p>HTTPError是URLError的子类，当HTTP发生错误将举出HTTPError。<br>HTTPError也是HTTP有效回应的实例，因为HTTP协议错误是有效的回应，包括状态码，headers和body。所以看到在HTTPError初始化的时候定义了这些有效回应的参数。<br>当使用HTTPError类的对象时，可以查看状态码，headers等。<br>下面我们用一个例子来看一下如何使用这两个exception类。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import urllib.error</span><br><span class="line">try:</span><br><span class="line">    headers = &#123;&apos;User_Agent&apos;: &apos;Mozilla/5.0 (X11; Ubuntu; </span><br><span class="line">                Linux x86_64; rv:57.0) Gecko/20100101 </span><br><span class="line">                Firefox/57.0&apos;&#125;</span><br><span class="line">    response = urllib.request.Request(&apos;http://python.org/&apos;, </span><br><span class="line">                                       headers=headers)</span><br><span class="line">    html = urllib.request.urlopen(response)</span><br><span class="line">    result = html.read().decode(&apos;utf-8&apos;)</span><br><span class="line">except urllib.error.URLError as e:</span><br><span class="line">    if hasattr(e, &apos;reason&apos;):</span><br><span class="line">        print(&apos;错误原因是&apos; + str(e.reason))</span><br><span class="line">except urllib.error.HTTPError as e:</span><br><span class="line">    if hasattr(e, &apos;code&apos;):</span><br><span class="line">        print(&apos;错误状态码是&apos; + str(e.code))</span><br><span class="line">else:</span><br><span class="line">    print(&apos;请求成功通过。&apos;)</span><br></pre></td></tr></table></figure></p>
<p>以上代码使用了try..exception的结构，实现了简单的网页爬取，当有异常时，如URLError发生时，就会返回reason，或者HTTPError发生错误时就会返回code。异常的增加丰富了爬取的结构，使其更加健壮。</p>
<p>为什么说更加健壮了呢？</p>
<p>不要小看了这些异常的错误，这些异常的错误非常好用，也非常关键。想想看，当你编写一个需要不断自动运行爬取并解析的代码时，你是不希望程序中间被打断而终止的。如果这些异常状态没有设置好，那么就很有可能弹出错误而被终止，但如果设置好了完整的异常，则遇到错误时就会执行发生错误的代码而不被打断（比如向上面代码一样打印错误code等）。</p>
<p>这些打断程序的错误可能是很多种，尤其当你使用代理ip池的时候，会发生很多不同错误，这时异常就起到作用了。</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><ul>
<li>介绍了爬虫的定义和学习路线</li>
<li>介绍了爬虫的过程</li>
<li>介绍开始爬虫学习的urllib库的使用，包含以下几个方法：<br>request请求： urlopen, Request<br>error异常</li>
</ul>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/数据挖掘/">数据挖掘</a>
  </div>

        
  <div class="tags">
    <a href="/tags/python/">python</a>, <a href="/tags/爬虫/">爬虫</a>
  </div>

        
  <div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a title="分享到QQ空间" href="#" class="bds_qzone" data-cmd="qzone"></a><a title="分享到新浪微博" href="#" class="bds_tsina" data-cmd="tsina"></a><a title="分享到腾讯微博" href="#" class="bds_tqq" data-cmd="tqq"></a><a title="分享到人人网" href="#" class="bds_renren" data-cmd="renren"></a><a title="分享到微信" href="#" class="bds_weixin" data-cmd="weixin"></a></div>
  <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["qzone","tsina","tqq","renren","weixin"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["qzone","tsina","tqq","renren","weixin"]}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>
<!-- 返回顶部 -->
<div id="toTop">
	<a href="#">▲</a>
	<a href="#footer">▼</a>
</div></div></div>
      <aside id="sidebar" class="alignright">
        
           <div class="search">
  <form action="/search/index.html" method="get" accept-charset="utf-8">
<!--     <input type="search" name="wd"results="0" placeholder="搜索">
    <input type="hidden" name="wd" value="site:paradoxallen.github.io"> -->
     <input type="text" id="search" class="st-default-search-input" placeholder="搜索" style="height: 100%" />
  </form>
</div> 
        
          
<div class="widget tag">
  <h3 class="title" id="categories">分类</h3>
     <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/个人随笔/">个人随笔</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/博客开发/">博客开发</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/应用统计/">应用统计</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数学基础/">数学基础</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据分析/">数据分析</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据挖掘/">数据挖掘</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">18</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程相关/">编程相关</a><span class="category-list-count">25</span></li></ul> 
</div>
 

        
          
<div class="widget tagcloud">
  <h3 class="title">标签</h3>
  <div class="entry">
    <a href="/tags/Excel/" style="font-size: 12.5px;">Excel</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Machine-Learning-课程笔记/" style="font-size: 18.75px;">Machine Learning 课程笔记</a> <a href="/tags/SQL/" style="font-size: 13.75px;">SQL</a> <a href="/tags/hexo/" style="font-size: 11.25px;">hexo</a> <a href="/tags/python/" style="font-size: 20px;">python</a> <a href="/tags/博客/" style="font-size: 11.25px;">博客</a> <a href="/tags/可视化/" style="font-size: 10px;">可视化</a> <a href="/tags/大气科学/" style="font-size: 10px;">大气科学</a> <a href="/tags/提问/" style="font-size: 10px;">提问</a> <a href="/tags/数据类型/" style="font-size: 10px;">数据类型</a> <a href="/tags/机器学习/" style="font-size: 16.25px;">机器学习</a> <a href="/tags/概率论与数理统计/" style="font-size: 10px;">概率论与数理统计</a> <a href="/tags/模型/" style="font-size: 12.5px;">模型</a> <a href="/tags/爬虫/" style="font-size: 17.5px;">爬虫</a> <a href="/tags/目录/" style="font-size: 10px;">目录</a> <a href="/tags/算法/" style="font-size: 11.25px;">算法</a> <a href="/tags/线性代数/" style="font-size: 10px;">线性代数</a> <a href="/tags/统计/" style="font-size: 10px;">统计</a> <a href="/tags/编码/" style="font-size: 10px;">编码</a> <a href="/tags/高等数学/" style="font-size: 10px;">高等数学</a>
  </div>
</div>

        
          
  <div class="widget tag">
    <h3 class="title">归档</h3>
	<ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">六月 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/05/">五月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/04/">四月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">十一月 2017</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">十月 2017</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/09/">九月 2017</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">二月 2017</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">一月 2017</a><span class="archive-list-count">4</span></li></ul>
  </div>

        
      </aside>
      <div class="clearfix"></div>
    </div>
  <footer id="footer"><div class="footer-content inner">
  <div class="alignleft">
  
    &copy; 2018 LRP
    
  </div>

  <!--
  <div class="alignright">
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, Theme
    <a href="https://github.com/pengloo53/Hexo-theme-light_cn">light_cn</a>
  </div>
  -->

  <!--
  <div>
    Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a>
  </div>
  -->
  
  <div class="clearfix"></div>
</div></footer>
  <script src="http://libs.baidu.com/jquery/2.1.1/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>

<!-- calendar widget -->


<!-- 百度统计 -->

	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?9acf0cedd48dc53be256ede5a98c2aaa";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>


<!-- fancybox -->

<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


</body>
</html>